\chapter{Introduction}

\section{Problem statement and objective}


Mathematical optimization is concerned with problems of the form
\[
  \min_{x\in \mathcal{X}}  f(x),
\]
where $f\colon\R^n\to\R\cup\{\infty\}$ is a function and
$\mathcal{X}\subset\dom(f)$ is a set.
Typically, the set $\mathcal{X}$ is called the \textit{feasible set},
elements $x\in\mathcal{X}$ are \textit{feasible points}, and $f$ is called the \textit{objective function},
or simply \textit{objective}.
Oftentimes, a point $x\in\R^n$ is referred to as a \textit{decision variable}.
If $\mathcal{X}$ is nonempty, then the problem is called \textit{feasible}.
In that case,
if there exists $x^\star\in\mathcal{X}$ such that $f(x^\star) \leq f(x)$ for all $x\in\mathcal{X}$,
then $x^\star$ is called \textit{solution}. The value $f(x^\star)$ is called
\textit{optimal value} or \textit{minimal value}.
In practice, the feasible set $\mathcal{X}$ is often defined implicitly through the use of auxillary functions,
which yields to the formulation
\begin{equation*}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^n} f(x) \\
    &\textup{subject to (s.\,t.)} \enspace &g(x) \leq 0 \\
    &\phantom{\textup{s.\,t.}} \enspace &h(x) = 0
    \end{aligned}
\end{equation*}
for functions $g\colon\R^n\to\R^m$, $h\colon\R^n\to\R^\ell$. The feasible
set is then given by $\mathcal{X} = \{\, x\in\R^n \mid g(x) \leq 0 \text{ and } h(x) = 0 \,\}$.
An important special case of optimization problems are
\textit{convex optimization} problems, in which the objective $f$ and
the map $g$ are convex functions, and $h$ is affine.
Convex problems have (among other things) the highly desirable property that
every local minimum is a global minimum, which makes
optimization algorithms that only use local information (like gradients)
work effectively.

In this work, we will condsider
convex problems that are subject to randomness.
Such \textit{stochastic optimization} problems
arise in many applications (\textcolor{red}{TODO: add refs}).
The problems we will analyze
take the following general form
\begin{equation}
    \label{eq:main-problem}
    \tag{P}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^n} \, \{\, f(x) := \E(F_\zeta(x)) + r(x) \,\} \\
    &\textup{s.\,t.} \quad A(\xi) x - b(\xi) \leq 0 \quad \textnormal{almost surely (a.\,s.)},
    \end{aligned}
\end{equation}
where $\zeta$ and $\xi$ are a random variables that capture uncertainty in the objective and
the constraints.
The objective $f$ is composed of two functions: The expectation
functional $\E(F_\zeta)\colon\R^n\to\R$, which we assume to
be smooth, and the potentially nonsmooth function $r\colon\R^n\to\R\cup\{\infty\}$,
which we will refer to as the \textit{regularizer}.
The constraints are affine inequalities with matrices $A(z)\in\R^{m\times n}$
and $b(z)\in\R^m$ for $z\in\R^p$.

Examples of these problems
appear in numerous areas of applied mathematics. One such domain is optimal control,
where the randomness often arises from some continuous uncertainty in some
variables, which leads to an infinite amount of constraints.
For example, such uncertainty could come
from an unknown future demand that is subject to gaussian noise (see \cref{ex:inventory-control}).
In that case, optimization algorithms for solving \eqref{eq:main-problem} need a way
to deal with the constraints one-by-one or in batches, as a simlutaneous treatment of
all constraints, like in the classical projected gradient method \cite{nemirovski2009robust},
would be impossible to implement.
This holds true even if the number of constraints is not infinite but merely very large,
as is the case in modern machine learning, where the random variables $\zeta$ and $\xi$ model
data points in a data set of size $N\in\N$,
and the decision variable $x$ represents parameters of some statistical model.
Problem \eqref{eq:main-problem} then takes on the specific form
\begin{equation*}
    \begin{aligned}
    &\min_{x \in \R^n} \left\{\, f(x) = \frac{1}{N} \sum_{i=1}^N F_i(x) + r(x) \,\right\} \\
    &\textup{s.\,t.} \quad A_i x - b_i \leq 0 \quad \textnormal{for all $i\in\{\, 1,\dots, N \,\}$}.
    \end{aligned}
\end{equation*}
Example problems that can be formulated in this way are support vector machines
and logistic regression with fairness constraints (\textcolor{red}{TODO}).

A classical approach to solve \eqref{eq:main-problem} is stochastic subgradient
descent (SGD) \cite{robbins1951stochastic}: We start from an initial point $x_0\in\R^n$.
In iteration $k\in\N$, we pick a \textit{step size} $\eta_k\in(0,\infty)$ and a
\textit{stochastic subgradient} $g_k$ of $f$ at $x_k$.
Then we set
\[
    x_{k+1} = \proj(x_k - \eta_k g_k)
\]
and repeat. The map $\proj\colon\R^n\to\R^n$ is the \textit{projection map onto} $\mathcal{X}$,
which is defined as $\proj(x) := \argmin_{y\in\mathcal{X}} \norm{x - y}$, and
ensures that the iterates stay in the feasible set $\mathcal{X}$. If $x^\star\in\mathcal{X}$
solves \eqref{eq:main-problem}, then one can show that, under suitable
choice of step sizes, $\norm{x_k - x^\star}^2$ converges to the solution
of \eqref{eq:main-problem} with rate $\mathcal{O}(1/k)$ for \textit{strongly convex} objectives
(\textcolor{red}{TODO: add ref for strongly convex}).
However, Nemirovski et al. \cite{nemirovski2009robust} showed that
this convergence is highly dependent on knowing the strong convexity constant,
and proposed a more robust version of the algorithm that utilizes suitable averages of the original
iterates. They proved that the resulting iterates $(\bar{x}_k)_{k\in\N}$ yield convergence of the
function values $f(\bar{x}_k)$ to that of $f(x^\star)$ with rate
$\mathcal{O}(1/\sqrt{k})$ for objectives $f$ which need only be convex.
Unfortunately, we can not apply SGD to our problem, because the complexity of $\mathcal{X}$ makes computing
the projection $\proj(x_k - \eta_K g_k)$ infeasible in our case.

% To solve problems of type \eqref{eq:main-problem},
% we propose a novel method, which combines the work of Nedić et al. \cite{nedich2023huber}
% with the proximal gradient method \cite{parikh2014proximal}, into a sequential
% proximal stochastic gradient method (SeqProx-SGD).
A classic idea to deal with complex feasibility sets is to use \textit{penalty functions}.
In this approach
the constrained problem \eqref{eq:main-problem} gets approximated by
an unconstrained problem, by introducing a convex function $\pi\colon\R^n\to\R$ that penalizes
points that are not feasible: $\pi(x) > 0$ for infeasible $x$ and $\pi(x) = 0$ for feasible $x$.
The resulting approximation to \eqref{eq:main-problem}
then takes the form
\begin{equation}
    \label{eq:basic-penalized-problem}
    \min_{x\in\R^n} \left\{\, f_\gamma(x) := f(x) + \gamma \pi(x) \,\right\},    
\end{equation}
where $\gamma\in (0,\infty)$ is a constant that is used to control the influence of the penalty function $\pi$
on the objective. For this unconstrained problem, the projection map is simply the identiy map, so one can easily
apply SGD to solve \eqref{eq:basic-penalized-problem}.
The larger $\gamma$, the closer the solution to \eqref{eq:basic-penalized-problem}
is to being feasible. Under suitable choice for $\pi$, one can show that
in the limit $\gamma \to \infty$ the sequence of solutions $(x_\gamma^\star)_{\gamma\in(0, \infty)}$
to \eqref{eq:basic-penalized-problem} converges to the solution $x^\star$ of \eqref{eq:main-problem}:
$\lim_{\gamma\to\infty} x^\star_\gamma = x^\star$. For certain choices of $\pi$, one
can even show that there exists some finite $\gamma\in(0, \infty)$, such that $x_\gamma^\star = x^\star$.
Such penalties are called \textit{exact penalties}. A standard example is the Hinge penalty
$\pi_{\ell_1}$, defined by $\pi_{\ell_1}(x) := \E(\norm{(A(\xi)x - b(\xi))_+}_1)$, where $\norm{\cdot}_1$ is
the $\ell_1$-norm, and $(y)_+$ applies $\R \ni t\mapsto\max(t, 0)$ to every element of $y\in\R^m$.
While the defining property of exact penalties is very desirable, they, like the Hinge penalty,
all suffer from necessarily being nonsmooth, which is known to slow down stochastic subgradient descent.
On the other hand, smooth penalties like the squared hinge penalty
$\pi_{\ell_2}(x) := \E(\norm{(A(\xi)x - b(\xi))_+}_2^2)$ with $\norm{\cdot}_2$ the $\ell_2$-norm,
often need $\gamma$ to grow very large to get solutions that are reasonably close to the feasible set.
This has the unfortunate side effect
that the gradient norm $\norm{\nabla f_\gamma}_2$ can grow very large, which
makes stochastic gradient descent iterates often very unstable in practice.
Additionally, one is forced to use step sizes that decay to $0$ quickly to counter
the large gradient norms, which
slows down convergence. 

A solution to the drawbacks of classical penalty methods was introduced by Nedić et al. \cite{nedich2023huber},
where the authors analyzed problems similar to ours of the form
\begin{equation}
    \label{eq:nedic-problem}
    \begin{aligned}
    &\min_{x \in \R^n} f(x) \\
    &\textup{s.\,t.} \quad a_i^\top x - b_i \leq 0 \quad \textnormal{for all $i\in\{\, 1,\dots,m \,\}$}.
    \end{aligned}
\end{equation}
Instead of a fixed penalty function $\pi$ however, the authors
introduced a sequence of smooth \textit{inexact} penalties $(\pi^{\textnormal{hub}}_k)_{k\in\N}$,
defined as follows: Let $(\delta_k)_{k\in\N}$ be a sequence of positive real numbers. For $k\in\N$, we define
\[
    \pi_k^\textnormal{hub}(x) := \frac{1}{m} \sum_{i=1}^{m} h_k^{\text{hub}}(x; a_i, b_i),
\]
where
\[
    h_k^{\text{hub}}(x; a, b)
    :=
    \begin{cases}
        \frac{\langle a, x \rangle - b}{|\!|a|\!|} \quad&\textnormal{if $\langle a, x \rangle - b > \delta_k$}, \\
        \frac{(\langle a, x \rangle - b + \delta_k)^2}{4\delta_k |\!|a|\!|} \quad&\textnormal{if $-\delta_k \leq \langle a, x \rangle - b \leq \delta_k$}, \\
        0, \quad&\textnormal{if $\langle a, x \rangle - b < -\delta_k$},
    \end{cases}
\]
for $x,a \in\R^n$, and $b\in\R$.
The authors then considered the sequence of unconstrained problems
\begin{equation}
    \label{eq:nedic-penalty-problem}
    \min_{x\in\R^n} \left\{\, f_k(x) := f(x) + \gamma_k \pi_k^{\textnormal{hub}}(x) \,\right\},
\end{equation}
for $k\in\N$ and $\gamma_k\in(0,\infty)$.
Whereas before we required $\pi(x) = 0$ for all feasible points,
the inexact penalties only satisfy $\pi_k^\textnormal{hub}(x) \geq 0$ for feasible $x\in\R^n$.
The crucial properties of the particular penalty sequence that the authors introduced
were that the sequence a) majorizes the hinge penalty, $\pi_k^\textnormal{hub}(x) \geq \pi_{\ell_1}(x)$ for all $x\in\R^n$,
b) converges to $\pi_{\ell_1}$ pointwis, $\lim_{k\to\infty} \pi_k^\textnormal{hub}(x) = \pi_{\ell_1}(x)$ for all $x\in\R^n$,
and c) has uniformly bounded gradients, $\sup_{k\in\N}\sup_{x\in\R^n} \norm{\nabla \pi_k^\textnormal{hub}(x)}_2 < \infty$.
By carefully choosing the sequence of parameters that control the convergence to $\pi_{\ell_1}$, denoted
by $(\delta_k)_{k\in\N}$, as well as the
sequence $(\gamma_k)_{k\in\N}$, the authors were able to show that there exists
a $\gamma_K\in(0,\infty)$ large enough such that the distance-to-feasibility of the solution $x_K^\star$
to the corresponding problem \eqref{eq:nedic-penalty-problem}, is independent of $\gamma_K$ and
only controlled by $\delta_K$. Written in mathematical notation, this means that
$\dist(x_K^\star, \mathcal{X}) := \inf_{x\in\mathcal{X}} \norm{x - x_K^\star} = \mathcal{O}(\delta_K)$. Thus,
this approach manages to combine the highly desirable properties of smooth unconstrained problems
with that of exact penalties.
The authors then go on to present an iterative stochastic gradient algorithm (see \cref{alg:nedic-sgd}),
which proceeds as follows: Start from an initial point $x_0\in\R^n$. Then, in iteration $k\in\N$,
we compute a subgradient of $f$ at $x_k$, denoted by $\tilde{\nabla}f(x_k)$. Then we
sample a random index $i\in\{\,1,\dots,m\,\}$ and calculate the gradient $\nabla h_k^{\text{hub}}(x_k; a_i, b_i)$.
Finally, update 
\[
    x_{k+1} := x_k - \eta_k(\tilde{\nabla}f(x_k) + \gamma_k \nabla h_k^{\text{hub}}(x_k; a_i, b_i)).
\]
After a set amount of $K\in\N$ iterations, one then computes the weighted average
\[
    \bar{x}_K := \sum_{k=0}^K \eta_k^{-1} x_k.
\]
For strongly convex objectives (that satisfy certain assumptions on the gradients),
the authors show that, for any $\epsilon \in (0, \infty)$,
one can choose $(\eta_k)_{k\in\N}, (\gamma_k)_{k\in\N}, (\delta_k)_{k\in\N}$ such that the
sequece $\bar{x}_K$ satisfies
\[
    \dist(\bar{x}_K, \mathcal{X}) = \mathcal{O}\Big(\frac{\log^{\epsilon} K}{K}\Big)
    \enspace
    \text{and}
    \enspace
    |f(\bar{x}_K) - f(x^\star)| = \mathcal{O}\Big(\frac{\log^{2\epsilon} K}{K}\Big).
\]
Note that this asymptotic rate is essentially as good as it gets, as we have
seen earlier that the projected gradient algorithm achieves the rate $\mathcal{O}(1 / K)$.
\begin{algorithm}[t]
\caption{(Nedić et al. \cite{nedich2023huber})}
\label{alg:nedic-sgd}
\begin{algorithmic}[1] % [1] enables line numbers
\Require Initial point $x_0\in\R^n$, step sizes $(\eta_k)_{k\in\N_0}$,
penalty weights $(\gamma_k)_{k\in\N_0}$
\For{$k = 0$ to $K - 1$}
    \State Uniformly sample random index $i \in \{\, 1,\dots,m \,\}$
    \State $g \gets \tilde{\nabla} f(x_{k}) + \gamma_k \nabla h_k^{\text{hub}} (x_k; a_i, b_i)$
    \State $x_{k+1} \gets x_k - \eta_k g$
\EndFor
\State $S_K \gets \sum_{k=1}^{K} \eta_k^{-1}$
\State $\bar{x}_K \gets S_K^{-1} \sum_{k=1}^{K} \eta_k^{-1} x_k$
\State \Return $\bar{x}_{K}$
\end{algorithmic}
\end{algorithm}

In this work, we aim to build on the incremental gradient method
(\cref{alg:nedic-sgd}).
First, we will extend the method to the more general situation of \eqref{eq:main-problem}.
Namely, our version of \eqref{eq:nedic-penalty-problem} has the form
\begin{equation*}
    \min_{x\in\R^n} \left\{\, f_k(x) := \E(F_\zeta(x)) + r(x) + \gamma_k \pi_k(x) \,\right\},
\end{equation*}
for $k\in\N$, where, as before, $\gamma_k\in(0, \infty)$.
We keep the penalties $\pi_k(x)$ more generic, but assume that there
exist smooth real-valued functions $(h_k)_{k\in\N}$ defined on $\R^m$,
such that
\[
    \pi_k(x) = \E(h_k(x_k; A(\xi), b(\xi))) \enspace\text{and}\enspace \pi_k(x) \downarrow_{k\to\infty} \pi_{\ell_1}(x)
\]
for all $x\in\R^n$.
This more general treatment allows for more flexibility in the design of the method,
and includes the softplus penalty introduced in \cite{li2025new}.

As opposed to the setting of \cref{alg:nedic-sgd},
we may not be able to calculate the gradient of our objective,
because $r$ may be nonsmooth
or because calculating the gradient of the
expectation functional, $\E(F_\zeta)$, may be infeasible. Say, for example, because the
distribution of $\zeta$ is unknown or, in the case of large-scale machine learning,
because the size of the dataset is too large to feasibly compute the full gradient of
$\E(F_\zeta(x))$ (which would require the evaluation of a very large sum) for
multiple iterations.
Nedić et al. deal with nonsmoothness by using \textit{subgradients}
instead of gradients, which are also defined at nondifferentiable
points.
However, in our approach we choose to instead use a \textit{proximal operator} \cite{parikh2014proximal} to deal
with nonsmooth objectives:
For $\eta\in(0, \infty)$ and $r\colon\R^n\to\R\cup\{\infty\}$,
the proximal operator $\prox_{\eta r}\colon\R^n\to\R^n$ is defined as
\[
    \prox_{\eta r}(x) := \argmin_{u\in\R^n} \left\{\, r(u) + \frac{1}{2\eta} \norm{u - x}^2_2 \,\right\}.
\]
Under mild conditions on $r$, the proximal operator is well-defined and always
yields a point in the domain of $f$.
The proximal operator allows us to only work with gradients of the differentiable terms in our objective.
To calculate these gradients for objectives involving intractable expectation functionals/large sums, we
work with \textit{stochastic gradients} instead of regular (full) gradients.
A stochastic gradient of a differentiable function $f$ at some point $x\in\R^n$
is any random variable $g$ such that
$\E(g) = \nabla f(x)$. In our case, we can calculate a stochastic gradient
of $\E(F_\zeta) + \gamma_k \pi_k$ at $x\in\R^n$, based on a sample
$(s,z)$ from the joint distribution of $\zeta$ and $\xi$, as
\[
    \nabla F_s(x) + \gamma_k \nabla h_k(x; A(z), b(z)).
\]
To deal with the variance introduced by using a stochastic gradient
instead of the full gradient, we will use a minibatch of samples
and average over the resulting stochastic gradients to get
a variance-reduced estimate of $\E(\nabla F_\zeta(x)) + \gamma_k \nabla \pi_k(x)$.
The pseudo-code for the full algorithm is presented in \cref{alg:seqprox-sgd}.
\begin{algorithm}[t]
\caption{Stochastic Inexact Penalty Method}
\label{alg:seqprox-sgd}
\begin{algorithmic}[1] % [1] enables line numbers
\Require Initial point $x_0\in\R^n$, step sizes $(\eta_k)_{k\in\N_0}$,
penalty weights $(\gamma_k)_{k\in\N_0}$,
smooth penalty functions $(h_k)_{k\in\N_0}$,
iterate weights $(w_k)_{k\in\N}$,
sample oracle for joint distribution of $\zeta$ and $\xi$
\For{$k = 0$ to $K-1$}
    \State Sample $(\zeta_k, \xi_k)$ from their joint distribution
    \State $g_k \gets \nabla F_{\zeta_k}(x_{k}) + \gamma_k \nabla h_k(x_k; A(\xi_k), b(\xi_k))$
    \State $x_{k+1} \gets \textnormal{prox}_{\eta_k r} (x_k - \eta_k g_k$)
\EndFor
\State $S_K \gets \sum_{k=1}^{K} w_k$
\State $\bar{x}_K \gets S_K^{-1} \sum_{k=1}^{K} w_k x_k$
\State \Return $\bar{x}_{K}$
\end{algorithmic}
\end{algorithm}
% \begin{algorithm}[t]
% \caption{Sequential Proximal Stochastic Gradient Descent (SeqProx-SGD)}
% \label{alg:seqprox-sgd-single-batch}
% \begin{algorithmic}[1] % [1] enables line numbers
% \Require Initial point $x_0\in\R^n$, step sizes $(\eta_k)_{k\in\N_0}$,
% penalty weights $(\gamma_k)_{k\in\N_0}$,
% smooth penalty functions $(h_k)_{k\in\N_0}$, sample oracle for distribution of $\xi$
% \For{$k = 0$ to $K-1$}
%     \State Sample $\xi_k$ from distribution of $\xi$
%     \State $z \gets A(\xi_k)x_k - b(\xi_k)$
%     \State $g \gets  \nabla F_{\xi_k}(x_{k}) + \gamma_k A(\xi_k)^\top \nabla h_k(z)$
%     \State $x_{k+1} \gets \textnormal{prox}_{\eta_k r} (x_k - \eta_k g$)
% \EndFor
% \State $S_K \gets \sum_{k=0}^{K} \eta_k^{-1}$
% \State $\bar{x}_K \gets S_K^{-1} \sum_{k=0}^{K} \eta_k^{-1} x_k$
% \State \Return $\bar{x}_{K}$
% \end{algorithmic}
% \end{algorithm}
% Further, we propose another method that additionally uses \textit{exponential moving averages},
% which are a popular tool in machine learning literature
% \textcolor{red}{(TODO: add citations)}, and we provide
% theoretical guarantees to support their use.
% The pseudocode for this averaged method is presented in \cref{alg:seqprox-agd}.
% \begin{algorithm}[t]
% \caption{Sequential Proximal Averaged Gradient Descent (SeqProx-AGD)}
% \label{alg:seqprox-agd}
% \begin{algorithmic}[1] % [1] enables line numbers
% \Require Initial point $x_0\in\R^n$, step sizes $(\eta_k)_{k\in\N_0}$,
% penalty weights $(\gamma_k)_{k\in\N_0}$, minibatch sizes $(\beta_k)_{k\in\N_0}$,
% smooth penalty functions $(h_k)_{k\in\N_0}$, sample oracle for distribution of $\xi$
% \State $\bar{x}_0 \gets x_0$
% \For{$k = 0$ to $K-1$}
%     \State Sample minibatch $\{\, \xi_{1}^k,\dots, \xi_{\beta_k}^k \,\}$ from distribution of $\xi$
%     \State $g \gets \beta_k^{-1} \sum_{i=1}^{\beta_k} \nabla F_{\xi_i^k}(x_{k}) + \gamma_k A(\xi_i^k)^\top \nabla h_k(A(\xi_i^k)x_k - b(\xi_i^k))$
%     \State $x_{k+1} \gets \textnormal{prox}_{\eta_k r} (x_k - \eta_k g$)
%     \State $\hat{x}_{k+1} \gets (1 - \frac{\mu\eta_k}{4 - \mu\eta_k}) \hat{x}_{k} + \frac{\mu\eta_k}{4 - \mu\eta_k} x_{k+1}$
% \EndFor
% \State \Return $\bar{x}_{K}$
% \end{algorithmic}
% \end{algorithm}

Our objective is to analyze
%both \cref{alg:seqprox-sgd,alg:seqprox-agd}
\cref{alg:seqprox-sgd} theoretically
and support our theoretical findings with numerical examples. \textcolor{red}{TODO: Mention here the outline.
Difficulties: No bounded gradient assumption for $f$, infinite constraints, non-feasibility of iterates.}


\section{Contributions}


\section{Related literature}

\section{Example applications}

% To motivate our investigation, we will present
% applications of our methods to specific optimal control and machine learning problems.
% We will come back to these examples in \cref{sec:numerical-examples}.
% \begin{example}[Inventory control]
%     \label{ex:inventory-control}
%     This example is adapted from section 4.8.2 in \cite{evans-control}.
%     Let $T \in (0, \infty)$ represent a time period, and, for $t\in (0, T)$,
%     we let $\xi_t$ be a random variable. We introduce the variables
%     \begin{align*}
%         x(t) &= \textnormal{amount of inventory at time $t$} \\
%         \alpha(t) &= \textnormal{rate of ordering from manufacturers} \\
%         d(t,\xi_{t}) &= \textnormal{customer demand (random)} \\
%         \gamma &= \textnormal{cost of ordering one unit} \\
%         \beta &= \textnormal{cost of storing one unit.}
%     \end{align*}
%     The cost of inventory at time $t \in (0, T)$ is given by
%     \[
%         c(\alpha(\cdot),t) = \gamma \alpha(t) + \beta x(t)\,,
%     \]
%     and the total cost over the entire time period is
%     \[
%         C(\alpha(\cdot)) = \int_0^T c(t) \, \textnormal{d} t\,.
%     \]
%     Initially, we hold $x(0) := x_0 \in (0,\infty)$ units of items. The relationship between $x(t)$, $\alpha(t)$, and $d(t,\xi)$ is
%     \[
%         \dot{x}(t) = \alpha(t) - d(t,\xi_t) \quad \textnormal{a.\,s.}
%     \]
%     Our goal is to choose an ordering policy $t\mapsto \alpha(t)$ such that
%     the total cost $C(\alpha(\cdot))$ is minimized, all while ensuring
%     that demand is filled, i.\,e. we want $x(t) \geq 0$ a.\,s. for all $t\in (0, T)$.
%     To turn this problem into one that looks like \eqref{eq:new-model-problem}, we descretize
%     the continuous time period $(0, \,T)$ to a discrete one $\{t_0, t_1,\dots,t_{n-1}\}$ for some $n\in\N$,
%     and $0 < t_0 < t_1 < \cdots < t_{n-1} \in (0, T)$.
%     For $k\in\{0,\dots, n-1\}$, we set $x_k := x(t_k)$, $\alpha_k := \alpha(t_k)$, and
%     $d_k(\xi) := d(t_k, \xi_{t_k})$ with $\xi := (\xi_1,\dots,\xi_n)^\top$.
%     This leads us to the following discrete version of the above stochastic optimization problem
%     (see \cref{sec:numerical-example-inventory-control} for details)
%     \begin{equation}
%         \label{eq:inventory-control-example}
%         \begin{aligned}
%         &\min_{\alpha \in \mathbb{R}^{n}} \,
%             \E\left( \sum_{k=0}^{n-1} \gamma \alpha_k + \beta \sum_{j=0}^{k-1} (\alpha_j - d_j(\xi)) \right) \\
%         &\textup{s.\,t.}\quad
%         % \quad x_k = x_0 + \sum_{j=0}^{k-1} (\alpha_j - d_j) &\quad\textnormal{for all $k\in\{1,\dots,n\}$}\phantom{\textnormal{\,.}} \\
%         x_0 + \sum_{j=0}^{k-1} (\alpha_j - d_j(\xi)) \geq 0 \quad\textnormal{a.\,s. for all $k\in\{1,\dots,n\}$}.
%         \end{aligned}
%     \end{equation}
%     Let $A = (a_{ij})_{i,j\in\{1,\dots,n\}}\in\R^{n\times n}$ be defined by $a_{ij} = 1$ if $i\geq j$ and $a_{ij} = 0$, otherwise.
%     Then we can rewrite the objective as
%     \[
%         \E\left( \sum_{k=0}^{n-1} \gamma \alpha_k + \beta  \langle A_k, \alpha_k - d_k \rangle \right)
%     \]
%     and the contraints as
%     \[
%         x_0 + \langle A_k, \alpha_k - d_k \rangle \geq 0 \quad\textnormal{a.\,s. for all $k\in\{1,\dots,n\}$},
%     \]
%     where $A_k$ denotes the $k$-th row of $A$.

%     Let $I_n$ denote the $n\times n$ matrix on $\R^n$, $\bm{1} := (1,\dots,1)^\top\in\R^n$, and let $d(\xi) := (d_0(\xi), \dots, d_{n-1}(\xi))^\top$,
%     $\alpha(\xi) := (\alpha_0(\xi), \dots, \alpha_{n-1}(\xi))^\top$.
%     Further, we define
%     $v := \bm{1}^\top(\beta A + \gamma I_n)$, $\eta(\xi) := \bm{1}^\top \beta A d(\xi)$, and
%     $b(\xi) := A d(\xi) - x_0 \bm{1}$.
%     Adding a quadratic regularizer
%     $\alpha \mapsto \lambda |\!|\alpha|\!|^2_2$ to the objective, with $|\!|\cdot|\!|_2$ the standard euclidian norm on $\R^n$
%     and $\lambda > 0$,
%     we can once again rewrite the above as
%     \begin{equation*}
%         \begin{aligned}
%             &\min_{\alpha \in \mathbb{R}^{n}} \,
%             \E\Big( \langle v, \alpha \rangle - \eta(\xi) + \lambda |\!|\alpha|\!|^2_2 \Big) \\
%             &\textup{s.\,t.}\quad
%             -A \alpha + b(\xi)  \leq 0 \quad\textnormal{a.\,s.},
%         \end{aligned}
%     \end{equation*}
%     which is a convex optimization problem of the form \eqref{eq:new-model-problem}.
%     As an extension, one could consider the case where the costs $\gamma$ and $\beta$
%     change over time and/or are subject to randomness.
%     We will come back to this problem in \cref{sec:numerical-example-inventory-control}.
% \end{example}
% \begin{example}[Maximal margin classification]
%     This example and its presentation are based on section 6.3 in \cite{fercoq2019almost}.
%     A common problem in machine learning
%     is binary classification, where we are given a dataset
%     $D := \{\, (a_i, b_i) \mid i\in\{\, 1,\dots,N \,\} \,\}$
%     of size $N\in\N$, where each
%     $a_i\in\R^dn$ is called \textit{feature}
%     and each $b_i\in \{ 0, 1 \}$ is called
%     \textit{label}.
%     As a specific example, one can think of the problem of classifying images of pets
%     as either a cat ("$0$") or dog ("$1$").
%     The features and labels can be seen as samples from an underlying
%     probability distribution $\Prob$ on the space $\R^n \times \{0, 1\}$.
%     Given this dataset, we aim to learn a rule to classify 
%     new samples $a_{n+1}\in\R^n$ from the distribution of the features
%     as either $0$ or $1$. Ideally this rule would
%     choose the class that maximizes $\Prob(a_{n+1}, \cdot)$.
%     One approach to find such a rule is called \textit{support vector machine (SVM)},
%     which is an algorithm for constructing a hyperplane that seperates the two
%     subsets $\{\, a\in \R^n \mid (a, 0) \in D  \,\}$
%     and $\{\, a\in \R^n \mid (a, 1) \in D  \,\}$
%     in a way that maximizes the so-called "margin", which
%     is defined as the distance of the hyperplane to the closest point
%     of either of the two subsets. New samples are then classified based
%     on which side of the hyperplane they lie in.
%     Whether or not such a hyperplane even exists depends on the nature
%     of the dataset.  Nevertheless, we can always write down the problem of finding
%     such a hyperplane as a convex optimization problem
%     with affine constraints:
%     \begin{equation}
%         \label{eq:svm-hard-margin}
%         \begin{aligned}
%         &\min_{x \in \R^n} \,
%             \frac{1}{2} |\!|x|\!|^2 \\
%         &\textup{s.\,t.}\quad
%         b_i a_i^\top x  \geq 1 \quad\textnormal{for all $i\in\{ \, 1,\dots,N \, \}$}.
%         \end{aligned}
%     \end{equation}
%     If a solution to the above problem exists, we say that the dataset is
%     \textit{linearly seperable}.
%     This is often not the case, which is why the above problem is usually relaxed to the unconstrained problem
%     \[
%         \min_{x\in\R^n} \frac{1}{2} |\!|x|\!|^2 + C \sum_{i=1}^{N} \max(0, 1 - b_i \langle a_i, x \rangle), 
%     \]
%     where $C\in (0, \infty)$. This problem always has a solution, regardless of whether
%     or not the dataset is actually linearly seperable. However,
%     the quality of its solution for the classification task
%     highly depends on the choice of $C$ \cite{hastie2004entire}. In particular, if $C$ is too
%     small, the resulting classifier will perform poorly.
%     The methods we develop in \cref{sec:spm} will effectively allow us to
%     directectly tackle the original problem \eqref{eq:svm-hard-margin} by letting $C\to\infty$
%     and thus we can circumvent the difficulty of choosing a suitable $C$.
%     This application is presented in \cref{sec:numerical-example-svm}.
% \end{example}
% We will come back to these two examples in \cref{ch:numerical-examples}.
% Our goal is to develop methods to solve problems like \eqref{eq:new-model-problem}.
% The difficulty here is that we are essentially dealing with infinitely many constraints,
% if the support of $x\mapsto A(\xi)x + b(\xi)$ is infinite. However, even in the case of
% finite support there are difficulties, as the number of constraints might
% still be so large that popular methods like projected stochastic gradient descent
% (see \cite{doi:10.1137/070704277})
% can become infeasible, as is the case in large-scale machine
% learning problems. Here, each realizatixon of the random variable $\xi$
% would correspond to one data point of a dataset that can be too large to be processed all at once. Instead,
% one is forced to process the data points one by one or in batches.

% One method to address this problem is \textit{random constraint projections} \textcolor{red}{TODO} ...
% In this work, we will focus on a different method, which is based on
% a well-known method to deal with constraints indirectly
% by penalizing infeasible points: Instead of solving
% \eqref{eq:new-model-problem} directly, one instead solves a sequence of unconstrained
% problems of the form
% \begin{equation}
%     \label{eq:new-penalized-problem}
%     \begin{aligned}
%     &\min_{x \in \mathbb{R}^d} \,
%         \left\{
%             f^k(x) := f(x) + \gamma_k \pi^k(x)
%         \right\},
%     \end{aligned}
% \end{equation}
% where $(\gamma_k)_{k\in\N}$ is an unbounded sequence of positive numbers,
% and, for all $k\in\N$, the function $\pi^k:\R^d\to [0, \infty)$ is a function
% that is meant to penalize infeasible points, i.\,e. we want $\pi^k(x)$ to be small
% when $x$ is feasible, and large otherwise.
% We will refer to the parameters $(\gamma_k)_{k\in\N}$ as \textit{penalty parameters}
% and the functions $\pi^k$ as \textit{penalty functions}.
% Assuming that $\pi^k$ is convex for all $k\in\N$, there always exists
% a unique solution $x_k^\star$ of \eqref{eq:new-penalized-problem}.
% Then, a natural question to ask is:

% \textit{Under which conditions does the sequence of minimizers $(x_k^\star)_{k\in\N}$, corresponding
% to the sequence of functions $(f^k)_{k\in\N}$ defined in \eqref{eq:new-penalized-problem},
% converge to the solution $x^\star$ of \eqref{eq:new-model-problem}?}

% If $\lim_{k\to\infty} x_k^\star = x^\star$, we call the sequence $(x_k^\star)_{k\in\N}$ \textit{consistent}.
% Unsurprisingly, consistency depends on the properties of $(\pi^k)_{k\in\N}$ and $(\gamma_k)_{k\in\N}$,
% as well as $A(\xi)$ and $b(\xi)$ to establish existence of $x^\star$. We will investigate
% consistency in \cref{sec:consistency}.

% We will highlight two special examples of penalty functions that satisfy the conditions
% needed for consistency.
% \begin{example}[Huber-like penalty]
%     \label{ex:huber-like-penalty}
%     This penalty appears in \cite{nedic2020convergence}.
%     Let $(\delta_k)_{k\in\N}$ be a sequence of positive real numbers. For $k\in\N$, we define
%     the sequence of \textbf{Huber-like penalties} as
%     \[
%         \pi^k_\textnormal{hub}(x) := \E \left(\sum_{i=1}^{m} \pi_{\delta_k}(x; A_i(\xi), b_i(\xi)) \right),
%     \]
%     where, as in the previous example, $A_i(\xi)$ is the $i$-th
%     row of $A(\xi)$, $b_i(\xi)$ is the $i$-th row of $b(\xi)$, and
%     \[
%         \pi_{\delta}(x; a, b)
%         :=
%         \begin{cases}
%             \frac{\langle a, x \rangle + b}{|\!|a|\!|} \quad&\textnormal{if $\langle a, x \rangle + b > \delta$}, \\
%             \frac{(\langle a, x \rangle + b + \delta)^2}{4\delta |\!|a|\!|} \quad&\textnormal{if $-\delta \leq \langle a, x \rangle + b \leq \delta$}, \\
%             0, \quad&\textnormal{if $\langle a, x \rangle + b < -\delta$},
%         \end{cases}
%     \]
%     for $\delta \in (0,\infty)$, $x,a \in\R^d$, and $b\in\R$.
%     This penalty has some nice properties: It is convex, smooth, and has uniformly bounded gradients,
%     $\sup_{k\in\N} (\sup_{x\in\R^d} \nabla \pi^k(x)) < \infty$. On top of this,
%     the sequence $(\pi^k)_{k\in\N}$ converges to the penalty $x\mapsto \sum_{i=1}^m \max(0, A_i(\xi)x + b_i(\xi))$,
%     which has the highly desirable property of being an \textit{exact} penalty: \textcolor{red}{TODO} ...
% \end{example}

\section{Notation}

Since we only ever work with functions, which are
proper, closed, and convex, the subdifferentials
are nonempty and we may always
select a subgradient at a point $x\in\R^n$, which we will
generically denote by $\tilde{\nabla} f(x)\in\partial f(x)$.


% \section{Example Section}
% \subsection{Example Subsection}
% \begin{definition}{\textbf{(example definition)}}\\
% \lipsum[1]
% \end{definition}
% \begin{definition}{\textbf{(example definition)}}\\
% \lipsum[1]
% \end{definition}

% \begin{theorem}{\textbf{(example theorem)}}\\
% \lipsum[1]
% \end{theorem}

% \begin{proof}
% \lipsum[1]
% \end{proof}


% %Beispielgrafik einfügen
% \begin{figure}[ht]
%      \centering
%      \includegraphics[width=12cm]{pic/Example Pic.jpg}
%      \label{fig:abb1}
%     \caption[Figure 1]{Figure}
% \end{figure}


% \lipsum[2-6]

% \begin{align}
%     SR &= e^{H} \\
%     H &= -\sum _{i=1} ^{S} p_{i}ln \left( p_{i} \right) \\
%     p_{i} &= \frac{w_{i}}{w_{\text{total}}} \\
%     w &= e^{-d * I}
% \end{align}

% \cite{lederer2021activation}
% \lipsum[7-10]
% \cite{lederer2022fundamentals}