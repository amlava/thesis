\chapter{Stochastic Inexact Penalty Method}

In this chapter we will analyze the convergence
properties of \cref{alg:seqprox-sgd} applied to
the constrained stochastic optimization problem
\begin{equation}
    \label{eq:main-problem-copy}
    \tag{P}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^n} \, \{\, f(x) := \E(F_\xi(x)) + r(x) \,\} \\
    &\textup{s.\,t.} \quad A(\xi) x - b(\xi) \leq 0 \quad \textnormal{a.\,s.},
    \end{aligned}
\end{equation}
where we implicitly assume the existence of a probability space
$(\Omega, \mathcal{F}, \Prob)$ on which the random variable $\xi\colon\Omega\to\R^p$,
as well as the expected value mapping $\E(\cdot)$,
are defined.
Further, we endow the probability space with a filtration $\mathcal{F} := (\mathcal{F}_k)_{k\in\N}$
defined by
\[
    \mathcal{F}_k := \sigma(\xi_0,\dots,\xi_{k-1})
\]
and denote the conditional expectation given $\mathcal{F}_k$ as
\[
    \E_k(X) := \E(X \mid \mathcal{F}_k)
\]
for all $k\in\N$. Similarly, we write
\[
    \Var_k(X) := \E_k\norm{X - \E_k(X)}^2
\]
for the conditional variance given $\mathcal{F}_k$, for all $k\in\N$.
Note that the sequence of iterates $(x_k)_{k\in\N}$ generated by \cref{alg:seqprox-sgd}
is adapted to $\mathcal{F}$ and thus $\E_k(x_k) = x_k$ for all $k\in\N$.
We denote the feasible set for problem \eqref{eq:main-problem-copy} as
\[
    \mathcal{X} := \{\, x\in\dom(f) \mid A(\xi)x - b(\xi) \leq 0 \enspace \textnormal{a.\,s.} \,\},
\]
and the set of solutions as
\[
    \mathcal{X}^\star := \{\, x^\star\in\dom(f) \mid f(x^\star) \leq f(x) \, \forall x\in\mathcal{X} \,\}.
\]

\Cref{alg:seqprox-sgd} works with penalty functions $(\pi_k)_{k\in\N_0}$,
which induce the sequence of unconstrained problems
\begin{equation}
    \label{eq:main-problem-penalized}
    \tag{$\textnormal{P}^\textnormal{k}$}
    \min_{x\in\R^n}
    \left\{\, f_k(x) := f(x) + \gamma_k \pi_k(x) \,\right\},
\end{equation}
for $k\in\N_0$ and where $(\gamma_k)_{k\in\N_0}$ is a sequence of positive real numbers.
We assume that the penalty functions will always take the form
\[
    \pi_k(x) := \E(h_k(x; A(\xi), b(\xi))),
\]
where, for all $k\in\N_0$,
we let $(h_k)_{k\in\N_0}$ be any sequence of functions from
$\R^n \times \R^{m\times n} \times \R^m$ to $\R$ with the following properties:
\begin{enumerate}
    \item For all $k\in\N_0$, $h_k$ is convex and differentiable.
    \item $\pi_k(x) \geq \E\norm{(A(\xi)x - b(\xi))_+}_1$ for all $x\in\R^n$ and $k\in\N_0$.
    \item There exists a sequence $(\alpha_k)_{k\in\N_0}$ such that
    $\pi_k(\tilde{x}) \leq \alpha_k$ and $\sup_{k\in\N_0} \gamma_k\alpha_k < \infty$
    for all $k\in\N_0$ and all feasible points $\tilde{x}\in\mathcal{X}$.
    \item The gradients of $(h_k)_{k\in\N_0}$ have uniformly bounded gradient norm:
        \[
            \sup_{k\in\N_0}\sup_{x\in\R^n} \norm{\nabla h_k(x; A(\xi), b(\xi))} < \infty.
        \]
\end{enumerate}
Further, for $k\in\N_0$, we define the solution set of problem \eqref{eq:main-problem-penalized} as
\[
    \mathcal{X}^\star_k := \{\, x^\star_k\in\dom(f) \mid f_k(x^\star_k) \leq f(x) \, \forall x\in\R^n \,\}.
\]
Since proximal methods seperate the smooth part of the above objective,
given by $x\mapsto \E(F_\xi(x)) + \gamma_k \pi_k(x)$,
from the possibly nonsmooth part, $x\mapsto r(x)$, it is useful to also define the functions
\[
    \psi_k(x) := \E(F_\xi(x)) + \gamma_k \pi_k(x),
\]
for all $k\in\N_0$.

We will denote the iterates of \cref{alg:seqprox-sgd} by $(x_k)_{k\in\N}$.
The parameters $(w_k)_{k\in\N_0}$ are used to compute the final output - a weighted average
of the iterates of the first $K\in\N$ iterations - given by
\[
    \bar{x}_K := S_K^{-1} \sum_{k=1}^K w_k x_k,
\]
where
\[
    S_K := \sum_{k=1}^K w_k.
\]
Since this is a convex combination\footnote{as long as $w_1,\dots,w_K$ are nonnegative and at least one $w_k$ is positive.}, it follows that $\bar{x}_K \in \dom(f)$ if
$f$ is convex and $(x_k)_{k\in\N} \subset \dom(f)$ (notice that the first iterate is excluded).

In the analysis of \cref{alg:seqprox-sgd}, we will
need to control the variance of the error introduced by
using stochastic gradients. We define the \textbf{stochastic error of the $k$-th stochastic gradient}
of \cref{alg:seqprox-sgd}, $g_k$, as
\[
    z_k := \nabla \psi_k(x_k) - g_k,
\]
for $k\in\N_0$. Note that $\E_k(z_k) = 0$, by definition of $g_k$
and the aforementioned fact that $x_k$ is $\mathcal{F}_k$-measurable.

To ensure
% (among other things) that the sets $\mathcal{X},\mathcal{X}^\star,\mathcal{X}^\star_k$
% are nonempty for all $k\in\N_0$,
that the the problems \eqref{eq:main-problem-copy} and \eqref{eq:main-problem-penalized}
are well-behaved enough to analyze convergence
of \cref{alg:seqprox-sgd}, we will make the following basic assumptions.
\begin{assumption}
    \label{ass:basic-assumptions}
    Problem \eqref{eq:main-problem} satisfies the following:
    \begin{enumerate}
        \item The function $x\mapsto F_\xi(x)$ is almost surely $L$-smooth for some $L\in(0,\infty)$,
            and there exists a point $x\in\R^n$ such that $\E\norm{\nabla F_\xi(x)}^2 < \infty$.
            Further, the expectation $x\mapsto \E(F_\xi(x))$ is $\mu$-strongly convex for some
            $\mu\in[0, \infty)$.
            If $\mu = 0$, we additionally assume that $f$ has bounded lower-level sets.
        \item The function $r\colon\R^d\to\overline{\R}$ is proper, convex, and locally Lipschitz continuous on $\dom(r)$.
        \item The matrix-valued map $A\colon\R^p\to\R^{m\times n}$, and the vector valued
            map $b\colon\R^p\to\R^m$, are both (Borel-)measurable.
        \item The sequence $(\gamma_k)_{k\in\N_0}$ is nondecreasing and unbounded.
        \item The iterate weights $(w_k)_{k\in\N}$ are nonnegative and $w_k > 0$ for at least
            one $k\in\{\,1,\dots, K\,\}$, where $K\in\N$ is the number of iterations of \cref{alg:seqprox-sgd}.
        \item There exists at least one feasible point.
    \end{enumerate}
\end{assumption}

Notice that \textbf{we allow the strong convexity constant to take on the value zero},
because it allows for a unified treatment of both strongly convex and convex
objectives.
That is, the statement "$f$ is $\mu$-strongly convex with $\mu\in[0, \infty)$"
means "$f$ is either convex or $\mu$-strongly convex with $\mu\in(0, \infty)$".
For brevity, we will typically just say "$f$ is $\mu$-strongly convex", even
if $\mu = 0$ is allowed. In cases
where we need $\mu > 0$, we will make that clear.
If $\mu = 0$, the additional assumption of bounded lower-level sets
ensures that the family of sets $(\mathcal{X}_k^\star)_{k\in\N_0}$
is nonempty and uniformly bounded (see \cref{lem:basic-properties}).
Simple ways to guarantee bounded lower-level sets in the
(non-strongly) convex case are to choose $r$ such that $\dom(r)$
is compact, or by letting $r(x) = \lambda \norm{x}^2$ for some $\lambda > 0$.

\Cref{ass:basic-assumptions} has multiple useful implications, which are captured
by the following lemma.
\begin{lemma}
    \label{lem:basic-properties}
    \Cref{ass:basic-assumptions} implies the following:
    \begin{enumerate}
        \item The gradients of $(\pi_k)_{k\in\N_0}$ are uniformly bounded: There exists $G\in(0,\infty)$ such that
            \[
                \sup_{k\in\N_0} \sup_{x\in\R^n} \norm{\nabla \pi_k(x)} \leq G.
            \]
            In particular, $\pi_k$ is Lipschitz continuous for all $k\in\N$.
        \item The objective $f$ of \eqref{eq:main-problem-copy} is $\mu$-strongly convex with
            $\mu\in[0, \infty)$, locally
            Lipschitz continuous, and subdifferentiable on $\dom(f)$. Further,
            $f$ has bounded lower-level sets.
        \item The objectives $(f_k)_{k\in\N_0}$ of \eqref{eq:main-problem-penalized} are
        $\mu$-strongly convex with $\mu\in[0, \infty)$, locally
            Lipschitz continuous, and subdifferentiable on $\dom(f_k)$.
            Further, $f_k$ has bounded lower-level sets for all $k\in\N_0$.
        \item The functions $(\psi_k)_{k\in\N_0}$ are differentiable and $\mu$-strongly convex with
            $\mu\in[0, \infty)$.
        \item Let $B\subset\R^n$ be a bounded subset. Then, for all $k\in\N_0$, the stochastic gradients
        defined by
        \[
            g_k(x) := \nabla F_\xi(x) + \gamma_k \nabla h_k(x; A(\xi), b(\xi)),
        \]
        for $x\in\R^n$,
        satisfy $\sup_{x\in B} \norm{g_k(x)} = \mathcal{O}(\gamma_k)$ almost surely.
        \item The set $\mathcal{X}^\star$ is nonempty compact. For all $k\in\N_0$,
            the set $\mathcal{X}_k^\star$ is nonempty compact, and the family
            $(\mathcal{X}_k^\star)_{k\in\N_0}$ is uniformly bounded.
            In particular, the map $x\mapsto \Pi_\mathcal{X}(x)$ is bounded
            when restricted to the set $\cup_{k=0}^\infty \mathcal{X}_k^\star$.
        \item The iterates $(x_k)_{k\in\N}$ of \cref{alg:seqprox-sgd} (excluding $x_0$)
            are all included in $\dom(f)$. In particular,
            $\bar{x}_K \in \dom(f)$ for all $K\in\N$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item Per definition, for all $k\in\N_0$,
        \[
            \nabla \pi_k(x) = \nabla \E(h_k(x; A(\xi), b(\xi))) = \E(\nabla h_k(x; A(\xi), b(\xi))),
        \]
        hence
        \[
            \norm{\nabla \pi_k(x)} \leq \E\norm{\nabla h_k(x; A(\xi), b(\xi))},
        \]
        where the last step follows from Jensen's inequality.
        The claim follows by our assumption that
        \[
            \sup_{k\in\N_0}\sup_{x\in\R^n} \norm{\nabla h_k(x; A(\xi), b(\xi))} < \infty
        \]
        almost surely.
        In particular, $\pi_k$ is Lipschitz continuous for all $k\in\N_0$.
        \item By $L$-smoothness of $F_\xi$, the expectation $\E(F_\xi)$ is also $L$-smooth, thus
            locally Lipschitz. By local Lipschitz continuity of $r$, it follows that $f = \E(F_\xi) + r$ is also
            locally Lipschitz. Similarly, as the sum of a $\mu$-strongly convex function and a convex
            function, $f$ is $\mu$-strongly convex. Any proper, convex, and continuous function
            is subdifferentiable.
            Finally, $f$ has bounded lower-level sets per assumption, if $\mu = 0$,
            and by properties of strongly convex functions, if $\mu > 0$ (see \textcolor{red}{TODO: Ref a proposition}).
        \item By the above, $f$ is $\mu$-strongly convex, locally
            Lipschitz continuous, and subdifferentiable on $\dom(f)$. Combining this with the fact that
            $\pi_k$ is convex, Lipschitz continuous, and differentiable, as well as $\gamma_k > 0$,
            it follows that $f_k = f + \gamma_k \pi_k$
            is also $\mu$-strongly convex, locally
            Lipschitz continuous, and subdifferentiable on $\dom(f)$, for all $k\in\N_0$.
            Bounded lower-level sets follow from the same argument as above.
            Finally, the claim follows from
            $\dom(f) = \dom(f_k) \enspace \forall k\in\N_0$.
        \item For all $k\in\N_0$, $\psi_k$ is the sum of two differentiable functions, one $\mu$-strongly convex, the other convex.
            Hence, $\psi_k$ is $\mu$-strongly convex and differentiable for all $k\in\N_0$.
        \item Let $B\subset \R^n$ be bounded. By the triangle inequality,
            \[
                \norm{g_k(x)} \leq \norm{\nabla F_\xi(x)} + \gamma_k \norm{\nabla h_k(x; A(\xi), b(\xi))}.
            \]
            for all $x\in B$.
            The claim now follows from local Lipschitz continuity of $F_\xi$ (a.\,s.), and
            the assumption $\sup_{x\in B} \norm{\nabla h_k(x; A(\xi), b(\xi))} < \infty
            \enspace \text{(a.\,s.)} \enspace \forall k\in\N_0$.
        \item As we have already shown, $f$ is convex and has bounded lower-level sets.
            The set $\mathcal{X}$ is the intersection of the closed convex sets
            $\{\, x\in\dom(f) \mid A(z)x - b(z) \leq 0 \,\}$ over $z$ in the support of $\xi$.
            Thus, $\mathcal{X}$ is itself closed and convex, which implies that $f$
            attains a minimizer on $\mathcal{X}$ (\textcolor{red}{TODO: Ref a proposition}).
            For $k\in\N_0$, we have also shown that
            $f_k$ is convex and has bounded lower-level sets, which implies that $f_k$ has an
            unconstrained minimizer.
            Compactness of the solution sets
            follows from continuity and the bounded lower-level sets properties
            (\textcolor{red}{TODO: Ref a proposition}).
            To prove uniform boundedness of the family $(\mathcal{X}_k^\star)_{k\in\N_0}$,
            fix any feasible point $\hat{x}\in\mathcal{X}$ (which exists per \cref{ass:basic-assumptions}).
            By our assumptions on the penalty sequence $(\pi_k)_{k\in\N_0}$,
            there exists $c\in(0,\infty)$ such that, for all $k\in\N_0$ and $x_k^\star\in\mathcal{X}_k^\star$,
            \[
                f(x_k^\star) \leq f_k(x_k^\star) \leq f_k(\hat{x}) = f(\hat{x}) + \gamma_k\pi_k(\hat{x})
                \leq
                f(\hat{x}) + \gamma_k\alpha_k
                \leq
                f(\hat{x}) + c,
            \]
            where we used
            optimality of $x_k^\star$ for $f_k$.
            Hence,
            \[
                \mathcal{X}_k^\star \subset \{\, x\in\R^n \mid f(x) \leq f(\hat{x}) + c \,\} =: f_{\leq f(\hat{x}) + c}
            \]
            for all $k\in\N_0$. Coupled with the fact that $f$ has bounded lower-level sets,
            it follows that the family $(\mathcal{X}_k^\star)_{k\in\N_0}$ is uniformly bounded.
            In particular, since $f_{\leq f(\hat{x}) + c}$ is also closed,
            by continuity of $f$, and thus compact, it follows that the image of the set
            $f_{\leq f(\hat{x}) + c}$ under the continuous map $\Pi_{\mathcal{X}}$
            is compact. Therefore, we obtain
            \[
                \sup_{k\in\N} \sup_{x_k^\star \in \mathcal{X}_k^\star} 
                \norm{\Pi_{\mathcal{X}}(x_k^\star)}
                \leq
                \sup_{x\in f_{\leq f(\hat{x}) + c}} 
                \norm{\Pi_{\mathcal{X}}(x)}
                < \infty,
            \]
            as desired.
        \item By definition of the proximal operator,
            \[
                x_{k + 1} = \argmin_{x\in\R^d} \Big\{\, r(x) + \frac{1}{2\eta_k} \norm{x - (x_k - \eta_k g_k)}^2 \,\Big\},
            \]
            for all $k\in\N_0$.
            Since $r$ is proper, it must therefore hold that $x_{k} \in \dom(r) = \dom(f)$, for all $k\in\N$.
            By definition of $\bar{x}_K$ as a convex combination of the iterates $x_1,\dots,x_K$, $K\in\N$,
            and convexity of $f$, it follows that $\bar{x}_K \in \dom(f)$.
    \end{enumerate}
    This concludes the proof.
\end{proof}
The fact that $x_k^\star$ must not be
feasible introduces difficulties that prevent the use of
standard arguments from the SGD literature to analyse convergence. Our proof
methods combine approaches from recent works, mainly
the already mentioned \cite{nedich2023huber}, as well as
\cite{cutler2023stochastic}. The latter paper
investiages \textit{stochastic optimization problems under distributional drift}.
While these kinds of problems are not exactly the same as the ones we are
working with, the two settings do indeed exhibit striking resemblences.
Namely, Cutler et al. \cite{cutler2023stochastic} investigated a sequence of
time-dependent composite problems of the form
\[
    \min_{x\in\R^n} g_t(x) + r_t(x),
\]
where, for all $t\in\N$, $g_t$ is smooth strongly convex, and $r_t$
is convex. Comparing to our problem \eqref{eq:main-problem-penalized},
rewritten as 
\begin{equation}
    \label{eq:main-problem-penalized-copy-with-psi}
    \tag{$\textnormal{P}^\textnormal{k}$}
    \min_{x\in\R^n}
    \left\{\, f_k(x) = \psi_k(x) + r(x) \,\right\},
\end{equation}
we see that the two settings almost match, except for smoothness properties. Namely,
we do not need the family $(\psi_k)_{k\in\N}$ itself to be smooth, but only that
it is composed of an $L$-smooth function and a differentiable one
with uniformly bounded gradients. Even though it would be realistic to
assume Lipschitz smoothness of the penalties $\pi_k$, this would
lead to smoothness constants that blow up for $k\to\infty$, in order to
satisfy the convergence assumption $\lim_{k\to\infty} \pi_k = \pi_{\ell_1}$.
A naive application of the techniques in Cutler et al. \cite{cutler2023stochastic}
would then lead to a restriction on the step sizes $(\eta_k)_{k\in\N}$ of
the form $\eta_k \in (0, 1/L_k)$ with $L_k$ the smoothness constant of $\psi_k$.
With some care however, we manage to circumvent this restriction, as well as
the Lipschitz assumption on $\nabla \pi_k$, and only require $\eta_k\in(0, 1/L)$,
which allows for a lot more freedom in the choice of step sizes.
The resulting inequality can then be used
to adapt the proof strategy from NediÄ‡ et al. \cite{nedich2023huber}
(where the authors did not use proximal maps in their algorithm)
to our proximal method.

\section{Almost sure convergence}
\label{sec:almost-sure-convergence}

In this section we will establish
conditions under which we can guarantee almost sure convergence of the sequence of iterates $(x_k)_{k\in\N_0}$.
The proof will also yield convergence in expectation of
$(x_k)_{k\in\N_0}$ to $x^\star$ along a subsequence. The main use of the almost sure convergence
result will be that we will have conditions on the stepsizes $(\eta_k)_{k\in\N_0}$ and the
penalty parameters $(\gamma_k)_{k\in\N_0}$ to ensure that $(x_k)_{k\in\N_0}$ is bounded with
probability one. This, together with some of the results we prove along the way, will
come in very handy in the subsequent analysis of
the quantitative convergence rates of our methods.

The proof for almost sure convergence hinges on two technical lemmata. The first is the well-known
Robbins-Siegmund lemma, which provides a general sufficient condition to guarantee almost sure
convergence of so-called "almost supermartingales".
\begin{lemma}[Robbins-Siegmund]
    \label{lem:robbins-siegmund}
    Let $(\mathcal{F}_k)_{k\in\N}$ be an increasing sequence of $\sigma$-algebras and $v_k, a_k,
    b_k, c_k$ be nonnegative $\mathcal{F}_k$-measurable random variables. If, for all $k\in\N$,
    \begin{equation}
        \label{eq:lem:robbins-siegmund-almost-supermartingales}
        \E(v_{k+1} \mid \mathcal{F}_k) \leq v_k(1+a_k) + b_k - c_k,        
    \end{equation}
    and $\sum_{k=1}^\infty a_k < \infty, \sum_{k=1}^\infty b_k < \infty$ a.\,s., then with probability
    one, $(v_k)_{k\in\N}$ is convergent and it holds that $\sum_{k=1}^\infty c_k < \infty$.
\end{lemma}
\begin{proof}
    See \cite{robbins1971convergence}.
\end{proof}
Our goal for the rest of this section is to derive a recursive inequality for the
sequence $\norm{x_k - x^\star}^2$, which resembles \eqref{eq:lem:robbins-siegmund-almost-supermartingales}.
As a first step, we will analyze the convergence of the sequence $(x_k^\star)_{k\in\N_0}$.
In particular, we will establish that $\dist(x_k^\star, \mathcal{X})$ converges
to zero for $k\to\infty$, and that this convergence is independent of the sequence
$(\gamma_k)_{k\in\N_0}$, as long as one insures that $\gamma_k$
is eventually large enough.

To do this, we will first show that one can locally bound the distance $\dist(x,\mathcal{X})$
by (a term proportional to) the penalty $\pi_k(x)$.
We will rely on
an extension of a classic result by Hoffman \cite{hoffman2003approximate},
who analyzed the distance of points $x\in\R^n$ to the set of solutions of linear systems of inequalities
$Ax \leq b$ with $A\in\R^{m\times n}$, $b\in\R^m$.
Crucially, there always exists a constant $\tau\in(0,\infty)$, such that
\[
    \tau \dist(x, S) \leq \norm{(Ax - b)_+}_\infty,
\]
where $S := \{\, y\in\R^n \mid Ay \leq b \,\}$.
Since we are essentially dealing with infnitely inequality systems,
we cannot directly apply Hoffman's lemma. Instead,
we will use the theory of metric regularity,
which will lead us to a local version of the above bound.
Luckily, a local bound is all that is needed for our purposes and
only requires the following natural assumptions.
\begin{assumption}
    \label{ass:compact-support-without-zero}
    There exists a compact set $\Xi\in\R^p$ such that $\xi$ is supported
    on $\Xi$ and $\Prob^\xi$ admits a Lebesgue-density $q$ such that
    \begin{enumerate}
        \item $q$ is continuous on $\Xi$ and
        \item $q(x) > 0$ for all $x\in \Xi$.
    \end{enumerate}
    Further, we assume that
    \[
        \int_{\Xi} \norm{A(z)x - b(z)}_1 \, dz < \infty,
    \]
    for all $x\in\R^n$.
\end{assumption}
A simple sufficient condition for the last part of the assumption is continuity of the maps $A$ and $b$
on $\Xi$. One can always satisfy \cref{ass:compact-support-without-zero} if the set
$\Xi$ (or a superset thereof) is known, by sampling uniformly.
In fact, \cref{lem:lower-bound-on-penalties-by-distance-to-feasibility} will
demonstrate that uniform sampling is optimal, in the sense that
it minimizes constants that depend on the distribution of $\xi$ in the derived upper bound.
This is very intuitive: Since we are working in the setting of almost sure constraints,
no realization of $\xi$ is more important than any other.
However, it should be noted that this principle only applies if the set $\Xi$
is fixed and cannot be somehow reduced to a smaller set, as ideally one
would only need to sample from the set of maximizers of $z\mapsto A(z)x - b(z)$ on $\Xi$.
Nevertheless, in the case where such a reduction is possible, the optimal way
to sample would still be to sample uniformly, albeit from the reduced set.

The next assumption is a common one in the optimization literature.
\begin{assumption}
    \label{ass:slater-point}
    There exists a \textit{Slater point}, i.\,e. a point $x\in\mathcal{X}$ such that $A(\xi)x < b(\xi)$
    almost surely.
\end{assumption}
We will now formulate and prove the lemma.
\begin{lemma}[Subfeasibility bound via penalty functions]
    \label{lem:lower-bound-on-penalties-by-distance-to-feasibility}
    (\textcolor{red}{TODO: Not all of assumption 1 is needed here, should probably split
    the assumption up into two parts.})
    Assume \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point} hold
    and let $C\subset\R^n$ be a compact subset. Then
    there exists a constant $\tau\in(0,\infty)$ such that
    \[
       \tau \dist(x, \mathcal{X}) \leq \pi_k(x),
    \]
    for all $x\in C$ and $k\in\N_0$.
\end{lemma}
\begin{proof}
    We consider the Banach space $Y := L_1(\Xi, \R^m)$ equipped with the norm
    \[
        \norm{y}_{Y} := \int_{\Xi} \norm{y(z)}_1 \, dz.
    \]
    The distance map on $Y$, denoted $\dist_Y$, is then given by
    \[
        \dist_Y(y, S) := \inf_{y^\prime\in S} \norm{y - y^\prime}_Y
    \]
    for $y\in Y$ and $S\subset Y$.
    We define the multifunction $\varPsi\colon \R^n \to 2^{Y}$ as
    \[
        \varPsi(x) := \{\, y\in Y \mid A(\xi)x - b(\xi) \leq y(\xi) \text{ \,a.\,s.} \,\}.
    \]
    This multifunction is closed and convex. Indeed, let $(x_k)_{k\in\N} \in \R^n$
    be a sequence that converges to some $x\in\R^n$, and assume that there exists
    a sequence $(y_k)_{k\in\N}$ with $y_k\in\varPsi(x_k)$ for all $k\in\N$, such that
    $\lim_{k\to\infty} y_k = y \in Y$.
    Then, with probability one,
    \[
        A(\xi)x - b(\xi) = \lim_{k\to\infty} A(\xi)x_k - b(\xi) \leq \lim_{k\to\infty} y_k(\xi) = y(\xi).
    \]
    Hence $y\in\varPsi(x)$, which proves closedness.
    For convexity, let
    $x_1, x_2 \in \R^n$ and $t\in [0, 1]$. Then
    \begin{align*}
        t\varPsi(x_1) + (1-t)\varPsi(x_2)
        = \{\, y &\mid y = ty_1 + (1-t)y_2 \text{ for } y_1, y_2\in Y \\
                &\phantom{===} \text{ such that } A(\xi)x_1 - b(\xi) \leq y_1(\xi) \text{ \,a.\,s.} \\
                &\phantom{===} \text{ and } A(\xi)x_2 - b(\xi) \leq y_2(\xi) \text{ \,a.\,s.} \,\}.
    \end{align*}
    Let $x := t x_1 + (1-t) x_2$. Then, for any $y \in t\varPsi(x_1) + (1-t)\varPsi(x_2)$, there exist
    $y_1, y_2\in Y$, such that
    \[
        A(\xi)x - b(\xi) = t(A(\xi)x_1 - b(\xi)) + (1-t)(A(\xi)x_2 - b(\xi)) \leq t y_1(\xi) + (1-t)y_2(\xi) = y(\xi).
    \]
    Hence,
    \[
        t\varPsi(x_1) + (1-t)\varPsi(x_2) \subset \varPsi(t x_1 + (1-t) x_2),
    \]
    proving convexity.
    Now let $x_0\in C$ be some arbitrary point.
    Per definition of the inverse $\varPsi^{-1}$, it holds that
    \[
        \varPsi^{-1}(0) = \{\, x\in\R^n \mid 0 \in \varPsi(x) \,\} = \{\, x\in\R^n \mid A(\xi)x - b(\xi) \leq 0 \text{ \,a.\,s.} \,\}
        =
        \mathcal{X}.
    \]
    Hence we can write $\dist(x, \mathcal{X}) = \dist(x, \varPsi^{-1}(0))$.
    Note that \cref{ass:slater-point} implies $0 \in \interior(\range \varPsi )$. Therefore we
    can apply \cref{prop:robinson-ursescu}, which guarantees the existence of a constant
    $c\in\R_{\geq 0}$ such that
    \[
        \dist(x, \varPsi^{-1}(y)) \leq c \dist_Y(y, \varPsi(x)),
    \]
    for all $(x, y)$ in some open neighborhood $U \subset \R^n\times Y$ containing $(x_0, 0)$.
    In particular,
    \begin{equation}
        \label{eq:proof:lem:lower-bound-on-penalties-by-distance-to-feasibility-1}
        \dist(x, \mathcal{X}) \leq c \dist_Y(0, \varPsi(x))        
    \end{equation}
    for all $x$ in the open set $U \cap (\R^n\times \{0\})$. Since $x_0$ is arbitrary,
    we can derive a similar bound that holds around an open neighborhood $U_x \subset \R^n$ of a point $x\in C$, for any
    $x\in C$, yielding corresponding constants $(c_x)_{x\in C}$.
    By compactness of $C$, the open covering
    \[
        C \subset \bigcup_{x\in C} U_x
    \]
    has a finite subcovering
    \[
        C \subset \bigcup_{i=1}^\ell U_{x_i}
    \]
    with $(x_i)_{i\in\{\, 1,\dots, \ell \,\}} \subset C$. The corresponding constants
    $(c_{x_i})_{i\in\{\, 1,\dots, \ell \,\}}$ have a maximum $c := \max_{i\in\{\, 1,\dots, \ell \,\}} c_{x_i}$.
    Thus, we have shown that there exists $c\in\R_{\geq 0}$ such that
    \[
        \dist(x, \mathcal{X}) \leq c \dist_Y(0, \varPsi(x))
    \]
    for all $x\in C$. Next, we will show that
    \begin{equation}
        \label{eq:proof:lem:lower-bound-on-penalties-by-distance-to-feasibility-2}
        \dist_Y(0, \varPsi(x))
        = \int_{\Xi} \norm{(A(z)x - b(z))_+}_1 \, dz = \norm{(Ax - b)_+}_Y
    \end{equation}
    for any $x\in C$.
    Fix $x\in C$ and let $y\in \varPsi(x)$.
    By definition of $\varPsi(x)$ and positivity of $q$ on $\Xi$,
    it holds that $y\in \varPsi(x) \iff A(z)x - b(z) \leq y(z)$ for all $z\in \Xi$.
    Set
    \[
        \phi(z) := (A(z)x - b(z))_+
    \]
    for $z\in \Xi$. Clearly, $\phi \in \varPsi(x)$.
    We will show that $\norm{\phi(z)} \leq \norm{y(z)}$
    for all $z\in \Xi$. We denote by $\phi_i(z), a_i(z), b_i(z), y_i$, the $i$th
    row of $\phi(z), A(z), b(z), y$. We have
    \[
        |\phi_i(z)| = \begin{cases}
            0, &a_i (z)x - b_i(z) \leq 0 \\
            a_i (z)x - b_i(z), &\text{else}
        \end{cases}
    \]
    and thus
    \[
        |y_i| \geq \begin{cases}
            a_i(z)x - b_i(z), &\text{if } a_i(z)x - b_i(z) \geq 0, \\
            0, &\text{else.}
        \end{cases}
        \enspace =
        |\phi_i(z)|,
    \]
    for all $i\in\{\,1,\dots,m\,\}$.
    It follows that
    \begin{align*}
        \norm{\phi(z)}_1 &= \sum_{i=1}^m |\phi_i(z)| \\
                      &\leq \sum_{i=1}^m |y_i| \\
                      &= \norm{y}_1,
    \end{align*}
    and therefore,
    \[
        \inf_{y \in \varPsi(x)} \int_{\Xi} \norm{y}_1 \, dz
        \geq
        \int_{\Xi} \norm{\phi(z)} \, dz,
    \]
    proving \eqref{eq:proof:lem:lower-bound-on-penalties-by-distance-to-feasibility-2}.
    To finish the proof, we need to establish a relationship between
    \eqref{eq:proof:lem:lower-bound-on-penalties-by-distance-to-feasibility-2}
    and $\pi_k(x)$.
    By compactness of $\Xi$ and continuity of $q$, the image $q(\Xi)$
    must be compact.
    In particular, since $q(x) > 0$ for all $x\in \Xi$, there must
    exist some uniform positive lower bound $c_q \in (0, \infty)$ such that $q(x) \geq c_q$
    for all $x\in \Xi$. If we denote the Lebesgue-measure by $\lambda$, we see that
    \[
        \Prob^\xi(A) = \int_{A} q(z) \, dz \geq c_q \lambda (A),
    \]
    for all measurable $A \subset \Xi$.
    Therefore, $\Prob^\xi$ and $\lambda$ are equivalent on $\Xi$, and
    $q^{-1}$ is a $\Prob^\xi$-density of $\lambda$.
    We thus have
    \begin{align*}
        \norm{y}_Y &= \int_{z\in \Xi} \norm{y(z)}_1 \, dz \\
        &= \int_{z\in \Xi} \norm{y(z)}_1 q^{-1}(z) \, \Prob^\xi(dz) \\
        &\leq c_q^{-1} \int_{z\in \Xi} \norm{y(z)}_1 \, \Prob^\xi(dz) \\
        &= c_q^{-1} \int \norm{y(\xi)}_1 \, d\Prob \\
        &= c_q^{-1} \, \E\norm{y(\xi)}_1.
    \end{align*}
    Combining with \eqref{eq:proof:lem:lower-bound-on-penalties-by-distance-to-feasibility-1}
    and \eqref{eq:proof:lem:lower-bound-on-penalties-by-distance-to-feasibility-2}, we
    obtain
    \[
        \dist(x, \mathcal{X}) \leq c c_q^{-1} \E\norm{(A(\xi)x - b(\xi))_+}_1.
    \]
    The claim follows after setting $\tau := c^{-1} c_q$
    and applying one of the defining properties of $(\pi_k)_{k\in\N_0}$.
\end{proof}
\begin{theorem}[Convergence of $x_k^\star$]
    \label{thm:distance-to-feasibility-bound}
    Assume \cref{ass:basic-assumptions,ass:slater-point,ass:compact-support-without-zero} hold
    and let $x^\star\in\mathcal{X}^\star$.
    Then, there exist constants $M, \tau\in (0,\infty)$, such that
    \[
        \frac{\mu}{2} |\!|x^\star - x_k^\star|\!|^2 + \frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2
        + (\tau\gamma_k - M) \, \dist(x_k^\star, \mathcal{X})
        \leq
        \gamma_k \alpha_k,
    \]
    for all $k\in\N_0$ and $x_k^\star\in\mathcal{X}^\star_k$.
    In particular,
    \[
        \dist(x_k^\star, \mathcal{X}) = \mathcal{O}(\alpha_k)
    \]
    and, if $\mu > 0$, we further have
    \[
        |\!|x^\star - x_k^\star|\!|^2 = \mathcal{O}(\gamma_k \alpha_k).
    \]
\end{theorem}
\begin{proof}
    Let $k\in\N_0$ and $x_k^\star\in\mathcal{X}_k^\star$. By optimality of $x_k^\star$ for $f_k$, there exists
    $0 \in \partial f_k(x_k^\star)$. Hence, by
    strong convexity, we obtain
    \begin{equation}
        \label{proof:thm:surrogate-bound-for-huber-penalty:eq1}
        \frac{\mu}{2} |\!|x^\star - x_k^\star|\!|^2
        \leq f_k(x^\star) - f_k(x_k^\star)
        = f(x^\star) - f(x_k^\star) + \gamma_k\pi_k(x^\star) - \gamma_k\pi_k(x_k^\star).
    \end{equation}
    We can write
    \begin{align*}
        f(x^\star) - f(x_k^\star) &= f(x^\star) - f(\proj(x_k^\star)) + f(\proj(x_k^\star)) - f(x_k^\star) \\
                                  &\leq -\frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2
                                    - \langle \tilde{\nabla} f(x^\star), \proj(x_k^\star) - x^\star \rangle + f(\proj(x_k^\star)) - f(x_k^\star), 
    \end{align*}
    where we again used strong convexity in the second step.
    Since $\proj(x_k^\star)\in\mathcal{X}$ and $x^\star$ is optimal for $f$ on $\mathcal{X}$,
    it holds that
    \[
        \langle \tilde{\nabla} f(x^\star), \proj(x_k^\star) - x^\star \rangle \geq 0,
    \]
    and thus
    \[
        f(x^\star) - f(x_k^\star) \leq -\frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2 + f(\proj(x_k^\star)) - f(x_k^\star).
    \]
    By \cref{lem:basic-properties}, we know that the
    family $(\mathcal{X}_k^\star)_{k\in\N_0}$ is uniformly bounded and
    \[
        \sup_{k\in\N_0} \sup_{x_k^\star\in\mathcal{X}_k^\star} \norm{\Pi_\mathcal{X}(x_k^\star)} < \infty.
    \]
    Hence, by
    local Lipschitz continuity of $f$ (\cref{lem:basic-properties}) and Cauchy-Schwarz,
    there exists a constant $M\in (0,\infty)$, such that
    \[
        f(\proj(x_k^\star)) - f(x_k^\star) \leq M \dist(x_k^\star, \mathcal{X}),
    \]
    for all $k\in\N_0$ and $x_k^\star \in \mathcal{X}_k^\star$.
    We now obain
    \[
        f(x^\star) - f(x_k^\star) \leq -\frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2 + M \dist(x_k^\star, \mathcal{X}).
    \]
    Plugging this into \eqref{proof:thm:surrogate-bound-for-huber-penalty:eq1}, we obtain
    \[
        \frac{\mu}{2} |\!|x^\star - x_k^\star|\!|^2
        \leq
        -\frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2 + M \dist(x_k^\star, \mathcal{X})
        + \gamma_k\pi_k(x^\star) - \gamma_k\pi_k(x_k^\star).
    \]
    Now, using our lower bound on $\pi_k(x_k^\star)$ from \cref{lem:lower-bound-on-penalties-by-distance-to-feasibility},
    and combining terms, we arrive at the inequality
    \[
        \frac{\mu}{2} |\!|x^\star - x_k^\star|\!|^2
        \leq
        -\frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2 + (M - \gamma_k\tau) \dist(x_k^\star, \mathcal{X})
        + \gamma_k\pi_k(x^\star).
    \]
    Using $\pi_k(x^\star) \leq \alpha_k$ and rearranging, we obtain
    \[
        \frac{\mu}{2} |\!|x^\star - x_k^\star|\!|^2
        +
        \frac{\mu}{2} |\!|x^\star - \proj(x_k^\star)|\!|^2
        +
        (\gamma_k\tau - M) \dist(x_k^\star, \mathcal{X})
        \leq
        \gamma_k \alpha_k.
    \]
    The asymptotic rate for $|\!|x^\star - x_k^\star|\!|^2$ now follows
    in the case $\mu > 0$.
    For the bound on $\dist(x_k^\star, \mathcal{X})$, we let $K$ be large enough such that
    $\gamma_k\tau > M$ for all $k \geq K$. Dividing by $\gamma_k$ on both sides and using the nonnegativity
    of the other terms on the left-hand side, we get
    \[
        c \cdot \dist(x_k^\star, \mathcal{X})
        \leq
        \frac{\gamma_k\tau - M}{\gamma_k} \dist(x_k^\star, \mathcal{X})
        \leq
        \alpha_k,
    \]
    for all $k \geq K$ and some constant $c\in(0,1)$, as desired.
\end{proof}
Having established the convergence of the sequences $(x_k^\star)_{k\in\N_0}$
to the feasible set $\mathcal{X}$, we will now shift our attention to the iterates $(x_k)_{k\in\N_0}$
of \cref{alg:seqprox-sgd}.
We begin with the following
fundamental recursive inequality.
\begin{lemma}[One-step improvement]
    \label{lem:one-step-improvement}
    Let \cref{ass:basic-assumptions} hold and
    let $\rho\in(0,1)$ and $\eta_k\in(0, \rho L^{-1}]$ for all $k\in\N$. Then
    the iterates $(x_k)_{k\in\N_0}$ generated by \cref{alg:seqprox-sgd} with step size
    schedule $(\eta_k)_{k\in\N_0}$ satisfy
    \begin{align}
        2\eta_k(f_k(x_{k+1}) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} +  \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2
    \end{align}
    almost surely, for all $x\in\R^n$ and $k\in\N_0$.
\end{lemma}
\begin{proof}
    For $k\in\N_0$ we denote by $g_k$ the stochastic gradient of $\psi_k$ at $x_k$
    that is used in iteration $k$ of \cref{alg:seqprox-sgd}.
    We also let $\psi(x) := \E(F_\xi(x))$ for $x\in\R^n$, so that
    $\psi_k = \psi + \gamma_k\pi_k$ and
    $\nabla \psi_k = \nabla \psi + \gamma_k \nabla\pi_k$.
    By $L$-smoothness of $\psi$ (\cref{ass:basic-assumptions}),
    we have
    \begin{align*}
        f_k(x_{k+1})
        &= \psi_k(x_{k+1}) + r(x_{k+1}) \\
        &= \psi(x_{k+1}) + \gamma_k\pi_k(x_{k+1}) + r(x_{k+1}) \\
        &\leq \psi(x_k) + \langle \nabla \psi(x_k), x_{k+1} - x_k \rangle + \frac{L}{2} \norm{x_{k+1} - x_k}^2 + \gamma_k\pi_k(x_{k+1}) + r(x_{k+1}) \\
        &= \psi(x_k) + \langle \nabla \psi_k(x_k), x_{k+1} - x_k \rangle + \frac{L}{2} \norm{x_{k+1} - x_k}^2
            + \gamma_k\pi_k(x_{k+1}) + r(x_{k+1}) \\
        &\phantom{===}+ \gamma_k \langle \nabla \pi_k(x_k), x_k - x_{k+1} \rangle.
    \end{align*}
    By convexity of $\pi_k$, we further have
    \[
        \pi_k(x_{k+1}) \leq \pi_k(x_k) + \langle \nabla \pi_k(x_{k+1}), x_{k+1} - x_k \rangle,
    \]
    and an application of Cauchy-Schwarz and Young's inequality yields
    \[
        \pi_k(x_{k+1})
        \leq \pi_k(x_k) + \frac{\epsilon_k^{-1}}{2} \norm{\nabla \pi_k(x_{k+1})}^2 + \frac{\epsilon_k}{2} \norm{x_{k+1} - x_k}^2,
    \]
    for any $\epsilon_k \in (0,\infty)$.
    Note that the gradients $(\nabla \pi_k)_{k\in\N_0}$ are bounded uniformly (\cref{lem:basic-properties}),
    hence there exists $G\in(0,\infty)$ such that
    \[
        \pi_k(x_{k+1})
        \leq \pi_k(x_k) + \frac{\epsilon_k}{2} G^2 + \frac{\epsilon_k^{-1}}{2} \norm{x_{k+1} - x_k}^2,
    \]
    for any $\epsilon_k\in(0,\infty)$.
    With this, we can further bound $f_k(x_{k+1})$ by
    \begin{align*}
        f_k(x_{k+1})
        &\leq \psi(x_k) + \langle \nabla \psi_k(x_k), x_{k+1} - x_k \rangle + \frac{L}{2} \norm{x_{k+1} - x_k}^2 + r(x_{k+1}) \\
        &\phantom{===}+ \gamma_k\Big( \pi_k(x_k) + \frac{\epsilon_k}{2} G^2 + \frac{\epsilon_k^{-1}}{2} \norm{x_{k+1} - x_k}^2 \Big)
            + \gamma_k \langle \nabla \pi_k(x_k), x_k - x_{k+1} \rangle. \\
        &= \psi_k(x_k) + \langle \nabla \psi_k(x_k), x_{k+1} - x_k \rangle + \frac{L + \gamma_k\epsilon_k^{-1}}{2} \norm{x_{k+1} - x_k}^2 + r(x_{k+1}) \\
        &\phantom{===} + \frac{\gamma_k \epsilon_k G^2}{2} + \gamma_k \langle \nabla \pi_k(x_k), x_k - x_{k+1} \rangle.
    \end{align*}
    By another application of Cauchy-Schwarz and Young's inequality, we have for any $\epsilon_k\in(0,\infty)$
    \[
         \langle \nabla \pi_k(x_k), x_k - x_{k+1} \rangle \leq \frac{\epsilon_k}{2} G^2 + \frac{\epsilon_k^{-1}}{2} \norm{x_{k+1} - x_k}^2,
    \]
    where again used the bound $\sup_{k\in\N_0}\sup_{x\in\R^n} \norm{\nabla \pi_k(x)} \leq G$, and therefore we obtain
    \begin{align*}
        f_k(x_{k+1})
        &\leq \psi_k(x_k) + \langle \nabla \psi_k(x_k), x_{k+1} - x_k \rangle + \frac{L + 2\gamma_k\epsilon_k^{-1}}{2} \norm{x_{k+1} - x_k}^2 + r(x_{k+1}) \\
        &\phantom{===} + \gamma_k \epsilon_k G^2
    \end{align*}
    for any $\epsilon_k\in(0,\infty)$. The rest of the proof follows
    the strategy of Cutler et. al. \cite{cutler2023stochastic}, lemma 2. Recall the
    error in the $k$-th stochastic gradient, $z_k = \nabla \psi_k(x_k) - g_k$.
    By adding and subtracting $\langle z_k, x_{k+1} - x_k \rangle$ in
    the above inequality, we get
    \begin{align*}
        f_k(x_{k+1})
        &\leq \psi_k(x_k) + \langle g_k, x_{k+1} - x_k \rangle + \frac{L + 2\gamma_k\epsilon_k^{-1}}{2} \norm{x_{k+1} - x_k}^2 + r(x_{k+1}) \\
        &\phantom{===} + \gamma_k \epsilon_k G^2 + \langle z_k, x_{k+1} - x_k \rangle.
    \end{align*}
    By yet another application of Cauchy-Schwarz and Young's inequality, we have
    \[
        \langle z_k, x_{k+1} - x_k \rangle
        \leq
        \frac{\delta_k}{2}\norm{z_k}^2 + \frac{\delta_k^{-1}}{2}\norm{x_{k+1} - x_k}^2,
    \]
    for all $\delta_k\in(0,\infty)$, and therefore
    \begin{align*}
        f_k(x_{k+1})
        &\leq \psi_k(x_k) + \langle g_k, x_{k+1} - x_k \rangle
            + \frac{L + 2\gamma_k\epsilon_k^{-1} + \delta_k^{-1}}{2} \norm{x_{k+1} - x_k}^2 + r(x_{k+1}) \\
        &\phantom{===} + \gamma_k \epsilon_k G^2 + \frac{\delta_k}{2} \norm{z_k}^2, \\
        &= \psi_k(x_k) + r(x_{k+1}) + \langle g_k, x_{k+1} - x_k \rangle + \frac{1}{2\eta_k}\norm{x_{k+1} - x_k}^2 \\
        &\phantom{===} + \frac{L + 2\gamma_k\epsilon_k^{-1} + \delta_k^{-1} - \eta_k^{-1}}{2} \norm{x_{k+1} - x_k}^2 + \gamma_k \epsilon_k G^2 + \frac{\delta_k}{2} \norm{z_k}^2,
    \end{align*}
    where in the last step we added and subtracted $(2\eta_k)^{-1} \norm{x_{k+1} - x_k}^2$ and moved $r(x_{k+1})$
    further forward.
    From the definition of the proximal operator, it follows that
    \begin{align*}
        x_{k+1} &= \prox_{\eta_k r} (x_k - \eta_k g_k) \\
                &= \argmin_{x\in\R^n} \Big\{\, r(x) + \frac{1}{2\eta_k}\norm{x - (x_k - \eta_k g_k)}^2 \,\Big\} \\
                &= \argmin_{x\in\R^n} \Big\{\, r(x) + \langle g_k, x - x_k \rangle + \frac{1}{2\eta_k}\norm{x - x_k}^2 \,\Big\},
    \end{align*}
    where the last step follows from expanding the square and dropping
    the constant term $\eta_k^2 \norm{g_k}^2$ from the minimization.
    The function $x\mapsto r(x) + \langle g_k, x - x_k \rangle + (2\eta_k)^{-1}\norm{x - x_k}^2$
    is $(2\eta_k)^{-1}$-strongly convex and minimized by $x_{k+1}$. Thus,
    comparing with our previous bound on $f_k(x_{k+1})$,
    we can conclude
    \begin{align*}
        f_k(x_{k+1})
        &\leq \psi_k(x_k) + r(x) + \langle g_k, x - x_k \rangle + \frac{1}{2\eta_k}\norm{x - x_k}^2
            - \frac{1}{2\eta_k}\norm{x - x_{k+1}}^2 \\
        &\phantom{===} + \frac{L + 2\gamma_k\epsilon_k^{-1} + \delta_k^{-1} - \eta_k^{-1}}{2} \norm{x_{k+1} - x_k}^2
            + \gamma_k \epsilon_k G^2 + \frac{\delta_k}{2} \norm{z_k}^2,
    \end{align*}
    for all $x\in\R^n$. By $\mu$-strong convexity of $\psi_k$, we have
    \begin{align*}
        \psi_k(x_k) + r(x) + \langle g_k, x - x_k \rangle
        &=
        \psi_k(x_k) + r(x) + \langle \nabla \psi_k(x_k), x - x_k \rangle + \langle z_k, x_k - x \rangle \\
        &\leq
        \psi_k(x) - \frac{\mu}{2}\norm{x - x_k}^2 + r(x) + \langle z_k, x_k - x \rangle \\
        &=
        f_k(x) - \frac{\mu}{2}\norm{x - x_k}^2 + \langle z_k, x_k - x \rangle,
    \end{align*}
    for all $x\in\R^n$. Hence,
    \begin{align}
        \label{eq:proof:lem:one-step-improvement}
        f_k(x_{k+1})
        &\leq f_k(x) + \Big( \frac{1}{2\eta_k} - \frac{\mu}{2} \Big) \norm{x - x_k}^2 + \langle z_k, x_k - x \rangle 
            - \frac{1}{2\eta_k}\norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + \frac{L + 2\gamma_k\epsilon_k^{-1} + \delta_k^{-1} - \eta_k^{-1}}{2} \norm{x_{k+1} - x_k}^2
            + \gamma_k \epsilon_k G^2 + \frac{\delta_k}{2} \norm{z_k}^2.
    \end{align}
    Fix $\alpha \in (0, 1 - \rho)$ and define $\epsilon_k := 2\alpha^{-1}\eta_k\gamma_k$. Then
    \[
        (L + 2\gamma_k\epsilon_k^{-1})\eta_k
        = \Big( L + 2\gamma_k \frac{\alpha}{2 \eta_k \gamma_k} \Big)\eta_k
        = L\eta_k + \alpha \leq \rho + \alpha < 1,
    \]
    where we used that $\eta_k \leq \rho L^{-1}$, which holds per assumption.
    Choosing
    \[
        \delta_k := \frac{\eta_k}{1 - (L\eta_k + \alpha)} \in (0,\infty)
    \]
    therefore yields
    \[
        L + 2\gamma_k\epsilon_k^{-1} + \delta_k^{-1} - \eta_k^{-1}
        = L + \frac{\alpha}{\eta_k} - \frac{1 - (L\eta_k + \alpha)}{\eta_k} - \frac{1}{\eta_k}
        = 0.
    \]
    Hence, we can drop the $\norm{x_{k+1} - x_k}^2$ term from \eqref{eq:proof:lem:one-step-improvement} and get
    \begin{align*}
        f_k(x_{k+1})
        &\leq f_k(x) + \Big(\frac{1}{2\eta_k} - \frac{\mu}{2}\Big)\norm{x - x_k}^2 + \langle z_k, x_k - x \rangle 
            - \frac{1}{2\eta_k}\norm{x - x_{k+1}}^2 \\
        &\phantom{===} + 2 \alpha^{-1} \eta_k \gamma_k^2 G^2 + \frac{\eta_k}{2(1 - (L\eta_k + \alpha))}\norm{z_k}^2.
    \end{align*}
    Subtracting by $f_k(x)$ and multiplying both sides by $2\eta_k$
    yields
    \begin{align*}
        2\eta_k(f_k(x_{k+1}) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \\
        &\phantom{===} + 4 \alpha^{-1} \eta_k^2 \gamma_k^2 G^2 + \frac{\eta_k^2}{1 - (L\eta_k + \alpha)}\norm{z_k}^2.
    \end{align*}
    For the specific choice $\alpha := (1 - \rho)/2$, it holds that $L\eta_k + \alpha \leq \rho + (1 - \rho)/2 = (1 + \rho)/2$.
    Hence, we arrive at
    \begin{align*}
        %\label{eq:proof:lem:one-step-improvement-2}
        2\eta_k(f_k(x_{k+1}) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} +  \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2,
    \end{align*}
    as desired.
\end{proof}
After rearranging the inequality from \cref{lem:one-step-improvement}, setting $x = x^\star$, and
dropping the $-\mu\eta_k$ term, we get
\begin{align*}
    \norm{x^\star - x_{k+1}}^2
    &\leq \norm{x^\star - x_k}^2 + 2\eta_k\langle z_k, x_k - x^\star \rangle \notag + 16 G^2\eta_k^2\gamma_k^2 + 4\eta_k^2\norm{z_k}^2 \\
    &\phantom{===} - 2\eta_k(f_k(x_{k+1}) - f_k(x^\star)),
\end{align*}
provided $\eta_k \leq 1/(2L)$.
By properties of conditional expectation and the definition of
stochastic gradients, we know that $\E_k \langle z_k, x_k - x \rangle = \langle \E_k(z_k), x_k - x \rangle = 0$.
Further note that
$\E_k\norm{z_k}^2 = \E_k\norm{g_k - \nabla \psi_k(x_k)}^2$ and
$\E_k(g_k) = \nabla \psi_k(x_k)$ per definition of $g_k$, hence $\E_k\norm{z_k}^2 = \Var_k(g_k)$.
Therefore, after applying conditional expectations to both sides of the above
inequality, we obtain
% \begin{align*}
%     2\eta_k\E_k(f_k(x_{k+1}) - f_k(x))
%     &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 - \E_k\norm{x - x_{k+1}}^2 \\
%     &\phantom{===} + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} + \frac{2\eta_k^2}{1 - \rho}\E_k\norm{z_k}^2.
% \end{align*}
% for all $x\in\R^n$.
% Since $\E_k\norm{z_k}^2 = \E_k\norm{g_k - \nabla \psi_k(x_k)}^2$ and
% $\E_k(g_k) = \nabla \psi_k(x_k)$ per definition of $g_k$, it holds that $\E_k\norm{z_k}^2 = \Var_k(g_k)$.
% We have thus arrived at the desired result.
% After applying conditional expectation $\E_k(\cdot)$ to both sides,
% we get
\begin{align*}
    %\label{eq:sec:almost-sure-convergence}
    \E_k\norm{x^\star - x_{k+1}}^2
    &\leq \norm{x^\star - x_k}^2 + 16 G^2\eta_k^2\gamma_k^2 + 4\eta_k^2\Var_k(g_k) \notag \\
    &\phantom{===} - 2\eta_k\E_k(f_k(x_{k+1}) - f_k(x^\star)).
\end{align*}
This is not yet quite in the form needed to apply \cref{lem:robbins-siegmund}. For one,
we need to bound the noise term $\Var_k(g_k)$. Second, we cannot
in general guarantee that $\E_k(f_k(x_{k+1}) - f_k(x^\star)) \geq 0$.
However, as we show in the next lemma, we can find a lower bound that
involves a nonnegative term plus a small negative term.
\begin{lemma}
    \label{lem:recursion-for-almost-sure-convergence}
    Assume that \cref{ass:basic-assumptions,ass:slater-point,ass:compact-support-without-zero} hold, and
    let $\eta_k\in(0,\rho L^{-1}]$ for all $k\in\N_0$ and some $\rho\in(0,1)$. Then the iterates
    $(x_k)_{k\in\N_0}$ generated
    by \cref{alg:seqprox-sgd} with step size schedule $(\eta_k)_{k\in\N_0}$ satisfy
    \begin{align*}
        \norm{x^\star - x_{k+1}}^2
        &\leq ( 1 - \mu\eta_k ) \norm{x^\star - x_k}^2 + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2 + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} \\
        &\phantom{===} + 2\eta_k\langle z_k, x_k - x^\star \rangle + \mathcal{O}(\eta_k\gamma_k\alpha_k) - 2\eta_k ( f_k(x_{k+1}) - f_k(x_k^\star) )
    \end{align*}
    almost surely, for all $k\in\N_0$.
\end{lemma}
\begin{proof}
    Let $k\in\N_0$. From \cref{lem:one-step-improvement}, we have
    \begin{align*}
        2\eta_k(f_k(x_{k+1}) - f_k(x^\star))
        &\leq ( 1 - \mu\eta_k ) \norm{x^\star - x_k}^2 + 2\eta_k\langle z_k, x_k - x^\star \rangle 
            - \norm{x^\star - x_{k+1}}^2 \\
        &\phantom{===} +  \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2
    \end{align*}
    almost surely.
    Write
    \begin{align}
        \label{eq:proof:lem:recursion-for-almost-sure-convergence}
        f_k(x_{k+1}) - f_k(x^\star)
        &=
       (f_k(x_{k+1}) - f_k(x_k^\star)) + (f_k(x_k^\star) - f_k(\proj(x_k^\star))) \\
        &\phantom{===}+ (f_k(\proj(x_k^\star)) - f_k(x^\star)). \notag
    \end{align}
    By definition, it holds that
    \begin{align*}
        f_k(\proj(x_k^\star)) - f_k(x^\star)
        &=
        f(\proj(x_k^\star)) + \gamma_k \pi_k(\proj(x_k^\star)) - f(x^\star) - \gamma_k \pi_k(x^\star) \\
        &\geq
        f(\proj(x_k^\star)) - f(x^\star) - \gamma_k \pi_k(x^\star) \\
        &\geq
        - \gamma_k \pi_k(x^\star),
    \end{align*}
    where the last step follows from the fact that $x^\star$ minimizes $f$ on $\mathcal{X}$
    and $\proj(x_k^\star)\in\mathcal{X}$. Further, feasibility implies $\pi_k(x^\star) \leq \alpha_k$.
    Hence, combining with \eqref{eq:proof:lem:recursion-for-almost-sure-convergence}, we have
    \[
        f_k(x_{k+1}) - f_k(x^\star) \geq (f_k(x_{k+1}) - f_k(x_k^\star)) + f_k(x_k^\star) - f_k(\proj(x_k^\star)) - \gamma_k\alpha_k.
    \]
    For the next steps, we let $\tilde{\nabla} f(x)$ denote the minimum-norm subgradient
    in $\partial f(x)$, for all $x\in\dom(f)$.
    To analyze $f_k(x_k^\star) - f_k(\proj(x_k^\star))$, we first use convexity and Cauchy-Schwarz to get
    \[
        f_k(x_k^\star) - f_k(\proj(x_k^\star))
        \geq \langle \tilde{\nabla} f_k(\proj(x_k^\star)), x_k^\star - \proj(x_k^\star) \rangle
        \geq -\norm{\tilde{\nabla} f_k(\proj(x_k^\star))} \dist(x_k^\star, \mathcal{X}).
    \]
    The sequence $(x_k^\star)_{k\in\N}$ converges to $x^\star$, which implies, by continuity
    of the projection map, $\lim_{k\to\infty} \proj(x_k^\star) = \proj(x^\star) = x^\star$.
    In particular, $(\proj(x_k^\star))_{k\in\N}$ is bounded, so \cref{lem:basic-properties} implies
    that there exists $K_1\in\N$ such that $\sup_{k\in\N} \norm{\tilde{\nabla} f_k(\proj(x_k^\star))}
    \leq c_1 \gamma_k$ for some $c_1\in(0,\infty)$ and all $k\geq K_1$. Hence,
    \[
        f_k(x_k^\star) - f_k(\proj(x_k^\star)) \geq -c_1 \gamma_k\, \dist(x_k^\star,\mathcal{X}),
    \]
    for all $k\geq K_1$.
    By \cref{thm:distance-to-feasibility-bound}, it holds that there exist
    $K_2\in\N$ and $c_2\in(0,\infty)$ such that, for all $k\geq K_2$, $\dist(x_k^\star,\mathcal{X}) \leq c_2 \, \alpha_k$.
    Setting $K := \max(K_1, K_2)$, it therefore holds that
    \[
        f_k(x_k^\star) - f_k(\proj(x_k^\star)) \geq -c_1c_2 \cdot \gamma_k \alpha_k
    \]
    and
    \begin{align*}
        f_k(x_{k+1}) - f_k(x^\star)
        &\geq
        f_k(x_{k+1}) - f_k(x_k^\star)
        -c_1c_2\cdot \gamma_k\alpha_k - \gamma_k\alpha_k \\
        &=
        f_k(x_{k+1}) - f_k(x_k^\star)
        -(1 + c_1c_2) \, \gamma_k\alpha_k,
    \end{align*}
    for all $k\geq K$.
    Plugging this into our original estimate, we have
    \begin{align*}
        2\eta_k\big(f_k(x_{k+1}) - f_k(x_k^\star) - \mathcal{O}(1)\gamma_k\alpha_k\big)
        &\leq ( 1 - \mu\eta_k ) \norm{x^\star - x_k}^2
            - \norm{x^\star - x_{k+1}}^2  \\
        &\phantom{===} +  \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2 + 2\eta_k\langle z_k, x_k - x^\star \rangle
    \end{align*}
    almost surely. Hence
    \begin{align*}
        2\eta_k ( f_k(x_{k+1}) - f_k(x_k^\star) )
        &\leq ( 1 - \mu\eta_k ) \norm{x^\star - x_k}^2
            - \norm{x^\star - x_{k+1}}^2 + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho}  \\
        &\phantom{===} + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2 + 2\eta_k\langle z_k, x_k - x^\star \rangle + \mathcal{O}(1)\eta_k\gamma_k\alpha_k
    \end{align*}
    almost surely, for all $k\geq K$. The claim follows.
\end{proof}
From the above lemma, we can derive
\begin{align*}
    \E_k\norm{x^\star - x_{k+1}}^2
    &\leq
    \norm{x^\star - x_k}^2
    + \frac{2\eta_k^2}{1 - \rho}\Var_k(g_k) + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho}
    + \mathcal{O}(\eta_k\gamma_k\alpha_k) \\
    &\phantom{===}- 2\eta_k\E_k(f_k(x_{k+1}) - f_k(x_k^\star)).
\end{align*}
The term $2\eta_k\E_k(f_k(x_{k+1}) - f_k(x_k^\star))$ is indeed nonnegative by optimality of $x_k^\star$
for $f_k$. Assuming that $\sum_{k=0}^\infty \eta_k\gamma_k\alpha_k < \infty$ and
$\sum_{k=0}^\infty \eta_k^2 \gamma_k^2 < \infty$, we are almost ready
to apply \cref{lem:robbins-siegmund}.
The following lemma will give a bound on the gradient noise $\Var_k(g_k)$.
\begin{lemma}[Bound on gradient noise]
    \label{lem:bound-on-variance}
    Let \cref{ass:basic-assumptions} hold. Then, in the situation of \cref{alg:seqprox-sgd}, there
    exists a constant $\sigma^2\in(0,\infty)$ such that
    \[
        \Var_k(g_k) \leq 4 L^2 \norm{x_k - x^\star}^2 + (1 + \gamma_k^2)\sigma^2,
    \]
    for all $k\in\N_0$.
\end{lemma}
\begin{proof}
    We will use the inequality $(a + b)^2 \leq 2a^2 + 2b^2 \enspace\forall a,b\in\R$
    multiple times.
    For $k\in\N_0$, we have
    \[
        \Var_k(g_k)
        \leq \E_k\norm{g_k}^2
        \leq 2\E_k\norm{\nabla F_\xi(x_k)}^2
            + 2\gamma_k^2\, \E_k\norm{\nabla h_k(x_k; A(\xi), b(\xi))}^2.
    \]
    Using the (a.\,s.)
    $L$-smoothness of $x\mapsto F_\xi(x)$, we have
    \begin{align*}
        \E_k\norm{\nabla F_\xi(x_k)}^2
        &=
        \E_k\norm{\nabla F_\xi(x_k) - \nabla F_\xi(x^\star) + \nabla F_\xi(x^\star)}^2 \\
        &\leq
        2\Big( \E_k\norm{\nabla F_\xi(x_k) - \nabla F_\xi(x^\star)}^2 + \E\norm{\nabla F_\xi(x^\star)}^2 \Big) \\
        &\leq 2 L^2\, \norm{x_k - x^\star}^2 + 2\,\E\norm{\nabla F_\xi(x^\star)}^2,
    \end{align*}
    where in the last step we also used that $x_k$ is $\mathcal{F}_k$-measurable and
    $\xi$ is independent of $\mathcal{F}_k$.
    By one of our assumptions, we can find a point $x\in\R^n$ such that
    $\E\norm{F_\xi(x)}^2 < \infty$. Hence, using smoothness, there exists a constant
    $M^2\in(0,\infty)$ such that
    \begin{align*}
        \E\norm{\nabla F_\xi(x^\star)}^2
        &=
        \E\norm{\nabla F_\xi(x^\star) - \nabla F_\xi(x) + \nabla F_\xi(x)}^2 \\
        &\leq
        2L^2\,\norm{x^\star - x}^2 + 2\,\E\norm{F_\xi(x)}^2 \\
        &\leq
        \frac{1}{4}M^2.
    \end{align*}
    Therefore, combining with the previous inequality, we get the bound
    \[
        \E_k\norm{\nabla F_\xi(x_k)}^2 \leq 2 L^2\, \norm{x_k - x^\star}^2 + \frac{1}{2} M^2.
    \]
    Since, per \cref{ass:basic-assumptions}, it holds that the family $(\nabla h_k)_{k\in\N}$ is uniformly
    bounded,
    there exists a constant
    $\tilde{M}^2\in(0,\infty)$ such that
    \[
        \E_k\norm{\nabla h_k(x_k; A(\xi), b(\xi))}^2 \leq \frac{1}{2}\tilde{M}^2.
    \]
    Putting everything together, we obtain
    \[
        \Var_k(g_k)
        \leq 4 L^2\, \norm{x_k - x^\star}^2 + M^2
            + \gamma_k^2 \tilde{M}^2
        = 4 L^2 \norm{x_k - x^\star}^2
            + M^2 + \gamma_k^2 \tilde{M}^2.
    \]
    Setting $\sigma^2 := \max(M^2, \tilde{M}^2)$ yields the desired result.
\end{proof}
We are now ready to prove the main theorem of this section.
\begin{theorem}[Almost sure convergence]
    \label{thm:almost-sure-convergence}
    Assume \cref{ass:basic-assumptions,ass:slater-point,ass:compact-support-without-zero} hold.
    Let $(x_k)_{k\in\N_0}$ be a sequence generated by \cref{alg:seqprox-sgd} with
    parameters $(\eta_k)_{k\in\N_0}$, $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$ that satisfiy
    \begin{enumerate}
        \item $\sum_{k=0}^\infty \eta_k = \infty$ and there exists $\rho\in(0,1)$ such that $\eta_k \leq \rho L^{-1}$ for all $k\in\N$.
        \item $\sum_{k=0}^\infty \eta_k\gamma_k\alpha_k < \infty$.
        \item $\sum_{k=0}^\infty \eta_k^2 \gamma_k^2 < \infty$.
    \end{enumerate}
    Then $\norm{x_{k} - x^\star}$ converges almost surely and,
    in particular, $(x_k)_{k\in\N}$ is bounded almost surely.
    In the case $\mu > 0$, it further holds that
    $\liminf_{k\to\infty} \E\norm{x_{k} - x^\star}^2 = 0$.
\end{theorem}
\begin{proof}
    By \cref{lem:recursion-for-almost-sure-convergence} (dropping the $-\mu\eta_k$ term), we have
    \begin{align*}
        \E_k\norm{x^\star - x_{k+1}}^2
        &\leq
        \norm{x^\star - x_k}^2
        + \frac{2\eta_k^2}{1 - \rho}\Var_k(g_k) + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} \\
        &\phantom{===} + \mathcal{O}(\eta_k\gamma_k\alpha_k)
        - 2\eta_k\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star)),
    \end{align*}
    \Cref{lem:bound-on-variance} lets us bound
    the variance by
    \[
        \Var_k(g_k) \leq 4 L^2 \norm{x_k - x^\star}^2 + (1 + \gamma_k^2)\sigma^2
    \]
    for a constant $\sigma^2\in(0,\infty)$ and all $k\in\N_0$.
    Thus, we have
    \begin{align*}
        \E_k\norm{x^\star - x_{k+1}}^2
        &\leq
        \norm{x^\star - x_k}^2
        + \frac{2\eta_k^2}{1-\rho}\Big( 4 L^2 \norm{x_k - x^\star}^2 + (1 + \gamma_k^2)\sigma^2 \Big) \\
        &\phantom{===} + \mathcal{O}(\eta_k\gamma_k\alpha_k) - 2\eta_k\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star))\\
        &=
        \Big( 1 + 8 L^2(1-\rho)^{-1} \eta_k^2 \Big)\norm{x^\star - x_k}^2
        + 2\eta_k^2(1-\rho)^{-1} (1 + \gamma_k^2) \sigma^2 \\
        &\phantom{===} + \mathcal{O}(\eta_k\gamma_k\alpha_k) - 2\eta_k\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star)),
    \end{align*}
    for all $k\in\N_0$.
    Define the nonnegative sequences $(a_k)_{k\in\N_0}, (b_k)_{k\in\N_0}, (c_k)_{k\in\N_0}$ by
    \begin{align*}
        a_k &:= 8L^2(1-\rho)^{-1}\eta_k^2 \\
        b_k &:= 2\eta_k^2(1-\rho)^{-1}(1 + \gamma_k^2) \sigma^2 + \mathcal{O}(\eta_k\gamma_k\alpha_k)\ \\
        c_k &:= 2\eta_k\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star)).
    \end{align*}
    Note that $c_k$ is indeed nonnegative, since $x_k^\star$ minimizes $f_k$.
    The above inequality now takes the form
    \[
        \E_k\norm{x^\star - x_{k+1}}^2
        \leq
        (1 + a_k)\norm{x^\star - x_k}^2
        + b_k - c_k.
    \]
    Our assumptions imply that $\sum_{k=0}^{\infty} a_k < \infty$ and $\sum_{k=0}^\infty b_k < \infty$,
    so we can apply
    \cref{lem:robbins-siegmund}, which implies that with probability one the sequence
    $\norm{x^\star - x_{k}}^2$ converges and
    $\sum_{k=0}^\infty \eta_k\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star)) < \infty$.
    By the bounded convergence theorem (\textcolor{red}{TODO: add ref}), it further holds that
    \begin{align*}
        \infty > \E\Big( \sum_{k=0}^\infty \eta_k\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star)) \Big)
        &=
        \sum_{k=0}^\infty \eta_k\E(\E_k(f_{k}(x_{k+1}) - f_{k}(x_k^\star))) \\
        &=
        \sum_{k=0}^\infty \eta_k\E(f_{k}(x_{k+1}) - f_{k}(x_k^\star)).
    \end{align*}
    Since $\sum_{k=0}^\infty \eta_k = \infty$, it must therefore hold that
    \[
        \liminf_{k\to\infty} \E(f_{k}(x_{k+1}) - f_{k}(x_k^\star)) = 0.
    \]
    Strong convexity of $f_k$ and optimaliy of $x_k^\star$ for $f_k$ imply
    \[
        f_{k}(x_{k+1}) - f_{k}(x_k^\star) \geq \frac{\mu}{2} \norm{x_{k+1} - x_k^\star}^2.
    \]
    If $\mu > 0$, we therefore have $\liminf_{k\to\infty} \E\norm{x_{k+1} - x_k^\star}^2 = 0$,
    which implies that
    \[
        \liminf_{k\to\infty} \E\norm{x_{k} - x^\star}^2 = 0,
    \]
    since we also know that $x_k^\star$ converges to $x^\star$
    in the case $\mu > 0$, by \cref{thm:distance-to-feasibility-bound}.
\end{proof}


\section{Convergence rates in expectation}

In the previous section, we established that the iterates $(x_k)_{k\in\N_0}$
of \cref{alg:seqprox-sgd} are bounded with probability one,
provided that the parameters $(h_k)_{k\in\N_0}$, $(\eta_k)_{k\in\N_0}$, $(\gamma_k)_{k\in\N_0}$
satisfy certain conditions, which are captured in the following assumption.
\begin{assumption}
    \label{ass:step-sizes-and-penalties}
    The parameters
    $(h_k)_{k\in\N_0}$, $(\eta_k)_{k\in\N_0}$, $(\gamma_k)_{k\in\N_0}$ of \cref{alg:seqprox-sgd} satisfiy
    \begin{enumerate}
        \item $\sum_{k=0}^\infty \eta_k = \infty$ and there exist constants $\rho\in(0,1)$ and $K\in\N$
            such that $\eta_k \leq \rho L^{-1}$ for all $k\in\N$ with $k\geq K$.
        \item $\sum_{k=0}^\infty \eta_k\gamma_k\alpha_k < \infty$.
        \item $\sum_{k=0}^\infty \eta_k^2 \gamma_k^2 < \infty$.
    \end{enumerate}
\end{assumption}
\begin{lemma}
    \label{lem:functional-upper-bound}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point,ass:step-sizes-and-penalties} hold.
    Then, for all $k\in\N$, it holds that
    \begin{align*}
        2\eta_k(f_k(x_k) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho}
            + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2
            + \eta_k^2 \mathcal{O}(\gamma_k^2)
    \end{align*}
    almost surely.
\end{lemma}
\begin{proof}
    By \cref{lem:one-step-improvement},
    \begin{align}
        2\eta_k(f_k(x_{k+1}) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} +  \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho} + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2
    \end{align}
    almost surely, for all $k\in\N_0$ and $x\in\R^n$.
    Let $\tilde{\nabla} f_k(x_k)$ denote a subgradient
    of $f_k$ at $x_k$. By convexity of $f_k$ and Cauchy-Schwarz, we have
    \[
        f_k(x_{k+1})
        \geq
        f_k(x_k) + \langle \tilde{\nabla} f_k(x_k), x_{k+1} - x_k \rangle
        \geq
        f_k(x_k) - \norm{\tilde{\nabla} f_k(x_k)} \norm{x_{k+1} - x_k}.
    \]
    Therefore,
    \begin{align}
        \label{eq:proof:lem:functional-upper-bound}
        2\eta_k(f_k(x_k) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho}
            + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2 + 2\eta_k \norm{\tilde{\nabla} f_k(x_k)}\norm{x_{k+1} - x_k}
    \end{align}
    almost surely, for all $x\in\R^n$.
    The sequence $(x_k)_{k\in\N_0}$ is bounded almost surely (\cref{thm:almost-sure-convergence}),
    hence \cref{lem:basic-properties} implies
    \[
        \norm{\tilde{\nabla} f_k(x_k)} = \mathcal{O}(\gamma_k)
    \]
    almost surely.
    We will now analyze $\norm{x_{k+1} - x_k}$. Let $y_k := \prox_{\eta_k r} (x_k)$. Then, using
    the triangle inequality and the nonexpansiveness property of the proximal operator (\textcolor{red}{TODO}),
    we have
    \begin{align*}
        \norm{x_{k+1} - x_k} &= \norm{x_{k+1} - y_k + y_k - x_k} \\
                            &\leq \norm{\prox_{\eta_k r}(x_k - \eta_k g_k) - \prox_{\eta_k r}(x_k)} + \E\norm{y_k - x_k} \\
                            &\leq \eta_k \norm{g_k} + \norm{y_k - x_k}.
    \end{align*}
    For the second term, we use the definition
    of $y_k$ as the solution to
    \[
        \min_{x\in\R^n} r(x) + \frac{1}{2\eta_k} \norm{x - x_k}^2.
    \]
    By the first-order optimality condition, there exists a subgradient $\tilde{\nabla} r(y_k) \in \partial r(y_k)$ such that
    \[
        \tilde{\nabla} r(y_k) + \frac{y_k - x_k}{\eta_k} = 0 \enspace\iff\enspace \frac{x_k - y_k}{\eta_k} \in \partial r(y_k).
    \]
    Since $x_k$ is in the domain of $r$ for all $k\in\N$ \textcolor{red}{(TODO)} (not necessarily
    for $k=0$), we can now use the above, together with convexity, to get
    \[
        r(x_k) - r(y_k) \geq \langle \eta_k^{-1} (x_k - y_k), x_k - y_k \rangle = \eta_k^{-1} \norm{x_k - y_k}^2,
    \]
    for all $k\in\N$.
    Finally, local Lipschitz continuity of $r$  (\cref{ass:basic-assumptions}) yields (a.\,s.)
    \[
        L_r \norm{x_k - y_k} \geq r(x_k) - r(y_k) \geq \eta_k^{-1} \norm{x_k - y_k}^2
        \enspace\iff\enspace
        \norm{x_k - y_k} \leq \eta_k L_r,
    \]
    for some $L_r \in (0,\infty)$, where we again used that $(x_k)_{k\in\N_0}$ is bounded almost surely
    (\cref{thm:almost-sure-convergence})
    and the prox operator is nonexpansive (\textcolor{red}{TODO}).
    Combining with \eqref{eq:proof:lem:functional-upper-bound}, we therefore obtain
    \begin{align*}
        2\eta_k(f_k(x_k) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho}
            + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2
            + \eta_k^2 \mathcal{O}(\gamma_k) \big( \norm{g_k} + L_r \big),
    \end{align*}
    almost surely, for all $k\in\N$.
    Finally, we can use \cref{lem:basic-properties}, which tells us that $\norm{g_k} = \mathcal{O}(\gamma_k)$
    almost surely (again using boundedness of $(x_k)_{k\in\N_0}$), hence we arrive at
    \begin{align*}
        2\eta_k(f_k(x_k) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + \frac{8 G^2\eta_k^2\gamma_k^2}{1 - \rho}
            + \frac{2\eta_k^2}{1 - \rho}\norm{z_k}^2
            + \eta_k^2 \mathcal{O}(\gamma_k^2),
    \end{align*}
    as desired.
\end{proof}
\begin{lemma}
    \label{lem:bound-on-norm(zk)}
    Let \cref{ass:basic-assumptions,ass:step-sizes-and-penalties} hold.
    For all $k\in\N_0$, it holds that
    \[
        \norm{z_k}^2 \leq 8L_{\text{loc}}^2 + 8\gamma_k^2G^2,
    \]
    with probability one, where $L_{\text{loc}} \in (0,\infty)$.
\end{lemma}
\begin{proof}
    We have
    \begin{align*}
        \norm{g_k} &= \norm{\nabla F_{\xi_k}(x_k) + \gamma_k h_k(x_k; A(\xi), b(\xi))} \\
                &\leq \norm{\nabla F_{\xi_k}(x_k)} + \gamma_k\norm{h_k(x_k; A(\xi), b(\xi))}.
    \end{align*}
    By almost-sure smoothness of $(x_k)_{k\in\N_0}$ (\cref{thm:almost-sure-convergence}) and
    local Lipschitz smoothness of $x\mapsto F_{\xi}(x)$ (a.\,s.), it holds that
    \[
        \norm{F_{\xi_k}(x_k)} \leq L_\text{loc} \enspace \text{(a.\,s.)}.
    \]
    By \cref{ass:basic-assumptions}, we have
    \[
        \sup_{k\in\N_0}\sup_{x\in\R^n} \norm{\nabla h_k(x; A(\xi), b(\xi))} \leq G.
    \]
    Using the inequality $(a + b)^2 \leq 2 (a^2 + b^2)$ $\forall a,b\in\R$, we have
    \[
        \norm{g_k}^2 \leq 2L^2_\text{loc} + 2\gamma_k^2G^2
    \]
    with probability one.
    The claim follows from $\nabla \psi_k(x_k) = \E_k(g_k)$ and
    \[
        \norm{g_k - \nabla \psi_k(x_k)}^2
        \leq 2\norm{g_k}^2 + 2\norm{\nabla \psi_k(x_k)}^2
        \leq 2\norm{g_k}^2 + 2\E_k\norm{g_k}^2,
    \]
    where the last step uses Jensen's inequality.
\end{proof}

\subsection{Convex case}

\begin{lemma}
    \label{lem:parameters-for-convex-case}
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}, (h_k)_{k\in\N_0}$
    of \cref{alg:seqprox-sgd} satisfy
    \[
        \eta_k = \mathcal{O}\Big( \frac{1}{k^c \log^{(1 + 3e) / 2}(k+1)} \Big),\,
        \gamma_k = \mathcal{O}(\log^{e}(k+1)),\,
        \alpha_k = \mathcal{O}\Big( \frac{1}{k^d} \Big)
    \]
    with $c\in[1/2, 1)$, $d \in (1/2, \infty)$, and $e \in (0,\infty)$.
    Then, \cref{ass:step-sizes-and-penalties} holds.
\end{lemma}
\begin{proof}
    See the proof of lemma 9 in \cite{nedich2023huber}.
\end{proof}
\begin{lemma}
    \label{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point}
    hold.
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$
    be as in \cref{lem:parameters-for-convex-case}.
    % with $c = 1/2$, $d \in (1/2, \infty)$, and $e \in (0,\infty)$.
    Further, let the iterate weights $(w_k)_{k\in\N}$
    be defined as $w_k := \eta_k$, for all $k\in\N$.
    Then there exists a constant $k_0\in\N$
    such that the averaged iterate $\bar{x}_K$, generated by \cref{alg:seqprox-sgd}
    with parameters $(\eta_k)_{k\in\N_0}$, $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$, $(w_k)_{k\in\N}$,
    satisfies
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        + \frac{\norm{x^\star - x_{k_0}}^2}{2S_K}
        + \frac{\sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)}{2 S_K} \notag \\
        &\phantom{===}
            + \frac{\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k}{S_K}
            + \frac{\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x^\star \rangle}{S_K}
    \end{align*}
    almost surely,
    for all $K\in\N$ with $K \geq k_0$, where $S_{t,k} := \sum_{i=t}^k w_i$, and
    $\bar{x}_{t,k} := S_{t,k}^{-1} \sum_{i=t}^k w_i x_i$ $\forall t,k\in\N$.
    Furthermore, there exist constants $k_1\in\N$ and $\tau \in (0, \infty)$ such that
    \begin{align*}
        \dist(\bar{x}_K, \mathcal{X})
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
            + \frac{\gamma_{k_1}^{-1}\norm{x^\star - x_{k_1}}^2}{\tau S_K}
            + \frac{\sum_{k=k_1}^{K} \eta_k^2\mathcal{O}(\gamma_k)}{\tau S_K} \\
        &\phantom{===}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k\alpha_k}{\tau S_K}
            + 2 \frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_K}
    \end{align*} 
    almost surely, for all $K\in\N$ with $K \geq k_1$.
\end{lemma}
\begin{proof}
    Let $k_0\in\N$ be large enough such that $\eta_{k_0} \leq 1/(2L)$ and fix
    some $k\in\N$ with $k \geq k_0$.
    \Cref{lem:parameters-for-convex-case} implies that the parameters
    $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}, (h_k)_{k\in\N_0}$ satisfy
    \cref{ass:step-sizes-and-penalties}.
    Thus, we can apply \cref{lem:functional-upper-bound}, which implies
    \begin{align*}
        2\eta_k(f_k(x_k) - f_k(x))
        &\leq ( 1 - \mu\eta_k ) \norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + 16 G^2\eta_k^2\gamma_k^2
            + 4\eta_k^2\norm{z_k}^2
            + \eta_k^2 \mathcal{O}(\gamma_k^2),
    \end{align*}
    almost surely, for all $x\in\R^n$.
    Combining with \cref{lem:bound-on-norm(zk)},
    we find that
    \begin{align*}
        2\eta_k(f_k(x_k) - f_k(x))
        &\leq (1 - \mu\eta_k)\norm{x - x_k}^2 + 2\eta_k\langle z_k, x_k - x \rangle 
            - \norm{x - x_{k+1}}^2 \notag \\
        &\phantom{===} + \eta_k^2\mathcal{O}(\gamma_k^2)
    \end{align*}
    almost surely, for all $x\in\R^n$.
        Assume now that $x\in\mathcal{X}$. By a property of $\pi_k$, the fact that
    $(x_k)_{k\in\N}$ is bounded almost surely (\cref{thm:almost-sure-convergence}), and \cref{lem:lower-bound-on-penalties-by-distance-to-feasibility},
    we can deduce that there exists $\tau\in(0,\infty)$ such that
    \begin{align*}
        f_k(x_{k}) - f_k(x)
        &= f(x_k) - f(x) + \gamma_k (\pi_k(x_k) - \pi_k(x)) \\
        &\geq f(x_k) - f(x) + \gamma_k (\tau\dist(x_k,\mathcal{X}) - \alpha_k)
    \end{align*}
    with probability one. Hence, we have
    \begin{align}
        \label{eq:lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)}
        2\eta_k (f(x_{k}) - f(x))
        &\leq
        (1-\mu\eta_k)\norm{x - x_k}^2 - \norm{x - x_{k+1}}^2 + 2\eta_k\langle z_k, x_k - x \rangle
            + \eta_k^2 \mathcal{O}(\gamma_k^2) \notag \\
            &\phantom{===} + 2\eta_k\gamma_k\alpha_k - 2\tau \eta_k \gamma_k \dist(x_k, \mathcal{X}),
    \end{align}
    for all $x\in\mathcal{X}$.
    We can drop the $-\mu\eta_k$ and $-\dist(x_k, \mathcal{X})$ terms. Then,
    summing both sides from $k = k_0$ to $K \in \N$ with $K \geq k_0$,
    we obtain
    \begin{align*}
        2\sum_{k=k_0}^K \eta_k(f(x_k) - f(x))
        &\leq \norm{x - x_{k_0}}^2
            - \norm{x - x_{K+1}}^2
            + 2\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x \rangle \notag \\
        &\phantom{===}
            + 2\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k
            + \sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2) \\
        &\leq \norm{x - x_{k_0}}^2
            + 2\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x \rangle \notag \\
        &\phantom{===}
            + 2\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k
            + \sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)
    \end{align*}
    almost surely, for all $x\in\mathcal{X}$.
    We define $S_{t, k} := \sum_{i=t}^{k} \eta_i$, $S_k := S_{1, k}$,
    and $\bar{x}_{t, k} := S_{t, k}^{-1} \sum_{i=t}^{k} \eta_i x_i$ for $t,k\in\N$.
    Setting $x := x^\star \in \mathcal{X}^\star$, dividing both sides by $2\sum_{k=k_0}^K \eta_k$ and using convexity
    of $f$, we obtain
    \begin{align*}
        f(\bar{x}_{k_0, K}) - f(x^\star)
        &\leq \frac{\norm{x^\star - x_{k_0}}^2}{2S_{k_0, K}}
            + \frac{\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x^\star \rangle}{S_{k_0, K}} \notag \\
        &\phantom{===}
            + \frac{\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k}{S_{k_0, K}}
            + \frac{\sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)}{2 S_{k_0, K}}.
    \end{align*}
    Note that 
    \[
        \bar{x}_K = \frac{S_{1, k_0-1}}{S_K} \bar{x}_{1, k_0-1} + \frac{S_{k_0, K}}{S_K} \bar{x}_{k_0, K}
    \]
    and
    \[
        \frac{S_{1, k_0-1}}{S_K} + \frac{S_{k_0, K}}{S_K} = 1,
    \]
    hence, using convexity of $f$ again, we have
    \[
        f(\bar{x}_K) - f(x^\star) \leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        + \frac{S_{k_0, K}}{S_K} (f(\bar{x}_{k_0, K}) - f(x^\star)).
    \]
    Combining with the latest bound on $(f(\bar{x}_{k_0, K}) - f(x^\star))$, we arrive at
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        +
        \frac{\norm{x^\star - x_{k_0}}^2}{2S_K}
            + \frac{\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x^\star \rangle}{S_K} \notag \\
        &\phantom{===}
            + \frac{\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k}{S_K}
            + \frac{\sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)}{2 S_K}
    \end{align*}
    almost surely.
        Next, we will derive the desired bound for $\dist(\bar{x}_K, \mathcal{X})$.
    Again fix $k \geq k_0$.
    For the choice $x := \proj(x_k)$,
    inequality \eqref{eq:lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)} gives us
    \begin{align*}
        2\eta_k (f(x_{k}) - f(\proj(x_k)))
        &\leq
        (1-\mu\eta_k)\dist(x_k, \mathcal{X})^2 - \norm{\proj(x_k) - x_{k+1}}^2
            + \eta_k^2 \mathcal{O}(\gamma_k^2) \notag \\
            &\phantom{===} + 2\eta_k\langle z_k, x_k - \proj(x_k) \rangle
                    + 2\eta_k\gamma_k\alpha_k - 2\tau \eta_k \gamma_k \dist(x_k, \mathcal{X}).
    \end{align*}
    Note that $\norm{\proj(x_k) - x_{k+1}} \geq \dist(x_{k+1}, \mathcal{X})$,
    hence, after additionally rearranging, we obtain
    \begin{align*}
        \dist(x_{k+1}, \mathcal{X})^2
        &\leq
        (1-\mu\eta_k)\dist(x_k, \mathcal{X})^2
            + \eta_k^2 \mathcal{O}(\gamma_k^2) + 2\eta_k\langle z_k, x_k - \proj(x_k) \rangle \notag \\
            &\phantom{===}
                    + 2\eta_k\gamma_k\alpha_k - 2\tau \eta_k \gamma_k \dist(x_k, \mathcal{X}) + 2\eta_k (f(\proj(x_k)) - f(x_{k})).
    \end{align*}
    Almost sure boundedness of $(x_k)_{k\in\N}$ (\cref{thm:almost-sure-convergence})
    implies that $(\Pi_\mathcal{X}(x_k))_{k\in\N_0}$ is also
    bounded almost surely, since continuous functions map compacta to compacta.
    Therefore, by
    local Lipschitz continuity of $f$ (\cref{lem:basic-properties}),
    there exists a constant $M\in[0,\infty)$ such that
    \[
        f(\proj(x_k)) - f(x_{k}) \leq M \dist(x_k, \mathcal{X}) \enspace (\text{a.\,s.}).
    \]
    Combining with the previous inequality and gathering terms involving $\dist(x_k,\mathcal{X})$,
    we arrive at
    \begin{align*}
        \dist(x_{k+1}, \mathcal{X})^2
        &\leq
        (1-\mu\eta_k)\dist(x_k, \mathcal{X})^2
            + \eta_k^2 \mathcal{O}(\gamma_k^2) + 2\eta_k\langle z_k, x_k - \proj(x_k) \rangle \notag \\
            &\phantom{===}
                    + 2\eta_k\gamma_k\alpha_k
                    - 2\eta_k (\tau\gamma_k - M) \dist(x_k, \mathcal{X}).
    \end{align*}
    Since $\gamma_k \uparrow \infty$, there exists $k_1\in\N$ such that
    $\tau_k\gamma_k - M \geq \tau\gamma_k/2$ for all natural numbers $k\geq k_1$. Thus,
    \begin{align}
        \label{eq:proof:lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case:dist-bound}
        \dist(x_{k+1}, \mathcal{X})^2 &\leq
        (1-\mu\eta_k)\dist(x_k, \mathcal{X})^2 + \eta_k^2 \mathcal{O}(\gamma_k^2)
            + 2\eta_k\langle z_k, x_k - \proj(x_k) \rangle \notag \\
            &\phantom{===}
            + 2\eta_k\gamma_k\alpha_k - \tau \eta_k\gamma_k \dist(x_k, \mathcal{X}),
    \end{align}
    for all $k \geq k_1$.
    Dropping the $-\mu\eta_k$ term and 
    multiplying both sides by $\gamma_k^{-1}$, we get
    \begin{align*}
        \gamma_k^{-1} \dist(x_{k+1}, \mathcal{X})^2
        &\leq
        \gamma_k^{-1} \dist(x_k, \mathcal{X})^2
            + \eta_k^2\mathcal{O}(\gamma_k) \\
            &\phantom{===}
            + 2\gamma_k^{-1}\eta_k\langle z_k, x_k - \proj(x_k) \rangle
            + 2\eta_k\alpha_k - \tau \eta_k \dist(x_k, \mathcal{X}),
    \end{align*}
    for all $k \geq k_1$.
    Since $\gamma_k$ is nondecreasing, we have $\gamma_k^{-1} \geq \gamma_{k+1}^{-1}$ for all $k\in\N$.
    Setting $d_j := \gamma_j^{-1} \dist(x_j, \mathcal{X})^2 \enspace \forall j\in\N$, we therefore have
    \[
        d_{k+1}
        \leq
        d_k
            + \eta_k^2\mathcal{O}(\gamma_k)
            + 2\gamma_k^{-1}\eta_k\langle z_k, x_k - \proj(x_k) \rangle
            + 2\eta_k\alpha_k
            - \tau \eta_k \dist(x_k, \mathcal{X}),
    \]
    for all $k\geq k_1$.
    Rearranging again and summing from $k=k_1$ to $k=K$ for $K\in\N$ with $K \geq k_1$, we have
    \begin{align*}
        \tau \sum_{k=k_1}^{K} \eta_k \dist(x_k, \mathcal{X})
        &\leq
        d_{k_1} - d_{K+1} + \sum_{k=k_1}^{K} \eta_k^2 \mathcal{O}(\gamma_k)
            + 2 \sum_{k=k_1}^{K}\eta_k\alpha_k \\
        &\phantom{===} + 2\sum_{k=k_1}^K \gamma_k^{-1}\eta_k \langle z_k, x_k - \proj(x_k) \rangle \\
        &\leq
        d_{k_1} + \sum_{k=k_1}^{K} \eta_k^2\mathcal{O}(\gamma_k)
            + 2 \sum_{k=k_1}^{K}\eta_k\alpha_k \\
        &\phantom{===} + 2\sum_{k=k_1}^K \gamma_k^{-1}\eta_k \langle z_k, x_k - \proj(x_k) \rangle,
    \end{align*}
    where we used $d_{K+1} \geq 0$ in the last step.
    The distance functional $x\mapsto\dist(x,\mathcal{X})$
    is convex (\textcolor{red}{TODO: Show this in an example.}), so if we divide both sides of
    the above inequality by $\tau \cdot S_{k_1, K}$, we obtain
    \begin{align*}
        \dist(\bar{x}_{k_1, K}, \mathcal{X})
        &\leq
        \frac{d_{k_1}}{\tau S_{k_1, K}} + \frac{\sum_{k=k_1}^{K} \eta_k^2 \mathcal{O}(\gamma_k)}{\tau S_{k_1, K}}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k\alpha_k}{\tau S_{k_1, K}} \\
        &\phantom{===} + 2 \frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_{k_1, K}}.
    \end{align*} 
    To derive a bound for $\dist(\bar{x}_K, \mathcal{X})$, we proceed similarly as before. We have
    \[
        \bar{x}_K = \frac{S_{1, k_1-1}}{S_K} \bar{x}_{1, k_1-1} + \frac{S_{k_1, K}}{S_K} \bar{x}_{k_1, K}
    \]
    and
    \[
        \frac{S_{1, k_1-1}}{S_K} + \frac{S_{k_1, K}}{S_K} = 1.
    \]
    Combining with the latest bound on $\dist(\bar{x}_{k_1, K}, \mathcal{X})$, we obtain the desired bound
    \begin{align*}
        \dist(\bar{x}_K, \mathcal{X})
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
            + \frac{S_{k_1, K}}{S_K} \dist(\bar{x}_{k_1, K}, \mathcal{X}) \\
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
            + \frac{d_{k_1}}{\tau S_K}
            + \frac{\sum_{k=k_1}^{K} \eta_k^2\mathcal{O}(\gamma_k)}{\tau S_K}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k\alpha_k}{\tau S_K} \\
        &\phantom{===} + 2 \frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_K}
    \end{align*}
    almost surely, or all $K \geq k_1$.
    This concludes the proof.
\end{proof}

\begin{theorem}[Convergence rates in expectation - convex case]
    \label{thm:convergence-rates-in-expectation-convex-case}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point}
    hold.
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$
    be the same as in \cref{lem:parameters-for-convex-case}.
    Further, let the iterate weights $(w_k)_{k\in\N}$
    be defined as $w_k := \eta_k$, for all $k\in\N$.
    Then, the averaged iterate $\bar{x}_K$,
    generated by \cref{alg:seqprox-sgd} with parameters $(\eta_k)_{k\in\N_{0}}$,
    $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$, $(w_k)_{k\in\N}$, satisfies
    \[
        \E| f(\bar{x}_{K}) - f(x^\star) |
        = \max(1, \tau^{-1}) \, \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{K^{1-c}} \Big).
    \]
    Further, it also holds that
    \[
        \E(\dist(\bar{x}_K, \mathcal{X}))
        = \tau^{-1} \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{K^{1-c}} \Big).
    \]
\end{theorem}
\begin{proof}
    \Cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case} gives us
    an upper bound for $f(\bar{x}_K) - f(x^\star)$,
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        + \frac{\norm{x^\star - x_{k_0}}^2}{2S_K}
        + \frac{\sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)}{2 S_K} \notag \\
        &\phantom{===}
            + \frac{\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k}{S_K}
            + \frac{\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x^\star \rangle}{S_K}
    \end{align*}
    almost surely, for all $K\in\N$ with $K \geq k_0$, where $S_K := \sum_{k=1}^K \eta_k$.
    Note that
    \[
        \E(\langle z_k, x_k - x^\star \rangle)
        = \E(\E_k(\langle z_k, x_k - x^\star \rangle))
        = \E(\langle \E_k(z_k), x_k - x^\star \rangle)
        = 0,
    \]
    for all $k\in\N$. Hence, taking expectations on both sides of the above inequality, we get
    \begin{align}
        \label{eq:proof:thm:convergence-rates-in-expectation-convex-case-0}
        \E(f(\bar{x}_K) - f(x^\star))
        &\leq
        \frac{S_{1, k_0-1}}{S_K} \E(f(\bar{x}_{1, k_0-1}) - f(x^\star))
        + \frac{\E\norm{x^\star - x_{k_0}}^2}{2S_K}
        + \frac{\sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)}{2 S_K} \notag \\
        &\phantom{===}
            + \frac{\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k}{S_K}.
    \end{align}
    By convexity, we have for all $k\in\N$
    \begin{align*}
        f(\bar{x}_k) - f(x^\star) &\geq \langle \tilde{\nabla} f(x^\star), \bar{x}_k - x^\star \rangle \\
                            &= \langle \tilde{\nabla} f(x^\star), \bar{x}_k - \proj(\bar{x}_k) \rangle
                                + \langle \tilde{\nabla} f(x^\star), \proj(\bar{x}_k) - x^\star \rangle,
    \end{align*}
    for any subgradient $\tilde{\nabla} f(x^\star) \in \partial f(x^\star)$. Optimality of $x^\star$
    for $f$ on $\mathcal{X}$ implies that there exists a subgradient
    $\tilde{\nabla} f(x^\star) \in \partial f(x^\star)$ such that
    $\langle \tilde{\nabla} f(x^\star), \proj(\bar{x}_k) - x^\star \rangle \geq 0$.
    Combining these facts with the above inequality and applying Cauchy-Schwarz,
    we obtain
    \begin{equation}
        \label{eq:proof:thm:convergence-rates-in-expectation-convex-case}
        f(\bar{x}_k) - f(x^\star) \geq - \norm{\tilde{\nabla} f(x^\star)} \dist(\bar{x}_k, \mathcal{X}),
    \end{equation}
    for all $k\in\N$.
    Taking expectations in the bound for $\dist(\bar{x}_K, \mathcal{X})$
    from \cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)},
    we know that there exists $k_1\in\N$ such that for all $K\in\N$ with $K\geq k_1$,
    it holds that
    \begin{align*}
        \E(\dist(\bar{x}_K, \mathcal{X}))
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \E(\dist(\bar{x}_{1, k_1-1}, \mathcal{X}))
            + \frac{\gamma_{k_1}^{-1}\norm{x^\star - x_{k_1}}^2}{\tau S_K}
            + \frac{\sum_{k=k_1}^{K} \eta_k^2\mathcal{O}(\gamma_k)}{\tau S_K} \\
        &\phantom{===}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k\alpha_k}{\tau S_K}.
    \end{align*}
    The first two terms are constants.
    Since $\eta_k = \mathcal{O}\big(\big(k^c \log^{(1 + 3e) / 2}(k+1)\big)^{-1}\big)$,
    it follows that (\textcolor{red}{TODO: Show this with integral test.})
    \begin{equation}
        \label{eq:proof:thm:convergence-rates-in-expectation-convex-case:SK-complexity}
        S_K = \mathcal{O}\Big( \frac{K^{1-c}}{\log^{(1 + 3e) / 2} (K)} \Big)        
    \end{equation}
    By \cref{lem:parameters-for-convex-case}, it holds that
    $\sum_{k=1}^K \gamma_k\eta_k \alpha_k = \mathcal{O}(1)$ and
    $\sum_{k=1}^K \eta_k^2\gamma_k^2 = \mathcal{O}(1)$.
    Therefore, we obtain
    \[
        \E(\dist(\bar{x}_K, \mathcal{X}))
        = \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{\tau K^{1-c}} \Big).
    \]
    Combining with \eqref{eq:proof:thm:convergence-rates-in-expectation-convex-case}, we obtain
    \[
        \E( f(\bar{x}_k) - f(x^\star) )
        \geq - \norm{\nabla f(x^\star)} \cdot \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{\tau K^{1-c}} \Big).
    \]
    For the upper bound in
    \eqref{eq:proof:thm:convergence-rates-in-expectation-convex-case-0}, we
    similarly find that
    \[
        \E(f(\bar{x}_{K}) - f(x^\star))
        \leq
        \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{K^{1-c}} \Big).
    \]
    Putting the two bounds together, we obtain
    \[
        \E| f(\bar{x}_{K}) - f(x^\star) |
        = \max(1, \tau^{-1}) \, \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{K^{1-c}} \Big),
    \]
    as desired.
\end{proof}

\subsection{Strongly convex case}

\begin{lemma}
    \label{lem:parameters-for-strongly-convex-case}
    In the setting $\mu > 0$,
    let $\eta_k := 2/(\mu k)$ for all $k\in\N$ and $\eta_0 := 2/\mu$. Then,
    for any $e\in(0,\infty)$, and any sequences $(\gamma_k)_{k\in\N_0}$,
    $(h_k)_{k\in\N_0}$
    with $\gamma_k = \mathcal{O}(\log^{e}(k+1))$ and
    $\alpha_k = \mathcal{O}(1/k)$,
    \cref{ass:step-sizes-and-penalties} holds.
\end{lemma}
\begin{proof}
    Fix $e\in (0,\infty)$ and let $\gamma_k = \mathcal{O}(\log^{e}(k))$,
    $\alpha_k = \mathcal{O}(1/k)$.
    Clearly, there exist $\rho\in(0,1)$ and $K\in\N$ large enough
    such that $\eta_k \leq \rho L^{-1}$ for all $k \geq K$. It also holds that
    \begin{enumerate}
        \item $\sum_{k=0}^\infty \eta_k = \infty$,
        \item $\sum_{k=0}^\infty \gamma_k^2 \eta_k^2 = \sum_{k=1}^{\infty} \mathcal{O}(\log^{2e}(k)/k^2) < \infty$,
        \item $\sum_{k=0}^\infty \gamma_k \eta_k \alpha_k = \sum_{k=1}^{\infty} \mathcal{O}(\log^e(k)/k^2) < \infty$.
    \end{enumerate}
    This proves the claim.
\end{proof}

\begin{lemma}
    \label{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point}
    hold, and assume that we are in the setting $\mu > 0$.
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$
    be as in \cref{lem:parameters-for-strongly-convex-case}.
    Further, let the iterate weights $(w_k)_{k\in\N}$
    be defined as $w_k := \eta_k^{-1}$, for all $k\in\N$.
    Then there exists a constant $k_0\in\N$
    such that the averaged iterate $x_K$,
    generated by \cref{alg:seqprox-sgd}
    with parameters $(\eta_k)_{k\in\N_0}$, $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$, $(w_k)_{k\in\N}$,
    satisfies
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        +
        \frac{e_{k_0}}{2S_K}
            + \frac{\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle}{S_K} \\
        &\phantom{===} + \frac{\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2)}{S_K}
            + \frac{\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k}{S_K}.
    \end{align*}
    almost surely,
    for all $K\in\N$ with $K \geq k_0$, where $e_{k_0} := \eta_{k_0-1}^{-2} \norm{x^\star - x_{k_0}}^2$
    $S_{t,k} := \sum_{i=t}^k w_i$, and
    $\bar{x}_{t,k} := S_{t,k}^{-1} \sum_{i=t}^k w_i x_i$ $\forall t,k\in\N$.
    Furthermore, there exist constants $k_1\in\N$ and $\tau \in (0, \infty)$ such that
    \begin{align*}
        \dist(\bar{x}_K, \mathcal{X})
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
            + \frac{d_{k_1}}{\tau S_K}
            + \frac{\sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)}{\tau S_K}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k}{\tau S_K} \\
        &\phantom{===} + 2\frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_K}
    \end{align*} 
    almost surely, for all $K\in\N$ with $K \geq k_1$, where $d_{k_1} := \gamma_{k_1}^{-1} \eta_{k_1-1}^{-2} \dist(x_{k_1}, \mathcal{X})^2$.
\end{lemma}

\begin{proof}
    Let $k_0\in\N$ be large enough such that $\eta_{k_0} \leq 1/(2L)$ and fix
    some $k\in\N$ with $k \geq k_0$.
    \Cref{lem:parameters-for-strongly-convex-case} implies that the parameters
    $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}, (h_k)_{k\in\N_0}$ satisfy
    \cref{ass:step-sizes-and-penalties}.
    We can apply the exact same steps as in the proof of
    \cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case},
    until we arrive at inequality \eqref{eq:lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)}.
    From this starting point,
    we can closely follow the proof stategy of lemma 13 in \cite{nedich2023huber} to arrive
    at our desired result.
    First, we will prove the claim for $f(\bar{x}_k) - f(x^\star)$. Dropping the
    $-\dist(x_k,\mathcal{X})$ term from \eqref{eq:lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)},
    setting $x := x^\star \in \mathcal{X}^\star$,
    and multiplying both sides by $\eta_k^{-2}$, we get
    \begin{align*}
        2\eta_k^{-1} (f(x_{k}) - f(x^\star))
        &\leq
        \eta_k^{-2}(1-\mu\eta_k)\norm{x^\star - x_k}^2 - \eta_k^{-2}\norm{x^\star - x_{k+1}}^2
            + 2\eta_k^{-1}\langle z_k, x_k - x^\star \rangle \notag \\
            &\phantom{===} + \mathcal{O}(\gamma_k^2) + 2\eta_k^{-1}\gamma_k\alpha_k.
    \end{align*}
    For the choice of step size $\eta_k = 2/(\mu k)$, it holds that
    \[
        \frac{1 - \mu\eta_k}{\eta_k^2} = \frac{\mu^2 k^2 (1 - 2/k)}{4}
        = \frac{\mu^2 (k^2 - 2k)}{4} = \frac{\mu^2 ((k - 1)^2 - 1)}{4}
        \leq \frac{\mu^2 (k - 1)^2}{4}
        = \eta_{k-1}^{-2}.
    \]
    Setting $e_j := \eta_{j-1}^{-2} \norm{x^\star - x_j}^2$ for all $j\in\N$, we thus have
    \[
        2\eta_k^{-1} (f(x_{k}) - f(x^\star))
        \leq
        e_k - e_{k+1} + 2\eta_k^{-1}\langle z_k, x_k - x^\star \rangle + \mathcal{O}(\gamma_k^2)
            + 2\eta_k^{-1}\gamma_k\alpha_k
    \]
    almost surely, for all $k\in\N$.
    Summing both sides over $k = k_0,\dots,K$ for $K\in\N$ yields
    \begin{align*}
        2\sum_{k=k_0}^{K} \eta_k^{-1} (f(x_{k}) - f(x^\star))
        &\leq
        e_{k_0} - e_{K} + 2\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle + 2\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2) \\
        &\phantom{===} + 2\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k \\
        &\leq
        e_{k_0} + 2\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle + 2\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2) \\
        &\phantom{===} + 2\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k,
    \end{align*}
    where we used $e_K \geq 0$ in the second step.
    We define $S_{t, k} := \sum_{i=t}^{k} \eta_i^{-1}$, $S_k := S_{1, k}$,
    and $\bar{x}_{t, k} := S_{t, k}^{-1} \sum_{i=t}^{k} \eta_i^{-1} x_i$ for $t,k\in\N$.
    Using convexity of $f$, we get
    \begin{align*}
        f(\bar{x}_{k_0,K}) - f(x^\star)
        &\leq
        S_K^{-1}\sum_{k=k_0}^{K} \eta_k^{-1} f(x_{k}) - f(x^\star) \\
        &\leq
        \frac{e_{k_0}}{2S_{k_0, K}} + \frac{\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle}{S_{k_0, K}}
            + \frac{\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2)}{S_{k_0, K}} \\
        &\phantom{===} + \frac{\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k}{S_{k_0, K}},
    \end{align*}
    for all $K\in\N$, as desired.
    Note that 
    \[
        \bar{x}_K = \frac{S_{1, k_0-1}}{S_K} \bar{x}_{1, k_0-1} + \frac{S_{k_0, K}}{S_K} \bar{x}_{k_0, K}
    \]
    and
    \[
        \frac{S_{1, k_0-1}}{S_K} + \frac{S_{k_0, K}}{S_K} = 1,
    \]
    hence, using convexity of $f$ again, we have
    \[
        f(\bar{x}_K) - f(x^\star) \leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        + \frac{S_{k_0, K}}{S_K} (f(\bar{x}_{k_0, K}) - f(x^\star)).
    \]
    Combining with the latest bound on $(f(\bar{x}_{k_0, K}) - f(x^\star))$, we arrive at
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        +
        \frac{e_{k_0}}{2S_K}
            + \frac{\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle}{S_K} \\
        &\phantom{===} + \frac{\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2)}{S_K}
            + \frac{\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k}{S_K}.
    \end{align*}
    Next, we will derive the desired bound for $\dist(\bar{x}_K, \mathcal{X})$.
    Our starting point is inequality
    \eqref{eq:proof:lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case:dist-bound}
    from the proof of \cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case}.
    Multiplying both sides by $\gamma_k^{-1}\eta_k^{-2}$ yields
    \begin{align*}
        \gamma_k^{-1}\eta_k^{-2} \dist(x_{k+1}, \mathcal{X})^2 &\leq
        \gamma_k^{-1}\eta_k^{-2}(1-\mu\eta_k)\dist(x_k, \mathcal{X})^2
            + \mathcal{O}(\gamma_k) \\
            &\phantom{===}
            + 2\gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle
            + 2\eta_k^{-1}\alpha_k - \tau \eta_k^{-1} \dist(x_k, \mathcal{X}),
    \end{align*}
    for all $k \geq k_1$.
    We have already shown that $\eta_k^{-2} (1 - \mu\eta_k) \leq \eta_{k-1}^2$ for all $k\in\N$.
    Also, since $\gamma_k$ is nondecreasing, we have $\gamma_k^{-1} \geq \gamma_{k+1}^{-1}$ for all $k\in\N$.
    Combining these two facts, we get
    \begin{align*}
        \gamma_{k+1}^{-1}\eta_k^{-2} \dist(x_{k+1}, \mathcal{X})^2
        &\leq
        \gamma_k^{-1}\eta_{k-1}^2 \dist(x_k, \mathcal{X})^2
            + \mathcal{O}(\gamma_k) + 2\gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle \\
            &\phantom{===}
            + 2\eta_k^{-1}\alpha_k - \tau \eta_k^{-1} \dist(x_k, \mathcal{X}),
    \end{align*}
    for all $k\geq k_1$.
    Setting $d_j := \gamma_j^{-1} \eta_{j-1}^{-2} \dist(x_j, \mathcal{X})^2 \enspace \forall j\in\N$ and rearranging again, we obtain
    \[
        \tau \eta_k^{-1} \dist(x_k, \mathcal{X})
        \leq
        d_k - d_{k+1}
            + \mathcal{O}(\gamma_k)
            + 2\eta_k^{-1}\alpha_k
            + 2\gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle,
    \]
    for all $k\geq k_1$. Summing over $k=k_1,\dots, K$ for $K\in\N$, we have
    \begin{align*}
        \tau \sum_{k=k_1}^{K} \eta_k^{-1} \dist(x_k, \mathcal{X})
        &\leq
        d_{k_1} - d_{K+1} + \sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)
            + 2 \sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k \\
        &\phantom{===} + 2\sum_{k=k_1}^K \gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle \\
        &\leq
        d_{k_1} + \sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)
            + 2 \sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k \\
        &\phantom{===} + 2\sum_{k=k_1}^K \gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle,
    \end{align*}
    where we used $d_{K+1} \geq 0$ in the last step.
    The distance functional $x\mapsto\dist(x,\mathcal{X})$
    is convex (\textcolor{red}{TODO: Show this in an example.}), so if we divide both sides of
    the above inequality by $\tau \cdot S_{k_1, K}$, we obtain
    \begin{align*}
        \dist(\bar{x}_{k_1, K}, \mathcal{X})
        &\leq
        \frac{d_{k_1}}{\tau S_{k_1, K}} + \frac{\sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)}{\tau S_{k_1, K}}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k}{\tau S_{k_1, K}} \\
        &\phantom{===} + 2 \frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_{k_1, K}}.
    \end{align*} 
    To derive a bound for $\dist(\bar{x}_K, \mathcal{X})$, we proceed similarly as before. We have
    \[
        \bar{x}_K = \frac{S_{1, k_1-1}}{S_K} \bar{x}_{1, k_1-1} + \frac{S_{k_1, K}}{S_K} \bar{x}_{k_1, K}
    \]
    and
    \[
        \frac{S_{1, k_1-1}}{S_K} + \frac{S_{k_1, K}}{S_K} = 1.
    \]
    Combining with the latest bound on $\dist(\bar{x}_{k_1, K}, \mathcal{X})$, we obtain the desired bound
    \begin{align*}
        \dist(\bar{x}_K, \mathcal{X})
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
            + \frac{S_{k_1, K}}{S_K} \dist(\bar{x}_{k_1, K}, \mathcal{X}) \\
        &\leq
        \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
            + \frac{d_{k_1}}{\tau S_K}
            + \frac{\sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)}{\tau S_K}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k}{\tau S_K} \\
        &\phantom{===} + 2 \frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_K}
    \end{align*} 
    almost surely, or all $K \geq k_1$.
    This concludes the proof.
\end{proof}

\begin{theorem}[Convergence rates in expectation - strongly convex case]
    \label{thm:convergence-rates-in-expectation}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point}
    hold and assume that we are in the setting $\mu > 0$.
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$
    be the same as in \cref{lem:parameters-for-strongly-convex-case}.
    Further, let the iterate weights $(w_k)_{k\in\N}$
    be defined as $w_k := \eta_k^{-1}$, for all $k\in\N$.
    Then, the averaged iterate $\bar{x}_K$,
    generated by \cref{alg:seqprox-sgd} with parameters $(\eta_k)_{k\in\N_{0}}$,
    $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$, $(w_k)_{k\in\N}$, satisfies
    \[
        \E| f(\bar{x}_{K}) - f(x^\star) | = \max(1, \tau^{-1}) \, \mathcal{O} \Big( \frac{\log^{2e}(K)}{K} \Big).
    \]
    Further, it also holds that
    \[
        \E(\dist(\bar{x}_{K}, \mathcal{X})) = \tau^{-1} \, \mathcal{O} \Big( \frac{\log^{e}(K)}{K} \Big).
    \]
\end{theorem}
\begin{proof}
    \Cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)} gives us
    an upper bound for $f(\bar{x}_K) - f(x^\star)$,
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        +
        \frac{e_{k_0}}{2S_K}
            + \frac{\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle}{S_K} \\
        &\phantom{===} + \frac{\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2)}{S_K}
            + \frac{\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k}{S_K}.
    \end{align*}
    almost surely, for all $K\in\N$ with $K \geq k_0$. Note that
    \[
        \E(\langle z_k, x_k - x^\star \rangle)
        = \E(\E_k(\langle z_k, x_k - x^\star \rangle))
        = \E(\langle \E_k(z_k), x_k - x^\star \rangle)
        = 0,
    \]
    for all $k\in\N$. Hence, taking expectations on both sides of the above inequality, we get
    \begin{align}
        \label{eq:proof:thm:convergence-rates-in-expectation-0}
        \E(f(\bar{x}_K) - f(x^\star))
        &\leq
        \frac{S_{1, k_0-1}}{S_K} \E(f(\bar{x}_{1, k_0-1}) - f(x^\star))
        +
        \frac{\E(e_{k_0})}{2S_K}
        +
        \frac{\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2)}{S_K} \\
        &\phantom{===} + \frac{\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k}{S_K}. \notag
    \end{align}
    By convexity, we have for all $k\in\N$
    \begin{align*}
        f(\bar{x}_k) - f(x^\star) &\geq \langle \tilde{\nabla} f(x^\star), \bar{x}_k - x^\star \rangle \\
                            &= \langle \tilde{\nabla} f(x^\star), \bar{x}_k - \proj(\bar{x}_k) \rangle
                                + \langle \tilde{\nabla} f(x^\star), \proj(\bar{x}_k) - x^\star \rangle,
    \end{align*}
    for any subgradient $\tilde{\nabla} f(x^\star) \in \partial f(x^\star)$. Optimality of $x^\star$
    for $f$ on $\mathcal{X}$ implies that there exists a subgradient
    $\tilde{\nabla} f(x^\star) \in \partial f(x^\star)$ such that
    $\langle \tilde{\nabla} f(x^\star), \proj(\bar{x}_k) - x^\star \rangle \geq 0$.
    Combining these facts with the above inequality, and applying Cauchy-Schwarz,
    we obtain
    \begin{equation}
        \label{eq:proof:thm:convergence-rates-in-expectation}
        f(\bar{x}_k) - f(x^\star) \geq - \norm{\tilde{\nabla} f(x^\star)} \dist(\bar{x}_k, \mathcal{X}),
    \end{equation}
    for all $k\in\N$.
    Taking expectations in the bound for $\dist(\bar{x}_K, \mathcal{X})$
    from \cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)},
    we know that there exists $k_1\in\N$ such that for all $K\in\N$ with $K\geq k_1$,
    it holds that
    \[
        \E(\dist(\bar{x}_K, \mathcal{X}))
        \leq
        \frac{S_{1, k_1-1}}{S_K} \E(\dist(\bar{x}_{1, k_1-1}, \mathcal{X}))
            + \frac{\E(d_{k_1})}{\tau S_K}
            + \frac{\sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)}{\tau S_K}
            + 2 \frac{\sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k}{\tau S_K}.
    \]
    The first two terms are constants.
    Since $\eta_k^{-1} = (\mu k)/2$, it follows that $S_K = \mathcal{O}(K^2)$. Also, since
    $\alpha_k = \mathcal{O}(1/k)$, we have $\eta_k^{-1} \alpha_k = \mathcal{O}(1)$.
    Therefore,
    \begin{align*}
        \E(\dist(\bar{x}_K, \mathcal{X}))
        &=
        \mathcal{O}(K^{-2})
            + \mathcal{O}\Big( K^{-2} \sum_{k=1}^{K} \log^e(k) \Big)
            + \mathcal{O}( K^{-1} ) \\
        &=
        \mathcal{O}(\tau^{-1} K^{-2})
            + \mathcal{O}( \tau^{-1} \log^e(K) / K )
            + \mathcal{O}( \tau^{-1} K^{-1} )  \\
        &= \tau^{-1} \, \mathcal{O} \Big( \frac{\log^{e}(K)}{K} \Big).
    \end{align*}
    Combining with \eqref{eq:proof:thm:convergence-rates-in-expectation}, we obtain
    \[
        \E( f(\bar{x}_k) - f(x^\star) )
        \geq - \norm{\nabla f(x^\star)} \cdot \mathcal{O} \Big( \frac{\log^{2e}(K)}{\tau K} \Big).
    \]
    Note that $\mathcal{O}(\gamma_k^2) = \mathcal{O}(\log^{2e}(k))$ and
    $\eta_k^{-1}\gamma_k\alpha_k = \mathcal{O}(\log^e(k))$,
    so for the upper bound in \eqref{eq:proof:thm:convergence-rates-in-expectation-0}, we find that
    \begin{align*}
        \E(f(\bar{x}_{K}) - f(x^\star))
        &\leq
        \mathcal{O}(K^{-2})
        + \mathcal{O} \Big( K^{-2} \sum_{k=1}^{K} \log^{2e}(k) \Big)
        + \mathcal{O} \Big( K^{-2} \sum_{k=1}^{K} \log^{e}(k) \Big) \\
        &=
        \mathcal{O} \Big( \frac{\log^{2e}(K)}{K} \Big).
    \end{align*} 
    Putting the two bounds together, we obtain
    \[
        \E| f(\bar{x}_{K}) - f(x^\star) |
        = \max(1, \tau^{-1}) \, \mathcal{O} \Big( \frac{\log^{2e}(K)}{K} \Big),
    \]
    as desired.
\end{proof}

\section{High-probability guarantees}

In the previous section, we proved
that the expected values
$\E|f(\bar{x}_k) - f(x^\star)|$ and $\dist(\bar{x}_k, \mathcal{X})$
converge to zero as $k\to\infty$, provided
the parameters $(\eta_k)_{k\in\N_0}$, $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$,
and $(w_k)_{k\in\N}$ are suitably chosen.
We will now turn our attention towards establishing \textit{high-probability guarantees}
for convergence, in contrast
to guarantees that hold only in expectation. In particular, we will investigate under which
conditions we can guarantee that an iterate $\bar{x}_k$ is both a) close to the feasible set
and b) has function value close to the optimal value $f(x^\star)$ with high probability.
This notion of "closeness with high probability" is formalized in the following definition.
\begin{definition}[$(\epsilon, \delta)$-solution]
    Let $\epsilon\in(0,\infty)$ and $\delta\in(0, 1)$.
    We call a random variable $x\colon\Omega\to\R^n$ an \textbf{$(\epsilon, \delta)$-solution} of \eqref{eq:main-problem-copy},
    if
    \[
        \Prob\Big( \max\big( | f(x) - f(x^\star) |, \dist(x,\mathcal{X}) \big) \geq \epsilon \Big) \leq \delta.
    \]
\end{definition}
In other words, an iterate $x_k$ of \cref{alg:seqprox-sgd} is an $(\epsilon, \delta)$-solution, if we can guarantee that
$| f(x_k) - f(x^\star) | < \epsilon$ and $\dist(x_k, \mathcal{X}) < \epsilon$ with probability
greater than $1 - \delta$.
We can now state the central question we aim to answer in this section as follows:
\begin{list}{}
  {
    \leftmargin=5pt
    \rightmargin=5pt
    \itemsep=0pt
    \parsep=-2pt
  }
\item \textit{For any given $(\epsilon, \delta)$, how many iterations are needed
    to guarantee that the sequence $(\bar{x}_k)_{k\in\N}$
    generated by \cref{alg:seqprox-sgd} reaches an
    $(\epsilon, \delta)$-solution of \eqref{eq:main-problem-copy}?}
\end{list}
Our approach to answer this question is standard. We rely
on the Azuma-Hoeffding inequality (\cref{prop:azuma-hoeffding}) and
the almost-sure bounds we derived in the previous sections.
\begin{lemma}
    \label{lem:xk-is-martingale}
    Let $(\nu_k)_{k\in\N}$ be an arbitrary sequence of real numbers
    and let $(Y_k)_{k\in\N}$ be a bounded sequence of random variables such that
    $Y_k$ is a function of $x_k$ for all $k\in\N$.
    Then the stochastic process $(X_k)_{k\in\N_0}$, where $X_0 := 0$
    and
    \[
        X_k := \sum_{i=1}^k \nu_i \langle z_i, Y_i \rangle
    \]
    for $k\in\N$, is a martingale with respect to the filtration $(\mathcal{G}_k)_{k\in\N_0}$
    defined by $\mathcal{G}_k := \sigma(z_0, \dots, z_k) \, \forall k\in\N_0$. 
\end{lemma}
\begin{proof}
    First, note that $Y_k$ is a function of $z_{0}, z_1 \dots, z_{k-1}$
    for all $k\in\N$. Indeed, for $k = 1$, we have
    \[
        x_{1} = x_0 + \eta_0 g_0 = x_0 + \eta_0 (z_0 + \nabla \psi_0(x_0)),
    \]
    and $x_0, \nabla \psi_0(x_0), \eta_0$ are deterministic quantities. Assuming that
    $x_k$ is a function of $z_0, z_{1}, \dots, z_{k-1}$, it follows inductively that
    \[
        x_{k+1} = x_{k} + \eta_k g_k = x_{k} + \eta_k (z_k + \nabla \psi_k(x_k))
    \]
    is a function of $z_{1}, \dots, z_{k}$. It follows that $Y_k$, as a function of $x_k$,
    is also a function of $z_0,z_1,\dots,z_{k-1}$, hence $Y_k$ is $\sigma(z_0, \dots, z_{k-1})$-measurable.
    Thus, the process $(X_k)_{k\in\N_0}$
    is adapted to the filtration $\mathcal{G}$.
    With probability one we also have, for all $k\in\N_0$,
    \[
        |X_k|
        \leq \sum_{i=1}^k |\nu_i \langle z_i, Y_i \rangle|
        \leq \sum_{i=1}^k \nu_i \norm{z_i} \norm{Y_i}
        = \sum_{i=1}^k \nu_i \mathcal{O}(\gamma_i) \mathcal{O}(1) \leq \nu_k \mathcal{O}(k \gamma_k),
    \]
    where the third step follows from \cref{lem:bound-on-norm(zk)} and the assumption that
    $(Y_k)_{k\in\N}$ is bounded.
    In particular,
    \[
        \E|X_k| < \infty
    \]
    for all $k\in\N_0$.
    Finally, we have
    \begin{align*}
        \E(X_{k} \mid \mathcal{G}_{k-1})
            &= \E\big( \nu_k \langle z_k, Y_k \rangle + X_{k - 1} \mid \mathcal{G}_{k-1} \big) \\
            &= \nu_k \E\big( \langle z_k, Y_k \rangle \mid \mathcal{G}_{k-1} \big) + X_{k - 1} \\
            &= \nu_k \big\langle \E(z_k \mid \mathcal{G}_{k-1}), Y_k \big\rangle + X_{k - 1} \\
            &= \nu_k \big\langle \E(z_k \mid x_k), Y_k \big\rangle + X_{k - 1} \\
            &= X_{k - 1},
    \end{align*}
    where we used that $x_k$ is a function of $z_{0}, \dots, z_{k-1}$ and,
    per definition of stochastic gradients, $\E(z_k \mid x_k) = 0$.
    Therefore, $(X_k)_{k\in\N_0}$ is a martingale.
\end{proof}

\subsection{Convex case}

\begin{theorem}[Convergence with high probability - convex case]
    \label{thm:convergence-with-high-probability-convex-case}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point}
    hold.
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$
    be the same as in \cref{lem:parameters-for-convex-case}.
    Further, let the iterate weights $(w_k)_{k\in\N}$
    be defined as $w_k := \eta_k$, for all $k\in\N$.
    Then, for arbitrary $\delta\in(0,\infty)$ and $K\in\N$, the
    averaged iterate $\bar{x}_K$, generated by \cref{alg:seqprox-sgd}
    with parameters $(\eta_k)_{k\in\N_{0}}$,
    $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$, $(w_k)_{k\in\N}$, satisfies the
    bounds (\textcolor{red}{TODO: Define M and R somewhere, $\tau$ too.})
    \[
        |f(\bar{x}_K) - f(x^\star)|
        \leq
        \delta \max(1, \tau^{-1}) \, \mathcal{O}\Big( \frac{M R \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
    \]
    and
    \[
        \dist(\bar{x}_K, \mathcal{X})
        \leq
        \delta \tau^{-1} \, \mathcal{O}\Big( \frac{M R \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
    \]
    with probability at least $1 - \exp(-\delta^2 / 2)$.
\end{theorem}
\begin{proof}
    Let $K\in\N$.
    From \cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)-convex-case}, we have the almost-sure bound
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        + \frac{\norm{x^\star - x_{k_0}}^2}{2S_K}
        + \frac{\sum_{k=k_0}^{K} \eta_k^2\mathcal{O}(\gamma_k^2)}{2 S_K} \notag \\
        &\phantom{===}
            + \frac{\sum_{k=k_0}^K \eta_k\gamma_k\alpha_k}{S_K}
            + \frac{\sum_{k=k_0}^K \eta_k\langle z_k, x_k - x^\star \rangle}{S_K}.
    \end{align*}
    By \cref{thm:almost-sure-convergence}, the numerators in the first two terms are bounded almost surely,
    whereas the sum of the last two terms is on the order
    $\mathcal{O}\big( \log^{(1 + 3e) / 2}(K) / \sqrt{K} \big)$,
    as argued in the proof of \cref{thm:convergence-rates-in-expectation-convex-case}.
    Thus,
    \[
        f(\bar{x}_K) - f(x^\star)
        \leq
        \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2}(K)}{\sqrt{K}} \Big)
            + \frac{\sum_{k=k_0}^K \eta_k \langle z_k, x_k - x^\star \rangle}{S_K}
    \]
    almost surely.
    To prove the claim, we will first bound the probability that the random variable
    \[
        \sum_{k=k_0}^K \nu_k \langle z_k, x_k - x^\star \rangle
    \]
    is large, where
    \[
        \nu_k := \frac{\eta_k}{S_K}.
    \]
    The sequence $(x_k)_{k\in\N_0}$ is bounded a.\,s. by \cref{thm:almost-sure-convergence}.
    Therefore, \cref{lem:xk-is-martingale} implies that $(X_k)_{k\in\N_0}$ is a martingale, and
    we also have
    \[
        |X_{k} - X_{k-1}| = \nu_k |\langle z_k, x_k - x^\star \rangle| \leq \nu_k \norm{z_k} \norm{x_k - x^\star}
        =: c_k
    \]
    for all $k\in\N$. We can thus apply the Azuma-Hoeffding inequality (\cref{prop:azuma-hoeffding}) to obtain
    \[
        \Prob\Big( \sum_{k=1}^K \nu_k \langle z_k, x_k - x^\star \rangle \geq t \Big)
        \leq
        \exp\Big( \frac{-t^2}{2 \sum_{k=1}^K c_k^2} \Big),
    \]
    for all $t \in (0, \infty)$.
    By \cref{lem:bound-on-norm(zk)}, it holds that $\norm{z_k} \leq M \log^e(k) \, \forall k\in\N_0$, where
    $~M := 4\max(1, \min_{k\in\N_0}\gamma_k)\max(L_\text{loc}, G)$. Further,
    using \eqref{eq:proof:thm:convergence-rates-in-expectation-convex-case:SK-complexity}, we have
    \[
        \nu_k
        = \frac{\eta_k}{S_K}
        = \mathcal{O} \Big( \frac{1}{\log^{(1 + 3e) / 2} (k) \sqrt{k} S_K} \Big)
        = \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{\sqrt{k} \sqrt{K}} \Big).
    \]
    % \[
    %     S_K = \mathcal{O}\Big( \frac{K^{1-c}}{\log^{(1 + 3e) / 2} (K)} \Big)
    % \]
    % \[
    %     \eta_k = \mathcal{O}\Big( \frac{1}{k^c \log^{(1 + 3e) / 2}(k+1)} \Big),\,
    %     \gamma_k = \mathcal{O}(\log^{e}(k+1)),\,
    %     \alpha_k = \mathcal{O}\Big( \frac{1}{k^d} \Big)
    % \]
    for all $k\in \{\, 1,\dots K \,\}$.
    Let
    $R\in(0,\infty)$ be large enough
    such that $\sup_{k\in\N} \norm{x_k - x^\star} \leq R$
    almost surely. 
    We obtain
    \[
        c_k
        = \nu_k \norm{z_k} \norm{x_k - x^\star}
        \leq \mathcal{O}\Big( \frac{M R \log^{(1 + 3e) / 2} (K)}{\sqrt{k} \sqrt{K}} \Big),
    \]
    which implies
    \[
        \sum_{k=1}^K c_k^2
        \leq \mathcal{O}\Big( \frac{M^2 R^2 \log^{1 + 3e}(K)}{K} \Big) \sum_{k=1}^K \frac{1}{k}
        = \mathcal{O}\Big( \frac{M^2 R^2 \log^{2 + 3e}(K)}{K} \Big)
    \]
    almost surely, where the last step follows from
    \[
        \sum_{k=1}^K \frac{1}{k} = \mathcal{O}(\log(K)).
    \]
    Hence, it holds that
    \[
        \Prob\Big( \sum_{k=1}^K \nu_k \langle z_k, x_k - x^\star \rangle \geq t \Big)
        \leq
        \exp\left(-\frac{1}{2}t^2 \mathcal{O}\Big(\frac{K}{2 M^2 R^2 \log^{2 + 3e}(K)}\Big) \right),
    \]
    for all $t \in (0,\infty)$. Substituting $t := \delta \, \mathcal{O}(MR \log^{(2+3e) / 2}(K) / \sqrt{K})$ for arbitrary $\delta \in (0, 1)$, we obtain
    \[
        \Prob\left( \sum_{k=1}^K \nu_k \langle z_k, x_k - x^\star \rangle
                    \geq
                    \delta \, \mathcal{O}\Big( \frac{MR\log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
            \right)
        \leq
        \exp\Big( -\frac{1}{2} \delta^2 \Big).  
    \]
    Therefore,
    \begin{equation}
        \label{eq:proof:thm:convergence-with-high-probability:upper-bound-on-f-convex-case}
        f(\bar{x}_K) - f(x^\star)
        \leq
        \mathcal{O}\Big( \frac{\log^{2e}(K)}{K} \Big)
            + \delta \, \mathcal{O}\Big( \frac{MR\log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
    \end{equation}
    with probability at least $1 - \exp(-\delta^2/2)$.
    To derive a lower bound, we will first prove the second claim, which follows from
    essentially the same argument. For $k\in\N$, define
    \[
        \tilde{\nu}_k := \frac{\gamma_k^{-1}\eta_k}{\tau S_K}
    \]
    and
    \[
        \tilde{X}_k := \sum_{k=1}^K \tilde{\nu}_k \langle z_k, x_k - \proj(x_k) \rangle,
    \]
    as well as $\tilde{X}_0 := 0$.
    The process $(x_k - \proj(x_k))_{k\in\N}$ is
    a function of $x_k$, and bounded by \cref{thm:almost-sure-convergence}.
    Hence, applying \cref{lem:xk-is-martingale}, we find that $(\tilde{X}_k)_{k\in\N_0}$ is a martingale.
    Further, we have
    \[
        |\tilde{X}_k - \tilde{X}_{k-1}|
        = \tilde{\nu}_k |\langle z_k, x_k - \proj(x_k) \rangle|
        \leq \tilde{\nu}_k \norm{z_k} \dist(x_k,\mathcal{X}) =: d_k,
    \]
    for all $k\in\N$.
    It holds that
    \[
        \tilde{\nu}_k = \frac{\gamma_k^{-1}\eta_k}{\tau S_K}
        \leq (\tau\gamma_0)^{-1}\frac{\eta_k}{S_K}
        = (\tau\gamma_0)^{-1}\nu_k
        \leq (\tau\gamma_0)^{-1} \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2} (K)}{k^{1/2} K^{1/2}} \Big),
    \]
    for all $k\in\N$.
    Letting $\tilde{R}\in(0,\infty)$ be large enough such that
    $\sup_{k\in\N} \dist(x_k, \mathcal{X}) \leq \tilde{R}$ a.\,s., we thus have
    \[
        d_k \leq (\tau\gamma_0)^{-2} \, \mathcal{O}\Big( \frac{M \tilde{R} \log^{(1 + 3e) / 2} (K)}{k^{1/2} K^{1/2}} \Big),
    \]
    for all $k\in\N$.
    It follows that
    \[
        \sum_{k=1}^K d_k^2
        \leq
        (\tau\gamma_0)^{-2} \mathcal{O}\Big( \frac{M^2 \tilde{R}^2 \log^{1 + 3e} (K)}{K} \Big) \sum_{k=1}^K \frac{1}{k}
        =
        (\tau\gamma_0)^{-2} \mathcal{O}\Big( \frac{M^2 \tilde{R}^2 \log^{2 + 3e} (K)}{K} \Big).
    \]
    Applying the Azuma-Hoeffding inequality (\cref{prop:azuma-hoeffding}),
    we obtain
    \begin{align*}
        \Prob\Big( \sum_{k=1}^K \tilde{\nu}_k \langle z_k, x_k - \proj(x_k) \rangle \geq t \Big)
        &\leq
        \exp\Big( \frac{-t^2}{2\sum_{k=1}^K d_k^2} \Big) \\
        &\leq
        \exp\left(-\frac{1}{2}t^2 \mathcal{O}\Big(\frac{\tau_k^2 \gamma_0^2 K}{2 M^2 \tilde{R}^2 \log^{2 + 3e}(K)}\Big) \right),
    \end{align*}
    for all $t\in(0, \infty)$ and $K\in\N$. Substituting
    $t := \delta \, \mathcal{O}(M\tilde{R} \log^{(2 + 3e) / 2}(K) / \tau\gamma_0\sqrt{K})$ for arbitrary $\delta\in(0, \infty)$,
    we obtain
    \[
        \Prob\left( \sum_{k=1}^K \tilde{\nu}_k \langle z_k, x_k - \proj(x_k) \rangle
                \geq \delta (\tau\gamma_0)^{-1} \mathcal{O}\Big( \frac{M\tilde{R} \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big) \right)
        \leq
        \exp\Big( -\frac{1}{2}\delta^2 \Big).
    \]
    for all $K\in\N$. We therefore have, for any $\delta\in(0,\infty)$ and $K\in\N$,
    \begin{align}
        \label{eq:proof:thm:convergence-with-high-probability-convex-case}
        \dist(\bar{x}_K, \mathcal{X})
        &\leq \tau^{-1} \, \mathcal{O}\Big( \frac{\log^{(1 + 3e) / 2}(K)}{\sqrt{K}} \Big) + 2\delta (\tau\gamma_0)^{-1}
        \mathcal{O}\Big( \frac{M\tilde{R} \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big) \notag \\
        &=
        \delta \tau^{-1} \,
        \mathcal{O}\Big( \frac{M\tilde{R} \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
    \end{align}
    with probability at least $1 - \exp(-\delta^2/2)$.
    Note that for any valid choice of $R$, we can choose $\tilde{R}$ such that 
    $\tilde{R} \leq R$, which brings us to the desired bound.
    We can derive a lower bound on $f(\bar{x}_K) - f(x^\star)$ via
    \begin{align*}
        f(\bar{x}_K) - f(x^\star) &\geq \langle \tilde{\nabla} f(x^\star), \bar{x}_K - x^\star \rangle \\
                            &= \langle \tilde{\nabla} f(x^\star), \bar{x}_K - \proj(\bar{x}_K) \rangle
                                + \langle \tilde{\nabla} f(x^\star), \proj(\bar{x}_K) - x^\star \rangle \\
                            &\geq \langle \tilde{\nabla} f(x^\star), \bar{x}_K - \proj(\bar{x}_K) \rangle,
    \end{align*}
    where the first step follows from convexity, and the second from optimality of $x^\star$ on $\mathcal{X}$.
    Cauchy-Schwarz now implies
    \[
        f(\bar{x}_K) - f(x^\star) \geq - \norm{\tilde{\nabla} f(x^\star)} \dist(\bar{x}_K, \mathcal{X}).
    \]
    Combining with the high-probability bound \eqref{eq:proof:thm:convergence-with-high-probability-convex-case}, we have,
    for any $\delta\in(0, \infty)$,
    \[
        f(\bar{x}_K) - f(x^\star)
        \geq - \tau^{-1} \norm{\tilde{\nabla} f(x^\star)}
            \delta \tau^{-1} \, \mathcal{O}\Big( \frac{M\tilde{R} \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
    \]
    with probability at least $1 - \exp(-\delta^2/2)$. Note that we can choose $\tilde{R} \leq R$
    without loss of generality. Hence, by combining with the upper bound \eqref{eq:proof:thm:convergence-with-high-probability:upper-bound-on-f-convex-case},
    we have for any $\delta\in(0,\infty)$ and all $K\in\N$,
    \[
        |f(\bar{x}_K) - f(x^\star)|
        \leq
        \max(1, \tau^{-1}) \, \mathcal{O}\Big( \frac{M R \log^{(2 + 3e) / 2}(K)}{\sqrt{K}} \Big)
    \]
    with probability at least $1 - \exp(-\delta^2/2)$, as desired.
\end{proof}

\subsection{Strongly convex case}

\begin{theorem}[Convergence with high probability - strongly convex case]
    \label{thm:convergence-with-high-probability}
    Let \cref{ass:basic-assumptions,ass:compact-support-without-zero,ass:slater-point}
    hold and assume that we are in the setting $\mu > 0$.
    Let the parameters $(\eta_k)_{k\in\N_0}, (\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$
    be the same as in \cref{lem:parameters-for-strongly-convex-case}.
    Further, let the iterate weights $(w_k)_{k\in\N}$
    be defined as $w_k := \eta_k^{-1}$, for all $k\in\N$.
    Additionally, we define
    $a_{\tau^{-1}} := \max\big( 2\norm{\tilde{\nabla} f(x^\star)} (\tau\gamma_0)^{-1}, 1 \big)$.
    Then, for arbitrary $\delta\in(0,\infty)$ and $K\in\N$, the
    averaged iterate $\bar{x}_K$, generated by \cref{alg:seqprox-sgd}
    with parameters $(\eta_k)_{k\in\N_{0}}$,
    $(\gamma_k)_{k\in\N_0}$, $(h_k)_{k\in\N_0}$, $(w_k)_{k\in\N}$, satisfies the
    bounds
    \[
        |f(\bar{x}_K) - f(x^\star)|
        \leq
        \tau^{-1} \mathcal{O}\Big( \frac{\log^{2e}(K)}{K} \Big)
            + \delta a_{\tau^{-1}} \frac{M R \log^{e}(K)}{\sqrt{K}}
    \]
    and
    \[
        \dist(\bar{x}_K, \mathcal{X})
        \leq \tau^{-1} \, \mathcal{O}\Big( \frac{\log^e(K)}{K} \Big)
        + 2\delta \tau^{-1} \gamma_0^{-1} \frac{M R \log^{e}(K)}{\sqrt{K}}
    \]
    with probability at least $1 - \exp(-\delta^2 / 2)$.
\end{theorem}
\begin{proof}
    Let $K\in\N$.
    From \cref{lem:upper-bound-on-f(xk)-f(xstar)-and-dist(xk)}, we have the almost-sure bound
    \begin{align*}
        f(\bar{x}_K) - f(x^\star)
        &\leq
        \frac{S_{1, k_0-1}}{S_K} (f(\bar{x}_{1, k_0-1}) - f(x^\star))
        +
        \frac{e_{k_0}}{2S_K}
            + \frac{\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle}{S_K} \\
        &\phantom{===} + \frac{\sum_{k=k_0}^{K} \mathcal{O}(\gamma_k^2)}{S_K}
            + \frac{\sum_{k=k_0}^{K}\eta_k^{-1}\gamma_k\alpha_k}{S_K}.
    \end{align*}
    % and
    % \begin{align*}
    %     \dist(\bar{x}_K, \mathcal{X})
    %     &\leq
    %     \frac{S_{1, k_1-1}}{S_K} \dist(\bar{x}_{1, k_1-1}, \mathcal{X})
    %         + \frac{d_{k_1}}{\tau S_K}
    %         + \frac{\sum_{k=k_1}^{K} \mathcal{O}(\gamma_k)}{\tau S_K}
    %         + 2 \frac{\sum_{k=k_1}^{K}\eta_k^{-1}\alpha_k}{\tau S_K} \\
    %     &\phantom{===} + 2\frac{\sum_{k=k_1}^K \gamma_k^{-1}\eta_k^{-1} \langle z_k, x_k - \proj(x_k) \rangle}{\tau S_K},
    % \end{align*}
    By \cref{thm:almost-sure-convergence}, the numerators in the first two terms are bounded almost surely,
    whereas the sum of the last two terms is on the order $\mathcal{O}(\log^{2e}(K) / K)$,
    as argued in the proof of \cref{thm:convergence-rates-in-expectation}.
    Thus,
    \[
        f(\bar{x}_K) - f(x^\star)
        \leq
        \mathcal{O}\Big( \frac{\log^{2e}(K)}{K} \Big)
            + \frac{\sum_{k=k_0}^K \eta_k^{-1}\langle z_k, x_k - x^\star \rangle}{S_K}
    \]
    almost surely.
    To prove the claim, we will first bound the probability that the random variable
    \[
        \sum_{k=k_0}^K \nu_k \langle z_k, x_k - x^\star \rangle
    \]
    is large, where
    \[
        \nu_k := \frac{\eta_k^{-1}}{S_K}.
    \]
    % First, note that $x_k$ is a function of $z_{0}, z_1 \dots, z_{k-1}$. Indeed, for $k = 1$, we have
    % \[
    %     x_{1} = x_0 + \eta_0 g_0 = x_0 + \eta_0 (z_0 + \nabla \psi_0(x_0)),
    % \]
    % and $x_0, \nabla \psi_0(x_0), \eta_0$ are deterministic quantities. Assuming that
    % $x_k$ is a function of $z_0, z_{1}, \dots, z_{k-1}$, it follows inductively that
    % \[
    %     x_{k+1} = x_{k} + \eta_k g_k = x_{k} + \eta_k (z_k + \nabla \psi_k(x_k))
    % \]
    % is a function of $z_{1}, \dots, z_{k}$.
    % Thus, the process
    % \[
    %     X_k := \sum_{i=1}^k \nu_i \langle z_i, x_i - x^\star \rangle
    % \]
    % is adapted to the filtration $\mathcal{G} := (\mathcal{G}_k)_{k\in\N_0}$, $\mathcal{G}_k := \sigma(z_0, z_1, \dots, z_k)$.
    % With probability one, we also have, for all $k\in\N_0$,
    % \[
    %     |X_k|
    %     \leq \sum_{i=1}^k |\nu_i \langle z_i, x_i - x^\star \rangle|
    %     \leq \sum_{i=1}^k \nu_i \norm{z_i} \norm{x_i - x^\star}
    %     = \sum_{i=1}^k \nu_i \mathcal{O}(\gamma_i) \mathcal{O}(1) \leq \nu_k \mathcal{O}(k \gamma_k),
    % \]
    % where the third step follows from \cref{thm:almost-sure-convergence,lem:bound-on-norm(zk)}.
    % In particular,
    % \[
    %     \E|X_k| < \infty
    % \]
    % for all $k\in\N_0$. Finally, we have
    % \begin{align*}
    %     \E(X_{k} \mid \mathcal{G}_{k-1})
    %         &= \E\big( \nu_k \langle z_k, x_k - x^\star \rangle + X_{k - 1} \mid \mathcal{G}_{k-1} \big) \\
    %         &= \nu_k \E\big( \langle z_k, x_k - x^\star \rangle \mid \mathcal{G}_{k-1} \big) + X_{k - 1} \\
    %         &= \nu_k \big\langle \E(z_k \mid \mathcal{G}_{k-1}), x_k - x^\star \big\rangle + X_{k - 1} \\
    %         &= \nu_k \big\langle \E(z_k \mid x_k ), x_k - x^\star \big\rangle + X_{k - 1} \\
    %         &= X_{k - 1},
    % \end{align*}
    % where we used that $x_k$ is a function of $z_{0}, \dots, z_{k-1}$ and,
    % per definition of stochastic gradients, $\E(z_k \mid x_k) = 0$.
    The sequence $(x_k)_{k\in\N_0}$ is bounded a.\,s. by \cref{thm:almost-sure-convergence}.
    Therefore, \cref{lem:xk-is-martingale} implies that $(X_k)_{k\in\N_0}$ is a martingale, and
    we also have
    \[
        |X_{k} - X_{k-1}| = \nu_k |\langle z_k, x_k - x^\star \rangle| \leq \nu_k \norm{z_k} \norm{x_k - x^\star}
        =: c_k
    \]
    for all $k\in\N$. We can thus apply the Azuma-Hoeffding inequality (\cref{prop:azuma-hoeffding}) to obtain
    \[
        \Prob\Big( \sum_{k=1}^K \nu_k \langle z_k, x_k - x^\star \rangle \geq t \Big)
        \leq
        \exp\Big( \frac{-t^2}{2 \sum_{k=1}^K c_k^2} \Big),
    \]
    for all $t \in (0, \infty)$.
    % Substituting $t := \epsilon \log^{e}(K) / K$, we obtain that
    % \[
    %     \Prob\Big( \sum_{i=1}^K \nu_k \langle z_k, x_k - x^\star \rangle \geq \epsilon \frac{\log^{e}(K)}{K} \Big)
    %     \leq
    %     \exp\Big( \frac{-\epsilon^2 \log^{2e}(K)}{2 K^2 \sum_{k=1}^K c_k^2} \Big)
    % \]
    % Fix $\delta\in(0, 1)$ and set (\textcolor{red}{TODO: Choose differently, look at duchi notes})
    % \[
    %     \epsilon := \sqrt{ 2 \log\Big( \frac{1}{\delta} \Big) \sum_{k=1}^K c_k^2 }.
    % \]
    % Then
    % \[
    %     \Prob\left( \sum_{i=1}^K \nu_k \langle z_k, x_k - x^\star \rangle
    %                 \geq \Big( 2 \log\Big( \frac{1}{\delta} \Big) \sum_{k=1}^K c_k^2 \Big)^{1/2} \right)
    %     \leq
    %     \delta.
    % \]
    By \cref{lem:bound-on-norm(zk)}, it holds that $\norm{z_k} \leq M \log^e(k) \, \forall k\in\N_0$, where
    $~M := 4\max(1, \min_{k\in\N_0}\gamma_k)\max(L_\text{loc}, G)$. Further, we have
    \[
        \nu_k
        = \frac{\eta_k^{-1}}{S_K}
        = \frac{k}{\sum_{i=1}^K i}
        = \frac{k}{K(K+1)}
        \leq \frac{1}{K},
    \]
    for all $k\in \{\, 1,\dots K \,\}$.
    Let
    $R\in(0,\infty)$ be large enough
    such that $\sup_{k\in\N} \norm{x_k - x^\star} \leq R$
    almost surely. 
    We obtain
    \[
        c_k
        = \nu_k \norm{z_k} \norm{x_k - x^\star}
        \leq \frac{M R \log^e(K)}{K},
    \]
    which implies
    \[
        \sum_{k=1}^K c_k^2
        \leq \sum_{k=1}^K \frac{M^2 R^2 \log^{2e}(K)}{K^2}
        = \frac{M^2 R^2 \log^{2e}(K)}{K}
    \]
    almost surely.
    Hence, it holds that
    \[
        \Prob\Big( \sum_{k=1}^K \nu_k \langle z_k, x_k - x^\star \rangle \geq t \Big)
        \leq
        \exp\Big( \frac{-t^2 K}{2 M^2 R^2 \log^{2e}(K)} \Big),
    \]
    for all $t \in (0,\infty)$. Substituting $t := \delta MR \log^e(K) / \sqrt{K}$ for arbitrary $\delta \in (0, 1)$, we obtain
    \[
        \Prob\Big( \sum_{k=1}^K \nu_k \langle z_k, x_k - x^\star \rangle \geq \delta \frac{MR\log^e(K)}{\sqrt{K}} \Big)
        \leq
        \exp\Big( -\frac{1}{2} \delta^2 \Big).  
    \]
    Therefore,
    \begin{equation}
        \label{eq:proof:thm:convergence-with-high-probability:upper-bound-on-f}
        f(\bar{x}_K) - f(x^\star)
        \leq
        \mathcal{O}\Big( \frac{\log^{2e}(K)}{K} \Big)
            + \delta \frac{MR\log^e(K)}{\sqrt{K}}
    \end{equation}
    with probability at least $1 - \exp(-\delta^2/2)$.
    To derive a lower bound, we will first prove the second claim, which follows from
    essentially the same argument. For $k\in\N$, define
    \[
        \tilde{\nu}_k := \frac{\gamma_k^{-1}\eta_k^{-1}}{\tau S_K}
    \]
    and
    \[
        \tilde{X}_k := \sum_{k=1}^K \tilde{\nu}_k \langle z_k, x_k - \proj(x_k) \rangle,
    \]
    as well as $\tilde{X}_0 := 0$.
    The process $(x_k - \proj(x_k))_{k\in\N}$ is
    a function of $x_k$, and bounded by \cref{thm:almost-sure-convergence}.
    Hence, applying \cref{lem:xk-is-martingale}, we find that $(\tilde{X}_k)_{k\in\N_0}$ is a martingale.
    Further, we have
    \[
        |\tilde{X}_k - \tilde{X}_{k-1}|
        = \tilde{\nu}_k |\langle z_k, x_k - \proj(x_k) \rangle|
        \leq \tilde{\nu}_k \norm{z_k} \dist(x_k,\mathcal{X}) =: d_k,
    \]
    for all $k\in\N$.
    It holds that
    \[
        \tilde{\nu}_k = \frac{\gamma_k^{-1}\eta_k^{-1}}{\tau S_K}
        \leq (\tau\gamma_0)^{-1}\frac{\eta_k^{-1}}{S_K}
        = (\tau\gamma_0)^{-1}\nu_k
        \leq (\tau\gamma_0)^{-1} \frac{1}{K},
    \]
    for all $k\in\N$.
    Letting $\tilde{R}\in(0,\infty)$ be large enough such that
    $\sup_{k\in\N} \dist(x_k, \mathcal{X}) \leq \tilde{R}$ a.\,s., we thus have
    \[
        d_k \leq \frac{M \tilde{R} \log^e(K)}{\tau\gamma_0 K},
    \]
    for all $k\in\N$. Applying the Azuma-Hoeffding inequality (\cref{prop:azuma-hoeffding}),
    we obtain
    \[
        \Prob\Big( \sum_{k=1}^K \tilde{\nu}_k \langle z_k, x_k - \proj(x_k) \rangle \geq t \Big)
        \leq
        \exp\Big( \frac{-t^2}{2\sum_{k=1}^K d_k^2} \Big)
        \leq
        \exp\Big( \frac{-t^2 \tau^2 \gamma_0^2 K}{2M^2\tilde{R}^2\log^{2e}(K)} \Big),
    \]
    for all $t\in(0, \infty)$ and $K\in\N$. Substituting
    $t := \delta M\tilde{R} \log^{e}(K) / \tau\gamma_0\sqrt{K}$ for arbitrary $\delta\in(0, \infty)$,
    we obtain
    \[
        \Prob\Big( \sum_{k=1}^K \tilde{\nu}_k \langle z_k, x_k - \proj(x_k) \rangle
                \geq \delta (\tau\gamma_0)^{-1} \frac{M\tilde{R} \log^{e}(K)}{\sqrt{K}} \Big)
        \leq
        \exp\Big( -\frac{1}{2}\delta^2 \Big).
    \]
    for all $K\in\N$. We therefore have, for any $\delta\in(0,\infty)$ and $K\in\N$,
    \begin{equation}
        \label{eq:proof:thm:convergence-with-high-probability}
        \dist(\bar{x}_K, \mathcal{X})
        \leq \tau^{-1} \, \mathcal{O}\Big( \frac{\log^e(K)}{K} \Big) + 2\delta (\tau\gamma_0)^{-1} \frac{M\tilde{R} \log^{e}(K)}{\sqrt{K}}
    \end{equation}
    with probability at least $1 - \exp(-\delta^2/2)$.
    Note that for any valid choice of $R$, we can choose $\tilde{R}$ such that 
    $\tilde{R} \leq R$, which brings us to the desired bound.
    We can derive a lower bound on $f(\bar{x}_K) - f(x^\star)$ via
    \begin{align*}
        f(\bar{x}_K) - f(x^\star) &\geq \langle \tilde{\nabla} f(x^\star), \bar{x}_K - x^\star \rangle \\
                            &= \langle \tilde{\nabla} f(x^\star), \bar{x}_K - \proj(\bar{x}_K) \rangle
                                + \langle \tilde{\nabla} f(x^\star), \proj(\bar{x}_K) - x^\star \rangle \\
                            &\geq \langle \tilde{\nabla} f(x^\star), \bar{x}_K - \proj(\bar{x}_K) \rangle,
    \end{align*}
    where the first step follows from convexity, and the second from optimality of $x^\star$ on $\mathcal{X}$.
    Cauchy-Schwarz now implies
    \[
        f(\bar{x}_K) - f(x^\star) \geq - \norm{\tilde{\nabla} f(x^\star)} \dist(\bar{x}_K, \mathcal{X}).
    \]
    Combining with the high-probability bound \eqref{eq:proof:thm:convergence-with-high-probability}, we have,
    for any $\delta\in(0, \infty)$,
    \[
        f(\bar{x}_K) - f(x^\star)
        \geq - \tau^{-1} \norm{\tilde{\nabla} f(x^\star)}
            \Big( \mathcal{O}\Big( \frac{\log^e(K)}{K} \Big) + 2 \delta \gamma_0^{-1} \frac{M\tilde{R} \log^{e}(K)}{\sqrt{K}} \Big)
    \]
    with probability at least $1 - \exp(-\delta^2/2)$. Note that we can choose $\tilde{R} \leq R$
    without loss of generality. Hence, by combining with the upper bound \eqref{eq:proof:thm:convergence-with-high-probability:upper-bound-on-f},
    we have for any $\delta\in(0,\infty)$ and all $K\in\N$,
    \[
        |f(\bar{x}_K) - f(x^\star)|
        \leq
        \tau^{-1} \mathcal{O}\Big( \frac{\log^{2e}(K)}{K} \Big)
            + \delta \max\big( 2\norm{\tilde{\nabla} f(x^\star)} (\tau\gamma_0)^{-1}, 1 \big) \frac{M R \log^{e}(K)}{\sqrt{K}}
    \]
    with probability at least $1 - \exp(-\delta^2/2)$, as desired.
\end{proof}

\section{Infeasible problems}

As we have seen in the SVM example (\textcolor{red}{TODO}), some problems
of interest may not be feasible. Yet, our methods can still be applied in those cases.
The question is then: What do the iterates converge to, if anything?
\begin{definition}
    Let $\delta \in [0,1]$. A point $x\in\R^n$ is called \textbf{$\delta$-feasible}, if
    \[
        \Prob(A(\xi)x - b(\xi) > 0) \leq \delta. 
    \]
    The \textbf{$\delta$-set}, denoted $\mathcal{X}_\delta$, is the set of all $\delta$-feasible points.
    A point $x\in\R^n$ is called \textbf{maximally feasible}, if
    there exists $\delta\in[0,1]$ such that $(x, \delta)$ solves
    \[
        \min_{(x, \delta)\in\R^n\times[0,1]} \delta \quad \text{s.\,t.} \enspace x\in \mathcal{X}_\delta.
    \]
\end{definition}
\noindent
\textbf{Conjecture:} Consider the following two statements:
\begin{enumerate}
    \item For any $\delta\in(0,1]$, the sequence of iterates $(x_k)_{k\in\N}$
    is eventually contained in $\mathcal{X}_\delta$ in probability.
    \item The sequence of iterates $(x_k)_{k\in\N}$ converges to a maximally feasible point in probability.
\end{enumerate}
At least one of these two statements must hold, regardless of whether \eqref{eq:main-problem-copy}
is feasible or not. Both statements hold iff. problem \eqref{eq:main-problem-copy} is feasible.