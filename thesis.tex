\documentclass[10pt,a4paper,oneside]{scrreprt}

% Basic setup
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{csquotes}
\onehalfspacing
\usepackage[english]{babel}

% --- Bib ---
\usepackage{biblatex}
\addbibresource{references.bib}

% Math
\usepackage{amsmath,amssymb,mathtools}
\usepackage{amsthm}            % define theorem counters (reliable \label)

% tcolorbox for styling the theorem environments
\usepackage[most]{tcolorbox}
\tcbuselibrary{skins,breakable}

% aliascnt to make cleveref distinguish environments that share a counter
\usepackage{aliascnt}

% Hyperref & cleveref (hyperref before cleveref)
\usepackage[
    colorlinks=true,
    linkcolor=blue,
    citecolor=red,
    urlcolor=teal
]{hyperref}
\usepackage[nameinlink]{cleveref}

% --------------------------
% Theorem counters (amsthm + aliascnt)
% --------------------------
% main theorem counter
\newtheorem{theorem}{Theorem}[chapter]

% create alias counters so cleveref knows the environment type
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}

\newaliascnt{proposition}{theorem}
\newtheorem{proposition}[proposition]{Proposition}
\aliascntresetthe{proposition}

\newaliascnt{corollary}{theorem}
\newtheorem{corollary}[corollary]{Corollary}
\aliascntresetthe{corollary}

\theoremstyle{definition}
\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}

\newcounter{algoCounter}
\theoremstyle{definition}
\newaliascnt{algorithm}{algoCounter}
\newtheorem{algorithm}[algorithm]{Algorithm}
\aliascntresetthe{algorithm}

\theoremstyle{definition}
\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}

\theoremstyle{definition}
\newaliascnt{notation}{theorem}
\newtheorem{notation}[notation]{Notation}
\aliascntresetthe{notation}

\newcounter{assumptionCounter}
\theoremstyle{definition}
\newaliascnt{assumption}{assumptionCounter}
\newtheorem{assumption}[assumption]{Assumption}
\aliascntresetthe{assumption}

\theoremstyle{remark}
\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Remark}
\aliascntresetthe{remark}

% --------------------------
% Box styling for these environments (grey-ish boxes)
% --------------------------
% A simple grey-ish boxed style that wraps the entire amsthm environment.
% \tcolorboxenvironment{theorem}{
%   enhanced,
%   breakable,
%   boxrule=0.6pt,
%   arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8,        % light grey background
%   colframe=gray!60,      % medium grey frame
%   colbacktitle=gray!12,
%   coltitle=black,
%   fonttitle=\bfseries,
%   before skip=10pt,
%   after skip=10pt,
% }
% \tcolorboxenvironment{lemma}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }
% \tcolorboxenvironment{proposition}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }
% \tcolorboxenvironment{corollary}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }
% \tcolorboxenvironment{definition}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }
% \tcolorboxenvironment{algorithm}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }
% \tcolorboxenvironment{example}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }
% \tcolorboxenvironment{remark}{
%   enhanced, breakable, boxrule=0.6pt, arc=2pt,
%   left=6pt, right=6pt, top=6pt, bottom=6pt,
%   colback=gray!8, colframe=gray!60,
%   before skip=10pt, after skip=10pt
% }

% --------------------------
% cleveref names (explicit)
% --------------------------
\crefname{theorem}{theorem}{theorems}
\Crefname{theorem}{Theorem}{Theorems}
\crefname{lemma}{lemma}{lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{proposition}{proposition}{propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{corollary}{corollary}{corollaries}
\Crefname{corollary}{Corollary}{Corollaries}
\crefname{example}{example}{examples}
\Crefname{example}{Example}{Examples}
\crefname{definition}{definition}{definitions}
\Crefname{definition}{Definition}{Definitions}
\crefname{algorithm}{algorithm}{algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{notation}{notation}{notations}
\Crefname{notation}{Notation}{Notations}
\crefname{assumption}{assumption}{assumptions}
\Crefname{assumption}{Assumption}{Assumption}
\crefname{remark}{remark}{remarks}
\Crefname{remark}{Remark}{Remarks}

% --------------------------
% boxedproofof environments
% --------------------------
% Plain proof-of (uses amsthm's proof)
\newenvironment{proofof}[1]{%
  \begin{proof}[Proof of \Cref{#1}]%
}{%
  \end{proof}%
}

\newenvironment{proofbf}[1]{%
  \begin{proof}[\textbf{Proof}]%
}{%
  \end{proof}%
}

% Boxed proof-of (tcolorbox)
\newenvironment{boxedproofof}[1]{%
  \begin{tcolorbox}[enhanced, breakable,
      boxrule=0.6pt, arc=2pt,
      left=6pt,right=6pt,top=6pt,bottom=6pt,
      colback=gray!8, colframe=gray!60,
      fonttitle=\bfseries,
      title={Proof of \cref{#1}}, before skip=10pt, after skip=10pt]%
}{%
  \end{tcolorbox}%
}

% ---------------------------------------------------------------------------
% End of preamble - continue with \begin{document} in your document body


% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Var}{\mathbb{V}\textnormal{ar}}
\newcommand{\dist}{\textnormal{dist}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\dom}{dom}

% ---------------- Document ----------------
\title{Stochastic Optimization Under Almost-Sure Affine Inequality Constraints}
\author{Amir Miri Lavasani}
\date{\today}

\begin{document}
\maketitle
\tableofcontents


\chapter{Introduction}

Stochastic optimization is concerned with problems of the form
\[
  \min_{x\in \mathcal{X}} \Big\{ f(x) := \E(F(x, \xi)) \Big\}.
\]
Here, $\emptyset \neq \mathcal{X}\subset\R^d$, $F\colon\R^d\times\R^m\to\R$, $\xi\colon\Omega\to\R^m$
is a random vector on some probability space $(\Omega, \mathcal{F}, \Prob)$,
and $\omega \mapsto F(x,\xi(\omega))$ is assumed to be measurable for all $x\in\R^d$, so that $f$
is well-defined. Typically, the set $\mathcal{X}$ is called the \textbf{feasible set} and $f$ is called the \textbf{objective function},
or simply \textbf{objective}.


We consider a constrained stochastic optimization problem of the form
\begin{equation*}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^d} \,
        \left\{
            j(x) := \E\left( \frac{1}{2} |\!| y-b |\!|^2 + \frac{\lambda}{2} |\!| x |\!|^2 \right)
        \right\} \\
    &\textup{subject to (s.\,t.)} \quad B(\xi)y = C(\xi)x \quad \text{almost surely (a.\,s.)} \\
    &\phantom{\textup{subject to (s.\,t.)}} \quad y \leq c\,,
    \end{aligned}
\end{equation*}
where $y, b, c \in\R^d$, $\lambda \in (0,\infty)$, $\xi: \Omega\to \R^m$ is a random vector on some fixed
probability space $(\Omega, \mathcal{F}, \Prob)$, and $B(z), \,C(z) \in \R^{n\times d}$ for all $z\in\R^m$.
As a norm, we
consider the standard Euclidian norm on $\R^d$.
Under the assumption that $B(z)$ is invertible for all $z\in\R^m$, we have $y = B(\xi)^{-1}C(\xi)x$
almost surely and the problem can be rewritten as
\begin{equation}
\label{eq:model-problem}
\tag{P}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^d} \,
    \left\{
        j(x) = \E\left( \frac{1}{2} |\!| A(\xi)x - b |\!|^2 + \frac{\lambda}{2} |\!| x |\!|^2 \right)
    \right\} \\
    &\textup{s.\,t.} \quad A(\xi)x \leq c \quad \text{a.\,s.}\,,
    \end{aligned}
\end{equation}
where $A(z) := B(z)^{-1}C(z) \in \R^{d\times d}$ for all $z\in\R^m$ and we assume
that $\E|\!|A(\xi)x|\!|^2$ is finite for all $x\in\R^d$.

A key difficulty in solving
the problem is the almost sure constraint. For one, it is not directly clear whether there even exists
a feasible point. For this, one would atleast need $A(\xi)$ to have bounded support.
Additionally, even if its nonempty, the feasibile set is in general still very difficult to compute explicitly,
due to its probabilistic nature. However, even simple situations can be problematic:
Suppose for instance that $\xi \in \{1, \dots, M\}$ for some $M\in\N$. Then, problem \eqref{eq:model-problem}
is equivalent to
\begin{equation}
\label{eq:model-problem-finite-randomness}
\tag{$\text{P}^{M}$}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^d} \,
    \left\{
        j(x) = \E\left( \frac{1}{2} |\!| A(\xi)x - b |\!|^2 + \frac{\lambda}{2} |\!| x |\!|^2 \right)
    \right\} \\
    &\textup{s.\,t.} \quad A(i)x \leq c \quad \forall i \in \{ 1,\dots, M \}.
    \end{aligned}
\end{equation}
% This is a stochastic convex optimzation problem with
% $n\times M$ determinist constraints, which can be solved with
% projected stochastic gradient methods \cite{doi:10.1137/070704277},
% where the projection is onto the polyhedral set
% $\{ x\in\R^d, \, A(i)x \leq c \textnormal{ for all } i \in \{ 1, \dots, M \} \}$.
The projection
itself is the solution to a quadratic program, which has a generic cost of $\mathcal{O}(n^3 M^3)$ operations if
the matrices $A(i)$ have no special structure (sparsity, for example) that can be exploited \cite{Boyd_Vandenberghe_2004}.
This projection can become expensive to compute if $n\times M$ is large.

In this work, we will consider a
different approach by introducing a family of unconstrained optimization problems
\begin{equation}
\label{eq:penalized-model-problem}
\tag{$\text{P}^k$}
    \begin{aligned}
    &\min_{x \in \mathbb{R}^d} \,
    \left\{ 
        j^k(x) := \E\left( \frac{1}{2} |\!| y-b |\!|^2 + \frac{\lambda}{2} |\!| x |\!|^2 \right) + \frac{\gamma_k}{2} \pi(x)
    \right\},
    \end{aligned}
\end{equation}
where $\gamma_k \in (0,\infty)$ for all $k\in\N$ and $\pi: \R^d\to\R$ has the properties $\pi(x) \geq 0$ and $\pi(x) = 0$
if and only if $x$ is feasible for \eqref{eq:model-problem}. If $\pi$ is also convex and
\eqref{eq:model-problem} has at least one feasible point, then
there always exists a solution $x^\star_k\in\R^d$ to problem \eqref{eq:penalized-model-problem}
for all $k\in\N$. There are three immediate questions:
\begin{enumerate}
    \item If $x^\star$ denotes the solution to
    \eqref{eq:model-problem}, when can we guarantee that
    $x^\gamma\to x^\star$?
    \item How do we choose $\pi$?
    \item How can we use this to numerically solve \eqref{eq:model-problem}?
\end{enumerate}
\noindent
\textcolor{red}{Outline}.\\
\textcolor{red}{Related literature}.\\
\textcolor{red}{Contributions}. Single-loop penalty methods, Batch sizes, relaxed gradient bound, general
treatment of penalty function, analysis of averaging, analysis of iterate moving averages.

\chapter{Theory Background}


In this chapter, we state some classic definitions and results that we will make use of in the later sections.
Proofs are omitted, but can be found in the cited sources.

\section{Convex Optimization}

The contents of this section can be found in \cite{Boyd_Vandenberghe_2004,garrigos2023handbook}.
Throughout, we let $|\!|\cdot|\!|$ denote the standard Euclidian norm and
$\langle \cdot, \cdot \rangle$ the standard inner product on $\R^d$.

\begin{definition}
    Let $f\colon \R^d \to \R$ be differentiable, and $L > 0$. We say that $f$ is
    \textbf{Lipschitz-smooth with constant $L$}, or simply \textbf{$L$-smooth},
    if its gradient is Lipschitz continuous, i.\,e. there exists a constant $L\in (0,\infty)$ such that
    \[
        |\!|\nabla f(y) - \nabla f(x)|\!| \leq L|\!|y - x|\!|
    \]
    for all $x,y\in\R^d$\,.
\end{definition}

\begin{proposition}
    \label{prop:lipschitz-gradients}
    Let $f\colon \R^d \to \R$ be $L$-smooth. Then
    \[
        f(y) \leq f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2} |\!|y-x|\!|^2
    \]
    for all $x, y\in\R^d$\,.
\end{proposition}

\begin{proposition}
    \label{prop:linear-combination-of-lipschitz-is-lipschitz}
    Let $f\colon \R^d \to \R$ be $L_f$-smooth and let  $g\colon \R^d \to \R$
    be $L_g$-smooth and define $h(x) := a\,f(x) + b\,g(x)$ for some positive
    constants $a,b\in (0, \infty)$. Then $h$ is Lipschitz-smooth with
    constant $a L_f + b L_g$.
\end{proposition}

\begin{definition}
    A set $C\subset\R^d$ is called \textbf{convex} if, for all 
    $x,y\in C$ and $t\in [0, 1]$, it holds that $tx + (1-t)y \in C$.
    We say that a function $f\colon \R^d\to\R$ is \textbf{convex} if
    \begin{align*}
        f((1-t) x + t y) \leq (1-t) f(x) + t f(y)
    \end{align*}
    for all $x, y\in\R^d$ and all $t\in (0, 1)$. We say that $f$ is \textbf{concave} if
    $-f$ is convex.
\end{definition}

\begin{proposition}
    \label{prop:convex-implies-continuous}
    Every convex function on $\R^d$ is continuous.
\end{proposition}

\begin{definition}
    Let $f\colon \R^d\to\R$ be convex. A vector $g\in\R^d$ is a \textbf{subgradient} of $f$ at $x\in\R^d$ if
    \[
        f(y) \geq f(x) + \langle g, y-x \rangle
    \]
    for all $y\in\R^d$.
    The set of all subgradients of $f$ at $x$ is denoted by $\partial f(x)$
    and we call this set the \textbf{subdifferential of} $f$ \textbf{at} $x$.
    If $\partial f(x) \neq \emptyset$, then we call $f$ \textbf{subdifferentiable at} $x$.
    If $\partial f(x) \neq \emptyset$ for all $x\in\R^d$, we call $f$ \textbf{subdifferentiable}.
\end{definition}

\begin{proposition}
    Let $f\colon \R^d\to\R$ be convex and differentiable. Then $f$ is subdifferentiable with
    $\partial f(x) = \{ \nabla f(x) \}$ for all $x\in\R^d$. In particular,
    \[
        f(y) \geq f(x) + \langle \nabla f(x), y-x \rangle
    \]
    for all $x,y\in\R^d$.
\end{proposition}

% \begin{notation}
%     If $f$ is subdifferentiable, but not necessarily differentiable, we will denote any subgradient of $f$
%     at $x\in\R^d$ simply by $\tilde{\nabla}f(x)$. Exceptions may occur, if we need to refer to two different
%     subgradients at the same point.
% \end{notation}

\begin{proposition}
    \label{prop:convex-implies-monotone-gradient}
    Let $f\colon \R^d\to\R$ be subdifferentiable. Then
    \[
        \langle g_y - g_x, y-x \rangle \geq 0
    \]
    for all $x,y\in\R^d$ and $g_x\in\partial f(x)$, $g_y\in\partial f(y)$.
\end{proposition}

\begin{definition}
    Let $f\colon \R^d \to \R$ and $\mu\in (0,\infty)$\,. We say that $f$ is ($\mu$-)\textbf{strongly convex} if
    \begin{align*}
        f((1-t)x + ty) \leq (1-t)f(x) + tf(y) - \mu t(1-t) |\!|x-y|\!|^2    
    \end{align*}
    for all $x,y\in\R^d$ and $t\in (0,1)$.
\end{definition}
\noindent
Clearly, strongly convex functions are convex.
\begin{proposition}
    \label{prop:operations-conserving-convexity}
    Let $f\colon \R^d\to\R$ and $g\colon \R^d\to\R$ be convex and $\alpha > 0$.
    Also, let $A\in\R^{d\times m}$ and $b\in\R^d$. Then, the functions
    \begin{enumerate}
        \item $x\mapsto f(x) + g(x)$,
        \item $x\mapsto \alpha f(x)$,
        \item $x\mapsto f(Ax + b)$,
    \end{enumerate}
    are all convex.
    If $f$ is $\mu$-strongly convex, then the above functions are also all $\mu$-strongly convex.
\end{proposition}

\begin{proposition}
    \label{prop:convex-circ-increasing-is-convex}
    Let $f\colon \R\to\R$ and $g\colon \R\to\R$ be convex and nondecreasing. Then the composition
    $f \circ g$ is also convex.
\end{proposition}

\begin{proposition}
    \label{prop:strongly-convex-subdifferentiable-bound}
    Let $f\colon \R^n \to \R$ be $\mu$-strongly convex and subdifferentiable. Then
    \[
        f(y) \geq f(x) + \langle g, y-x \rangle + \frac{\mu}{2}|\!|y-x|\!|^2
    \]
    for all $x,y\in\R^d$ and $g\in\partial f(x)$. This implies that, for all $g_x\in\partial f(x)$,
    $g_y\in\partial f(y)$, we have
    \[
        \langle g_y - g_x, x - y \rangle \geq \frac{\mu}{2} |\!| x - y |\!|^2,
    \]
    for all $x,y\in\R^d$. In particular, if $f$ is differentiable and $x^\star = \argmin_{x\in\R^d} f(x)$,
    we have
    \[
        f(x^\star) \leq f(x) - \frac{\mu}{2} |\!| x - x^\star |\!|^2
    \]
    for all $x\in\R^d$.
\end{proposition}

\begin{proposition}
    \label{prop:convex+quadratic=strongly-convex}
    Let $g\colon\R^d\to\R$ be convex and let $\mu\in(0, \infty)$. Then, the function $x\mapsto g(x) + \frac{\mu}{2} |\!|x|\!|^2$
    is $\mu$-strongly convex.
\end{proposition}

\begin{proposition}
    \label{prop:strongly-convex-implies-unique-minimizer}
    If $f\colon \R^d\to\R$ is strongly convex and $C\subset\R^d$ is convex, then $f$ admits a unique minimizer
    on $C$, i.\,e. there exists a point $x^\star\in C$ such that $f(x^\star) < f(x)$ for all $x\in C$.
\end{proposition}

\begin{proposition}
    \label{prop:pos-def-second-derivative-implies-convex}
    Let $f\colon\R^d\to\R$ be twice differentiable. If $f$ has positive definite hessian, then
    $f$ is convex. If, additionally, there exists some $\mu \in (0, \infty)$ such that
    $f^{\prime\prime}(x) - \mu \,I_d$ is positive definite for all $x\in\R^d$,
    where $I_d$ denotes the $d\times d$ identity matrix, then
    $f$ is $\mu$-strongly convex.
\end{proposition}

\begin{example}
    \label{ex:convex-functions}
    Examples of (strongly) convex functions include
    \begin{itemize}
        \item affine function;
        \item quadratic functions $f(x) := x^\top Ax + b^\top x + c$ for $x,b\in\R^d$, $c\in\R$, and $A\in\R^{d\times d}$ positive definite.
        If $A - \mu I_d$ is positive definite for some $\mu\in (0, \infty)$, then $f$ is $\mu$-strongly convex.
        \item $x \mapsto \exp(x)$, $x\mapsto -\log(x)$, $x\mapsto \max(0, x)$.
    \end{itemize}
\end{example}


\section{Probability Theory}

The contents of this section can be found in standard probability texts, for example
\cite{durrett2019probability,blitzstein2019introduction}.

\begin{definition}
    Let $\Omega$ be a set and let $2^\Omega$ denote its power set. A subset $\mathcal{F} \subset 2^\Omega$ is called a $\sigma$-\textbf{algebra
    over} $\Omega$ if it satisfies the following three conditions:
    \begin{enumerate}
        \item $\emptyset \in \mathcal{F}$.
        \item If $A, B \in \mathcal{F}$, then $B \backslash A \in \mathcal{F}$.
        \item For any countable sequence $A_1, A_2, \dots \in \mathcal{F}$, we have $\bigcup_{n=1}^\infty A_n \in \mathcal{F}$.
    \end{enumerate}
    If $\mathcal{F}$ is a $\sigma$-algebra over $\Omega$, then the tuple $(\Omega, \mathcal{F})$
    is called a \textbf{measurable space}. For any subset $\mathcal{G} \subset 2^\Omega$, we define
    the $\sigma$-\textbf{algebra generated by} $\mathcal{G}$ as the intersection over all $\sigma$-algebras
    that contain $\mathcal{G}$ as an element, and we denote this $\sigma$-algebra by $\sigma(\mathcal{G})$.
\end{definition}

\begin{example}
    \label{ex:borel-sigma-algebra}
    An important example of a $\sigma$-algebra over $\R^d$ is the \textbf{Borel} $\sigma$-\textbf{algebra}
    $\mathcal{B}(\R^d)$, which is defined to be the $\sigma$-algebra generated by the subset of all
    open sets on $\R^d$. Functions that are $\mathcal{B}(\R^d)$-measurable are called \textbf{Borel measurable}.
\end{example}

\begin{definition}
    Let $(\Omega, \mathcal{F})$ and $(E, \mathcal{G})$ be measurable spaces.
    A map $f\colon \Omega \to E$ is called $\mathcal{F},\mathcal{G}$-\textbf{measurable}
    if $f^{-1}(G) := \{ \omega\in\Omega \, | \, f(\omega) \in G \} \in \mathcal{F}$
    for all $G\in\mathcal{G}$.  We may say $f$ is $\mathcal{F}$-\textbf{measurable}
    or simply \textbf{measurable} if one or both of the $\sigma$-algebras are either
    clear from the context or not relevant.
\end{definition}

\begin{definition}
    Let $(\Omega, \mathcal{F})$ be a measurable space. A map $\mu : \mathcal{F} \to [0, \infty]$ is called a \textbf{measure on} $(\Omega, \mathcal{F})$ if it satisfies the following two conditions:
    \begin{enumerate}
        \item $\mu(\emptyset) = 0$.
        \item For any countable sequence $A_1, A_2, \dots \in \mathcal{F}$, we have $\mu(\bigcup_{n=1}^{\infty} A_n) = \sum_{n=1}^{\infty} \mu(A_n)$.
    \end{enumerate}
    If $\mu$ is a measure on $(\Omega, \mathcal{F})$, then the triplet $(\Omega, \mathcal{F}, \mu)$ is called a \textbf{measure space}.
\end{definition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \mu)$ be a measure space and let $f\colon \Omega \to \R$ be measurable. We define the ($\mu$-)\textbf{integral} of $f$, denoted $\int f \, \textnormal{d}\mu$, in three steps:
    \begin{enumerate}
        \item If $f(\omega) = \sum_{i=1}^n c_i 1_{A_i}(\omega)$ for some $n\in\N$, $c_1, \dots, c_n > 0$, and disjoint measurable sets $A_1, \dots, A_n\in\mathcal{F}$, then we define
        \begin{align*}
            \int f \, \textnormal{d}\mu := \sum_{i=1}^n c_i \mu(A_i).
        \end{align*}

        In this case $f$ is called a \textbf{simple function}. The set of all simple functions on $\Omega$ is denoted by $\mathcal{S}(\Omega)$.

        \item If $f$ is nonnegative, i.\,e. $f(\omega) \geq 0$ of all $\omega\in\Omega$, then we define
        \begin{align*}
            \int f \, \textnormal{d}\mu := \sup_{g \in \mathcal{S}(\Omega), \, g \leq f} \int g \, \textnormal{d}\mu.
        \end{align*}

        \item If $f$ is neither a simple function, nor nonnegative, but $\int |f| \, \textnormal{d}\mu < \infty$, then we define
        \begin{align*}
            \int f \, \textnormal{d}\mu := \int \max(0, f) \, \textnormal{d}\mu - \int \max(0, -f) \, \textnormal{d}\mu.
        \end{align*}
    \end{enumerate}
    Otherwise, we say that the ($\mu$-)integral of $f$ does not exist. If any of these three conditions apply to $f$, we say that $f$ is ($\mu$-)\textbf{integrable}.
\end{definition}

\begin{proposition}
    \label{prop:integrals}
    Let $(\Omega, \mathcal{F}, \mu)$ be a measure space and let $f\colon \Omega \to \R$ and
    $g\colon \Omega \to \R$ be integrable. Then
    \begin{enumerate}
        \item[(i)] $\int f + g \, \textnormal{d}\mu = \int f \, \textnormal{d}\mu + \int g \, \textnormal{d}\mu$.
        \item[(ii)] $\int c f \, \textnormal{d}\mu = c \int f \, \textnormal{d}\mu$ for all $c\in\R$.
        \item[(iii)] If $f \leq g$, then $\int f \, \textnormal{d}\mu \leq \int g \, \textnormal{d}\mu$. If additionally $f < g$ on some set $A\in\mathcal{F}$ with $\mu(A) > 0$, then $\int f \, \textnormal{d}\mu < \int g \, \textnormal{d}\mu$.
    \end{enumerate}
\end{proposition}

\begin{definition}
    Let $(\Omega, \mathcal{F})$ be a measurable space. If $\Prob: \mathcal{F} \to [0,1]$ is a measure on $(\Omega, \mathcal{F})$, we call $\Prob$ a \textbf{probability measure} and we call the triple $(\Omega, \mathcal{F}, \Prob)$ a \textbf{probability space}. In this context, elements of $\mathcal{F}$ are called \textbf{events}.
\end{definition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space. An event $A\in\mathcal{F}$ is said to hold \textbf{almost surely} (a.\,s. for short) if $\Prob(A) = 1$.
\end{definition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and $(E, \mathcal{G})$ a measurable space. A map $X\colon \Omega \to E$ is called
    a \textbf{random variable} on $(\Omega, \mathcal{F}, \Prob)$ if $X$ is $\mathcal{F},\mathcal{G}$-measurable.
    In the case $E = \R^d$, we may call $X$ a \textbf{random vector}.
    Further, we define the notation $\Prob(X \in G) := \Prob(X^{-1}(G))$.
    We define the \textbf{distribution of} $X$ to be the probability measure
    $\Prob^X := \Prob \circ X^{-1}$ on $(E, \mathcal{G})$. Finally, we define
    $\sigma(X) := \sigma(\{ X^{-1}(G),\, G\in \mathcal{G} \})$
    and call this the \textbf{$\sigma$-algebra
    generated by $X$}.
\end{definition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space. Two random variables $X$ and $Y$
    on $(\Omega, \mathcal{F}, \Prob)$ are called \textbf{independent} if $\Prob(X\in A,\, Y\in B)
    = \Prob(X\in A)\cdot \Prob(Y\in B)$ for all $A,B\in\mathcal{F}$. $X$ and $Y$ are called
    \textbf{identically distributed} if $\Prob^X = \Prob^Y$.
    We use the abbreviation \textbf{i.\,i.\,d.} as shorthand for \enquote{independent and identically distributed}.
\end{definition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $X$ be a random variable on this space. If $X$ is integrable, we define the \textbf{expected value of} $X$, denoted by $\E(X)$, as $\E(X) := \int X \, \textnormal{d}\Prob$.
\end{definition}
\noindent
The following three properties will be used multiple times throughout this text without explicit mention. They
follow directly from \cref{prop:integrals}.

\begin{proposition}
    \label{prop:properties-of-expected-value}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a
    probability space and let $X\colon \Omega \to \R$ and
    $Y\colon \Omega \to \R$ be integrable random variables. Then
    \begin{enumerate}
        \item[(i)] $\E(X + Y) = \E(X) + \E(Y)$.
        \item[(ii)] $\E(c X) = c\, \E(X)$ for all $c\in\R$.
        \item[(iii)] If $X \leq Y$, then $\E(X) \leq \E(Y)$. If additionally
        $X(\omega) < Y(\omega)$ for all $\omega$ in an event $A\in\mathcal{F}$ with $\Prob(A) > 0$,
        then $\E(X) < \E(Y)$.
    \end{enumerate}
\end{proposition}

\begin{proposition}
    \label{prop:expectation-preserves-convexity}
    Let $f\colon \R^d\times\Omega \to\R$ be convex in its first argument, i.\,e. $x~\mapsto~f(x, \omega)$
    is convex for all $\omega\in\Omega$. Then, the function $x\mapsto \E(f(x, \cdot))$ is convex. 
\end{proposition}

\begin{proposition}[Jensen's inequality]
    \label{prop:jensens-inequality} 
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $X\colon \Omega\to\R$ be a random variable. Then we have
    \[
        \E(X^2) \geq \E(X)^{2}.
    \]
    In particular: If $X^2$ is integrable, then $X$ must also be integrable.
\end{proposition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $X\colon \Omega \to \R$ be a random variable.
    If $X^2$ is integrable, we define the \textbf{variance of} $X$, denoted by $\Var(X)$, as $\Var(X) := \E|\!|X-\E(X)|\!|^2$.
\end{definition}
\noindent
One fact from probability theory is that, for any integrable random variable 
$X: \Omega\to\ E$, it holds that $\int X \,\textnormal{d}\Prob = \int I \,\textnormal{d}\Prob^X$, where $I\colon E\to E$ is the identity
operator. It is now easy to see that if two random variables $X$ and $Y$ are identically distributed,
they have the same expected value and variance.

\begin{proposition}
    \label{prop:variance-linear-for-independent-rvs}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $X_1,\dots,X_n\colon \Omega \to \R$
    be independent random variables, such that $X_i^2$ is integrable
    for all $i\in\{1,\dots, n\}$. Then $\Var(\sum_{i=1}^n a_i X_i) = \sum_{i=1}^{n} a_i^2\, \Var(X_i)$,
    for any $a_1,\dots, a_n\in\R$.
    If, additionally, they are all identically distributed, it holds that
    $\Var(1/n\sum_{i=1}^n X_i) = \Var(X_1)/n$.
\end{proposition}

\begin{proposition}
    \label{prop:variance-identity}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $X\colon \Omega \to \R$ be a random variable
    such that $X^2$ is integrable. Then $\Var(X) = \E(X^2) - \E(X)^2$.
\end{proposition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $X\colon \Omega\to\R^n$ be a random variable.
    Further, let $\mathcal{C}\subset\mathcal{F}$ be a $\sigma$-algebra.
    We call a random variable $Z\colon \Omega\to\R^n$ a \textbf{conditional expectation of} $X$ \textbf{given} $\mathcal{C}$, if
    \begin{enumerate}
        % \item There exists some measurable $h\colon \R^m\to\R^n$ such that $Z = h(Y)$.
        \item $Z$ is $\mathcal{C}$-measurable, and
        \item for all $C\in \mathcal{C}$, it holds that
        \begin{equation*}
            \int_{C} Z \, \textnormal{d}\Prob = \int_{C} X \, \textnormal{d}\Prob.
        \end{equation*}
    \end{enumerate}
    If $Z$ is a conditional expectation of $X$ given $\mathcal{C}$ then we use the notation $\E(X \,|\, \mathcal{C}) := Z$
    
    If $Y\colon\Omega\to \R^m$ is a random variable, such that $\mathcal{C} = \sigma(Y)$, then
    we use the notation $\E(X \,|\, Y) := \E(X \,|\, \sigma(Y))$.
    In that case, we call $\E(X \,|\, Y)$ the \textbf{conditional expectation of $X$ given $Y$}.
    Further, for $\omega\in\Omega$ with $Y(\omega) = y\in\R^m$, the \textbf{conditional expectation of $X$ given $Y=y$}, denoted
    by $\E(X \,|\, Y = y)$, is defined as $\E(X \,|\, Y = y) := \E(X \,|\, Y)(\omega)$.
\end{definition}
Note that $E(X \,|\, Y)$ is not unique. However, if $Z_1$ and $Z_2$ are both conditional expectations
of $X$ given $Y$, then we always have $Z_1 = Z_2$ almost surely. For simplicity, we will keep the
\enquote{almost surely} implicit.
\begin{remark}
    \label{remark:conditional-expectation}
    If $X$ and $Y$ are integrable random variables on a probability space $(\Omega, \mathcal{F}, \Prob)$, then
    the conditional expectation of $X$ given $Y=y$ can be thought of as the expected value of
    $X$ on a different probabiliy space $(\Omega, \mathcal{F}, \Prob_{Y=y})$, where
    $\Prob_{Y=y}(A) := \Prob(A \,|\, Y=y)$. More precisely,
    $\E(X \,|\, Y = y) = \int X \, \textnormal{d}\Prob_{Y=y}$. It follows that $\E(X \,|\, Y = \cdot)$
    inherits all basic properties of $\E(\cdot)$.
\end{remark}
Below, we state some special properties of conditional expectations.

\begin{proposition}[Properties of $\E(X \,|\, Y)$]
    \label{prop:conditional-expectation-properties}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and
    let $X\colon \Omega \to \R^n$, $Y\colon \Omega \to \R^m$ be integrable random variables. Further, let $F:\R^n\times \Omega\to\R$ such that
    $\omega\mapsto F(x, \omega)$ is $\mathcal{F}$-measurable for all $x\in\R^d$ and let $G:\R^n\to\R$
    be Borel measurable. Then,
    \begin{itemize}
        \item[\textnormal{(i)}] $\E(\E(X \,|\, Y)) = \E(X)$.
        \item[\textnormal{(ii)}] $\E(X \,|\, X) = X$.
        \item[\textnormal{(iii)}] if $F(x, \cdot) \leq G(x)$ a.\,s. for all $x\in\R^d$, it follows that $\E(F(X, \cdot) \,|\, X) \leq G(X)$.
    \end{itemize}
\end{proposition}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and
    let $X\colon \Omega \to \R^n$, $Y\colon \Omega \to \R^m$ be integrable random variables.
    The \textbf{conditional variance of X given Y}, denoted by $\Var(X \,|\, Y)$, is defined as
    \[
        \Var(X \,|\, Y) := \E\Big( ||X - \E(X \, | \, Y)||^2 \,\Big|\, Y \Big).
    \]
    For $y\in\R^d$, the \textbf{conditional variance of X given Y = y} is defined as
    \[
        \Var(X \,|\, Y = y) := \E\Big( ||X - \E(X \, | \, Y = y)||^2 \,\Big|\, Y = y \Big).
    \]
    It holds that $\Var(X \,|\, Y = Y(\omega)) = \Var(X \,|\, Y)(\omega)$ for all $\omega\in\Omega$.
\end{definition}

\begin{remark}
    Let $X$ and $Y$ be integrable random variables on a probability space $(\Omega, \mathcal{F}, \Prob)$.
    Similarly to the conditional expectation of $X$ given $Y = y$, the conditional variance
    $\Var(X \,|\, Y = y)$ can be thought of as the variance of $X$ on a different probability
    space $(\Omega, \mathcal{F}, \Prob_{Y=y})$ (see \cref{remark:conditional-expectation}).
    Hence, all basic properties of $\Var(\cdot)$ are inherited by the conditional variance.
    In particular, if $X_1,\dots, X_n$ are i.\,i.\,d. random variables, it holds that
    $\Var(X_1 + \cdots + X_n \,|\, Y) = 1/n \,\Var(X_1 \,|\, Y)$ -- a fact that will be used later.
\end{remark}

\begin{proposition}
    \label{prop:conditional-variance-properties}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and
    let $X\colon \Omega \to \R^n$ be an integrable random variable.
    Further, let $F:\R^n\times \Omega\to\R$ such that
    $\omega\mapsto F(x, \omega)$ is $\mathcal{F}$-measurable for all $x\in\R^d$ and $\E(F(X,\cdot)^2) < \infty$, and let $G:\R^n\to\R$
    be Borel measurable. Then, if $\Var(F(x, \cdot)) \leq G(x)$ for all $x\in\R^d$, it also holds that
    \[
        \Var(F(X, \cdot) \,|\, X) \leq G(X).
    \]
\end{proposition}


\section{Stochastic Optimization}
\label{sec:background-stochopt}

\begin{definition}[\cite{duchi2018introductory}]
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and let $f\colon \R^d\to\R$ be a convex function.
    A random vector $G\colon\Omega\to\R^d$ is called a \textbf{stochastic subgradient of} $f$
    at a point $x\in\R^d$ if $\E(G)\in\partial f(x)$, or equivalently
    \[
        f(y) \geq f(x) + \langle \E(G), y-x \rangle
    \]
    for all $y\in\R^d$. If, additionally, $f$ is differentiable at $x$, we may simply
    refer to $G$ as a \textbf{stochastic gradient}.
\end{definition}

\begin{example}
    Let $F\colon \R^d\times\Omega\to\R$ be continuously differentiable in its first argument
    and let $f(x) := \E(F(x, \cdot))$ for all $x\in\R^d$.
    Then, for any $x\in\R^d$, the random vector $G_x\colon \Omega\to\R^d$, defined by
    $G_x(\omega) := \nabla_x F(x, \omega)$, is a
    stochastic gradient of $f$ at $x$.
\end{example}

\begin{definition}
    Let $(\Omega, \mathcal{F}, \Prob)$ be a probability space. Further, let $f\colon \R^d\to\R$ and $g\colon \R^d\times \Omega \to\R^m$.
    Consider a stochastic optimization problem $\min_{x\in\R^d} f(x)$ subject to the constraints
    $g(x, \omega) \leq 0$ almost surely. We define the \textbf{active set of $x\in\R^d$ for scenario \textbf{$\omega\in\Omega$}}, as the set
    \[
        \mathcal{A}(x, \omega) := \{i\in\{1, \dots, m\}, \,  g_i(x, \omega) = 0 \},
    \]
    where $g_i(x, \omega)$ is the $i$th component of $g(x,\omega)$, for $i\in\{1,\dots, m\}$.
\end{definition}

Given a probability space $(\Omega, \mathcal{F}, \Prob)$, a \textbf{convex stochastic optimization problem} has the form
\[
        \min_{x\in\mathcal{X}} \Big\{ f(x) := \E(F(x, \xi)) \Big\}\,,
\]
where $\emptyset \neq \mathcal{X}\subset\R^d$, $\xi\colon\Omega\to\R^m$ is a random vector,
and $F\colon\R^d \times \R^m \to \R$ is a function that satisfies \begin{itemize}
    \item $x\to F(x, \xi(\omega))$ is convex for almost every $\omega\in\Omega$;
    \item $\omega\to F(x, \xi(\omega))$ is $\Prob$-measurable for all $x\in\R^d$.
\end{itemize}
With these assumptions, the problem is well-defined and $f$ is convex. We say that a problem of the above
form is \textbf{unconstrained} if $\mathcal{X} = \R^d$. In that case, if additionally $f$ is subdifferentiable,
a standard method to solve such a problem is \textit{stochastic gradient descent}.
\begin{algorithm}[Stochastic Gradient Descent (SGD)]
    \label{algo:sgd}
    Let $x_1\in\R^d$. For $k\in\N$, let $\tau_k\in (0, \infty)$ be a parameter, called \textbf{step size}.
    The \textbf{Stochastic Gradiend Descent (SGD)} iterates $(x_k)_{k\in\N}$ are defined by
    \[
        x_{k+1} := x_k - \tau_k \,G^k(x_k),
    \]
    where $G^k(x_k)$ is a stochastic subgradient of $f$ at $x_k$.
\end{algorithm}
The idea and analysis of this method
go back to Robbins and Monro \cite{robbins1951stochastic}.
The convergence of the iterates $(x_k)_{k\in\N}$,
generated by \cref{algo:sgd}, to a minimum $x^\star \in \argmin_{x\in\R^d} f(x)$ (if it exists)
depends heavily on the choice of step sizes $(\tau_k)_{k\in\N}$ and the behavior of $\E|\!|G^k(x_k)|\!|^2$.
In case of a \textit{strongly convex objective} $f$, if there exists a constant
$M^2 \in (0, \infty)$ such that $\sup_{k\in\N}\E|\!|G^k(x_k)|\!|^2 \leq M^2$, the conditions
\[
    \sum_{k=1}^\infty \tau_k = \infty, \quad \sum_{k=1}^\infty \tau_k^2 < \infty,
\]
ensure that \cref{algo:sgd} converges to the minimum of $f$.
\begin{proposition}
    \label{prop:sgd-iterates-as-minimizers}
    The iterates of \cref{algo:sgd} satisfy
    \[
        x_{k+1} = \argmin_{x\in\R^d} \Big\{ f(x_k) + \langle g^k(x_k), x - x_k \rangle + \frac{1}{2\tau_k} |\!|x-x_k|\!|^2 \Big\}
    \]
    for all $k\in\N$.
\end{proposition}
Note that the function $x\mapsto f(x_k) + \langle G^k(x_k), x - x_k \rangle + 1/2\tau_k |\!|x-x_k|\!|^2$ is
strongly convex, since it is the compositon of the quadratic
$x\mapsto f(x_k) + \langle G^k(x_k), x \rangle + (1/2\tau_k)\, x^\top x$ and the affine function $x\mapsto x - x_k$,
(see \cref{prop:operations-conserving-convexity,ex:convex-functions}),
and thus the above minimization problem is well defined (\cref{prop:strongly-convex-implies-unique-minimizer}).

% In most applications, SGD is used with \textit{momentum} -- a technique that goes back to
% Polyak in the deterministic setting \cite{POLYAK19641}.
% There are several equivalent ways to formulate the momentum method \cite{garrigos2023handbook}. We will stick to
% the \textit{iterate moving average} formulation.
% \begin{algorithm}
%     \label{algo:momentum}
%     Let $x_1,\hat{x}_1\in\R^d$. For $k\in\N$, let $\tau_k\in (0, \infty)$. Then, the \textbf{Momentum} algorithm defines a sequence $(\hat{x}_k)_{k\in\N}$ by    
%     \begin{align*}
%         x_{k+1} &:= x_k - \tau_k G^k(\hat{x}_k), \\
%         \hat{x}_{k+1} &:= \frac{\lambda_{k}}{\lambda_{k} + 1}\hat{x}_k + \frac{1}{\lambda_{k} + 1}x_{k+1},
%     \end{align*}
%     where $\lambda_k\in (0, \infty)$ is a parameter.
% \end{algorithm}
% We will analyze the momentum method, adapted to our setting, in \cref{sec:accelerated-ssgd}

In most applications, SGD is used with \textit{iterate averaging}. In \cite{doi:10.1137/070704277}, it was
shown that the averages $\bar{x}_k := \sum_{i=1}^{k} x_k$ efficiently converge in the case of convex objectives.
Another popular averagins scheme uses \textit{iterate moving averages}, where one considers
the moving average $\hat{x}_{k+1} := (1 - \hat{\rho}_k)\hat{x}_k + \hat{\rho}_k x_{k+1}$, where $\hat{\rho}_k\in [0, 1]$ and
$(x_k)_{k\in\N}$ still denote the standard SGD iterates. We will analyze the iterate average $\bar{x}_k$ in
\cref{sec:iterate-average} and the iterate moving average $\hat{x}_k$ in \cref{sec:ima}.


\section{Norms and Inequalities}


\chapter{Sequential Penalty Methods}

\label{sec:spm}

Throughout this chapter, we fix a probability space $(\Omega, \mathcal{F}, \Prob)$.
All maps $\R^n\to\R^m$, $n,m\in\N$, are implicitly considered to be measurable with respect to
the corresponding Borel $\sigma$-algebras on $\R^n$ and $\R^m$ (\cref{ex:borel-sigma-algebra}).
We consider a more general form of problem \eqref{eq:model-problem}:
\begin{equation}
    \label{eq:general-problem}
    \tag{Q}
    \begin{aligned}
        &\min_{x \in \mathbb{R}^d} \,
            f(x) \\
        &\textup{s.\,t.} \quad A(\xi)x \leq c \quad \text{a.\,s.}\,,
    \end{aligned}
\end{equation}
where $f\colon\R^d\to\R$, $A(y)\in\R^{n\times d}$ for $y\in\R^m$,
$c\in\R^n$ and $\xi:\Omega\to\R^m$ is a random variable.
We also define the corresponding unconstrained problems
\begin{equation}
    \label{eq:penalized-general-problem}
    \tag{$\textnormal{Q}^k$}
    \begin{aligned}
        &\min_{x \in \mathbb{R}^d} \,
        \left\{ 
            f^{k}(x) := f(x) + \frac{\gamma_k}{2} \pi(x)
        \right\},
    \end{aligned}
\end{equation}
where $\pi:\R^d\to\R$ and $\gamma_k \in (0, \infty)$ for all $k\in\N$.
We state the following assumptions to refer back to.
\begin{assumption}
    \label{ass:strongly-convex-radially-unbounded}
    The objective $f$ is $\mu$-strongly convex for some $\mu > 0$.
\end{assumption}
\begin{assumption}
    \label{ass:penalty-function}
    The penalty function $\pi$ is convex and satisfies $\pi(x) \geq 0$ for all $x$ and $\pi(x) = 0$ if
    and only if $A(\xi)x \leq c$ almost surely.
\end{assumption}
\begin{assumption}
    \label{ass:penalty-parameters}
    The sequence of penalty parameters $(\gamma_k)_{k\in\N}$ is strictly increasing, unbounded, and
    satisfies $\gamma_k\in (0, \infty)$ for all $k\in\N$.
\end{assumption}
\begin{assumption}
    \label{ass:feasible-point}
    There exists at least one feasible point for \eqref{eq:general-problem}.
\end{assumption}
% We will tackle the following objectives:
% \begin{enumerate}
%     \item Proof of convergence $x_k^\star \to x^\star$
%     as $\gamma_k \to\infty$ (as well as the values $f^k(x^\star_k) \to f(x^\star)$).
%     \item How to numerically solve problem \eqref{eq:model-problem}. In particular,
%     we will derive asymptotically optimal combinations of step sizes $\tau_k$, batch sizes $b_k$
%     and penalty parameters $\gamma_k$ for (a version of) stochastic gradient descent.
% \end{enumerate}

\section{Consistency of Solutions}

% The following theorem states sufficient conditions on the objective function $f$ and the
% penalty function $\pi$ such that $x_k^\star \to x^\star$ and $f^k(x^\star_k) \to f(x^\star)$
% as $\gamma_k \to\infty$.
\begin{theorem}
    \label{thm:consistency}
    In the situation of \eqref{eq:general-problem} and \eqref{eq:penalized-general-problem}, assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-function,ass:penalty-parameters,ass:feasible-point}
    hold.
    Let $x_k^\star$ be the solution to \eqref{eq:penalized-general-problem}
    and $x^\star$ the solution to \eqref{eq:general-problem} (in particular,
    these solutions exist and are unique). Then $x^\star_k \to x^\star$ and
    $f^k(x^\star_k) \to f(x^\star)$ as $k\to\infty$.
\end{theorem}
\noindent
First, some preparation.
\begin{lemma}
    \label{lem:radially-unbounded-proper-cont-then-attains-minimum}
    If $f\colon \R^d \to \R$ is radially unbounded, continuous, and $X \subset \R^d$ is nonempty
    and closed, then $f$ attains a minimum over $X$, i.\,e.
    there exists $x^\star\in X$ such that $f(x^\star) = \inf_{x\in X} f(x)$.
\end{lemma}

\begin{proof}
    Let $x_0\in X$. Since $f$ is radially unbounded, there exists $r > 0$
    such that $f(x) \geq f(x_0)$ for all $x\in\R^d$ with $|\!|x|\!| > r$,
    therefore any minimum of $f$ -- if it exists -- must be contained in the closed ball of
    radius $r$ around $0$, which we denote by
    $B_r$. In particular, for $C := X \cap B_r$ we have
    \[
        \inf_{x\in X} f(x) = \inf_{x\in C} f(x).
    \]
    By continuity of $f$, its domain must be a closed set, which implies that $C$ is compact.
    Assume now that $f$ does not attain a minimum on $C$.
    Then there must exist a sequence $(x_k)_{k\in\N} \subset C$
    such that $\lim_{k\to\infty} f(x_k) = \inf_{x\in C} f(x)$.
    Continuous functions map compact sets to compact sets,
    hence $\inf_{x\in C} f(x) \in f(C)$ and thus there must exist some
    $x^\star\in C$ such that $f(x^\star) = \inf_{x\in C} f(x)$.
\end{proof}

\begin{lemma}
    \label{lem:radially-unbounded-f-and-f(x^n)-bounded-implies-x^n-bounded}
    Let $f\colon \R^d \to \R$ be radially unbounded and assume that $\{ f(u_k), \, k\in\N \}$
    is bounded for some sequence
    $(u_k)_{k\in\N} \subset \R^d$. Then, $(u_k)_{k\in\N}$ is a bounded sequence.
\end{lemma}

\begin{proof}
    Assume $(u_k)_{k\in\N}$ is not bounded. Then there must exist some subsequence $(u_{k_r})_{r\in\N}$
    such that $|\!|u_{k_r}|\!| \to \infty$ for $r\to\infty$. However, $f$ is radially unbounded,
    which implies $f(u_{k_r}) \to \infty$ for $r\to\infty$, which contradicts our assumption that
    $(f(u_{k_r}))_{r\in\N}$ is bounded. Hence $(u_{k_r})_{r\in\N}$ must be bounded.
\end{proof}

\begin{lemma}
    \label{lem:if-every-subsequence-contains-convergent-subsequence-then-sequence-converges}
    Let $U := \{ u_k, \, k\in\N \}$ be a subset of $\R^d$.
    Suppose that any subsequence of $U$ contains a subsequence
    that converges to $u\in\R^d$.
    Then $u_k \to u$ for $k\to\infty$.
\end{lemma}

\begin{proof}
    Assume that $u_k \not\to u$.
    Then there must exist some $\epsilon > 0$ and a sequence of natural numbers $k_1 < k_2 < \dots$ such that
    \begin{equation}
        \label{proof:lem:if-every-subsequence-contains-convergent-subsequence-then-sequence-converges:ineq}
        |\!|u_{k_r} - u|\!| \geq \epsilon
    \end{equation}
    for all $r\in\N$. However, as a subsequence of $U$, the sequence
    $(u_{k_r})_{r\in\N}$
    must simultanously contain a subsequence that converges to $u$, which contradicts
    \eqref{proof:lem:if-every-subsequence-contains-convergent-subsequence-then-sequence-converges:ineq}.
    Thus, our assumption $u_k \not\to u$ must be false.
\end{proof}
\noindent
We will now prove the main theorem.
\begin{proofof}{thm:consistency}
    From strong convexity of $f$ (\cref{ass:strongly-convex-radially-unbounded}),
    convexity of $\pi$ (\cref{ass:penalty-function}) and $\gamma_k > 0$ (\cref{ass:penalty-parameters}),
    it follows that $f^k$ is also strongly convex for all $k\in\N$
    (\cref{prop:operations-conserving-convexity}).
    Thus, for every $k \in \N$, \cref{prop:strongly-convex-implies-unique-minimizer}
    implies that there exists a unique solution $x^\star_k$
    to problem \eqref{eq:penalized-general-problem}.
    Let $x$ be any feasible point for \eqref{eq:general-problem}, which exists by \cref{ass:feasible-point}.
    Then, for any $k\in\N$,
    \begin{equation}
        \label{proof:thm:consistency:eq} 
        f(x^\star_k) \leq f^{k}(x^\star_k) \leq f^{k}(x) = f(x).
    \end{equation}
    In particular, $(f^k(x^\star_k))_{k\in\N}$ is a bounded sequence.
    Since $f$ is radially unbounded (\textcolor{red}{Make lemma.}) and
    $\pi$ is nonnegative (\cref{ass:penalty-function}),
    $f^k$ must also be radially unbounded. It follows, by
    \cref{lem:radially-unbounded-f-and-f(x^n)-bounded-implies-x^n-bounded},
    that the sequence $(x^\star_k)_{k\in\N}$ is also bounded and thus it contains a subsequence
    $(x^\star_{k_r})_{r\in\N}$ that converges to a point $x^\star_\infty \in\R^d$. For any $k\in\N$, we have
    \begin{align*}
        f^{k+1}(x^\star_{k+1}) - f^k(x^\star_k) 
        &\geq
        f^{k+1}(x^\star_{k+1}) - f^k(x^\star_{k+1}) \\
        &=
        \frac{\gamma_{k+1}}{2} \, \pi(x^\star_{k+1}) - \frac{\gamma_k}{2} \, \pi(x^\star_{k+1}) \\
        &=
        \frac{\gamma_{k+1} - \gamma_k}{2} \, \pi(x^\star_{k+1}) \\
        &\geq
        0\,,
    \end{align*}
    where we used \cref{ass:penalty-parameters,ass:penalty-function} in the last step.
    This implies that $(f^k(x^\star_k))_{k\in\N}$ is a monotonically increasing sequence.
    We know from \eqref{proof:thm:consistency:eq} that $(f^k(x^\star_k))_{k\in\N}$ must also be bounded and
    thus $(f^k(x^\star_k))_{k\in\N}$ must converge.
    In particular, we have
    \begin{equation*}
        \limsup_{k\to\infty} \,[f^k(x^\star_k) - f(x^\star_k)] < \infty.
    \end{equation*}
    By plugging in definitions for $f^k$ and $f$, we get
    \begin{equation*}
        \limsup_{k\to\infty} \frac{\gamma_k}{2} \pi(x^\star_k) < \infty.
    \end{equation*}
    The function $\pi$ is convex (\cref{ass:penalty-function}) and thus continuous
    (\cref{prop:convex-implies-continuous}).
    Hence, since $\lim_{k\to\infty} \gamma_k = \infty$ (\cref{ass:penalty-parameters}),
    the above limit can be finite only if
    \begin{equation*}
        \pi(x^\star_{\infty}) = \lim_{r\to\infty} \pi(x^\star_{k_r}) = 0,
    \end{equation*}
    which implies that $x^\star_\infty$ is feasible, by \cref{ass:penalty-function}.
    To prove optimality of $x^\star_\infty$, let $x^\star$ be the solution to \eqref{eq:general-problem}.
    Then we have, again from \eqref{proof:thm:consistency:eq},
    \[
        f(x^\star_\infty)
        = \lim_{r\to\infty} f(x^\star_{k_r})
        \leq \lim_{r\to\infty} f^{k_r}(x^\star_{k_r})
        = \lim_{k\to\infty} f^k(x^\star_k) \leq f(x^\star),
    \]
    which implies $f(x^\star_\infty) = f(x^\star)$ by feasibility of $x^\star_\infty$ and optimality of $x^\star$. This in turn implies that all inequalities must, in fact, be equalities and thus
    \[
        \lim_{k\to\infty} f^k(x^\star_k) = f(x^\star),
    \]
    as desired. Finally, by uniqueness of $x^\star$ we must
    have $x^\star_\infty = x^\star$, proving that $x^\star$ is a limit point of $(x^\star_k)_{k\in\N}$.
    Note that \cref{ass:penalty-parameters} still holds if we replace $(\gamma_k)_{k\in\N}$ by any
    subsequence $(\gamma_{k_l})_{l\in\N}$ and the same arguments imply that $x^\star$ is also a
    limit point of $(\gamma_{k_l})_{l\in\N}$.
    Hence, by \cref{lem:if-every-subsequence-contains-convergent-subsequence-then-sequence-converges},
    we in fact have $\lim_{k\to\infty} x^\star_k = x^\star$.
\end{proofof}
\begin{assumption}
    \label{ass:random-matrix}
    The random matrix $A(\xi)$ has finite second moment, which means
    $\E|\!|A(\xi)|\!|^2_F < \infty$,
    where $|\!|M|\!|_F := \sqrt{\sum_{i=1}^{n}\sum_{j=1}^{n} M_{ij}^2}$ for $M\in\R^{n\times n}$.
\end{assumption}
\begin{lemma}
    \label{lem:j-satisfies-assumption}
    In the situation of \eqref{eq:model-problem}, assume that \cref{ass:random-matrix} holds.
    Then, $j$ satisfies \cref{ass:strongly-convex-radially-unbounded}.
\end{lemma}
\begin{proof}
    It holds that
    \begin{align*}
        j(x)
        &\overset{\textnormal{def}}{=} \E\left( \frac{1}{2}|\!|A(\xi)x - b|\!|^2 \right) + \frac{\lambda}{2} |\!|x|\!|^2 \\
        &\leq
        \E(|\!|A(\xi)||\!|^2)|\!|x|\!|^2 + |\!|b|\!|^2 + \frac{\lambda}{2} |\!|x|\!|^2
        \longrightarrow \infty, \quad ||x||\to\infty,
    \end{align*}
    thus $j$ is radially unbounded. Further, for all $x\in\R^d$,
    \[
        j^{\prime\prime}(x) = \E(A(\xi)^\top A(\xi)) + \lambda I_d,
    \]
    where $I_n$ is the $d\times d$-identity matrix. By definition,
    \[
        (A(\xi)^\top A(\xi))_{ij} = \sum_{k=1}^n A(\xi)_{ki}A(\xi)_{kj}.
    \]
    \Cref{ass:random-matrix} implies that $\E(A(\xi)_{ij}^2) < \infty$ for all $i,j\in\{1,\dots, d\}$.
    Hence, by Cauchy-Schwarz,
    \[
        \E|(A(\xi)^\top A(\xi))_{ij}|
        \leq \sum_{k=1}^n \E|A(\xi)_{ki}A(\xi)_{kj}|
        \leq \sum_{k=1}^n \E(A(\xi)_{ki}^2) \E(A(\xi)_{kj}^2)
        < \infty.
    \]
    Thus, for all $x\in\R^d$, $j^{\prime\prime}(x)$ exists and, since $\lambda > 0$,
    $j^{\prime\prime}(x) - \lambda I$ is positive definite. By
    \cref{prop:pos-def-second-derivative-implies-convex},
    it follows that $j$ is ($\lambda$-)strongly convex.
\end{proof}
\noindent
Applying this to our problem of interest, we have the following useful result for determining reasonable penalty
functions $\pi$.
\begin{corollary}
    \label{cor:application-of-consistency-thm}
    In the situation of \eqref{eq:model-problem} and \eqref{eq:penalized-model-problem}, assume that
    \cref{ass:penalty-function,ass:penalty-parameters,ass:feasible-point,ass:random-matrix} hold.
    Then there exists a unique solution $x^\star$ to \eqref{eq:model-problem} and, for all $k\in\N$, there
    exists a unique solution $x^\star_k$ to \eqref{eq:penalized-model-problem}. These solutions satisfy
    $\lim_{k\to\infty} x^\star_k = x^\star$, $\lim_{k\to\infty} j^k(x^\star_k) = j(x^\star)$.
\end{corollary}
\begin{proof}
    By \cref{lem:j-satisfies-assumption}, $j$ satisfies \cref{ass:strongly-convex-radially-unbounded}.
    The claim now follows from \cref{thm:consistency}.
\end{proof}


\section{Sequential SGD}

We will now analyze a form of stochastic gradient descent to efficiently solve \eqref{eq:general-problem}.
\begin{algorithm}
    \label{algo:ssgd-algo}
    For $k\in\N$, let $x_1\in\R^d,\tau_k,\gamma_k \in (0, \infty)$ and $b_k\in\N$.
    The \textbf{Sequential SGD (SSGD)} iterates have the form
    \[
        x_{k+1} := x_k - \tau_k \tilde{G}^k(x_k)\,,
    \]
    where 
    \[
        \tilde{G}^k(x) := \frac{1}{b_k} \sum_{j=1}^{b_k} G^{k}(x, \xi_k^j)\,,
    \]
    $(\xi_i^j)_{i=1,\dots,k, j=1,\dots b_k}$ are i.\,i.\,d.
    samples from the distribution of $\xi$ and $G^{k}(x, \xi)$
    is a stochastic subgradient of $f^k$ at $x$.
    We refer to $\tau_k$ as a \textbf{step size}, $\gamma_k$ as a \textbf{penalty parameter} and
    $b_k$ as a \textbf{batch size}.
\end{algorithm}
\noindent
Let $x^\star$ be the solution to \eqref{eq:general-problem}.
Our goal is to to determine appropriate parameters $\tau_k$, $\gamma_k$ and $b_k\in\N$,
such $\E|\!|x_k - x^\star|\!|$ converges to zero as fast as possible.
There are several difficulties here.
One is that we do not use gradients from our main objective in \eqref{eq:general-problem},
but from the surrogate objective \eqref{eq:penalized-general-problem}. In addition,
this surrogate depends on $\gamma_k$, which may need to satisfy $\gamma_k \to\infty$ -- that is,
the surrogate objective changes between iterations. Further,
the squared gradient norm $\E|\!|G^{k}(x, \xi)|\!|^2$ grows quadratically in $\gamma_k$ and $|\!|x|\!|$,
which goes against standard assumptions in the literature.
These difficulties prevent us from being able to directly apply standard analysis techniques
like the ones found in \cite{doi:10.1137/070704277}, for example.
Because of this, we will first decompose $\E|\!|x_k - x^\star|\!|$ as follows:
\begin{equation}
    \label{ineq:u_k-u^star}
    \E|\!|x_k - x^\star|\!| \leq  \E|\!|x_k - x_k^\star|\!| + |\!|x_k^\star - x^\star|\!|,
\end{equation}
where we used the triangle inequality.
In the following sections, we will derive bounds for the two terms
on the right-hand side and use those bounds to determine appropriate sequences $(\tau_k)_{k\in\N}$, $(\gamma_k)_{k\in\N}$ and $(b_k)_{k\in\N}$ to guarantee convergence of the algorithm.
In the following, all statements involving random variables are understood to hold almost surely,
unless otherwise stated. Looking at \eqref{ineq:u_k-u^star}, there are two terms we need to bound:
$\E|\!|x_k - x_k^\star|\!|$ and $|\!|x_k^\star - x^\star|\!|$.
We refer to the former as the \textbf{tracking error} and the latter as the \textbf{surrogate error.}
We will refer to the following additional assumptions.
\begin{assumption}
    \label{ass:sampling}
    We can sample arbitrarily many independent random variables $\xi_i^j$, $i,j\in\N$, from the distribution of $\xi$.
\end{assumption}
\begin{assumption}
    \label{ass:f-is-smooth}
    The objective $f$ is $L$-smooth.
\end{assumption}
\begin{assumption}
    \label{ass:smoothness-of-f-and-pi}
    The objective $f$ and the penalty $\pi$ are differentiable and have Lipschitz gradients. More specifically,
    there exist $L,L_\pi\in (0, \infty)$ such that, for all $x,y\in\R^d$, $|\!|\nabla f(x) - \nabla f(y)|\!| \leq L|\!|x - y|\!|$ and
    $|\!|\nabla \pi(x) - \nabla \pi(y)|\!| \leq L_\pi|\!|x - y|\!|$.
\end{assumption}
\begin{assumption}
    \label{ass:quad-bounded-variance}
    If $f^k$ is subdifferentiable at a point $x\in\R^d$ for some $k\in\N$,
    let $g^k(x)\in\partial f(x)$ and let $G^k(x, \xi)$ be a stochastic gradient of $f^k$ at $x$.
    Then there exists a constant $C>0$, such that
    \[
        \Var(G^k(x, \xi)) \leq C \left(|\!|x|\!|^2 + |\!|x|\!|^2\gamma_k^2 + \gamma_k^2 + 1\right).
    \]
\end{assumption}
\begin{assumption}
    \label{ass:active-set}
    There exists a $K\in\N$ such that, for all $k\geq K$, the
    active sets $\mathcal{A}(x_k^\star, \omega)$ are almost surely identical to $\mathcal{A}(x^\star, \omega)$, i.\,e.
    $\mathcal{A}(x_k^\star, \omega) = \mathcal{A}(x^\star, \omega)$ for all $k\geq K$ and almost every $\omega\in\Omega$.
\end{assumption}

\begin{lemma}
    \label{lem:j-and-pi-are-smooth}
    Let \cref{ass:random-matrix} hold. Then, the objective $j$ and the penalty $\pi$ in \eqref{eq:penalized-model-problem} are both Lipschitz smooth.
    More precisely, $j$ is $\E|\!|A^\top(\xi)A(\xi) + \lambda \,I_d|\!|_F$-smooth and $\pi$ is $\E|\!|A^\top(\xi)A(\xi)|\!|_F$-smooth.
\end{lemma}
\begin{proof}
    We will first show that any quadratic is Lipschitz-smooth. Let $f(x) := 1/2 \,x^\top A x + b^\top x + c$ for $x\in\R^d$,
    matrix $A\in\R^{d\times d}$, $b\in\R^d$, and $c\in\R$. Differentiating $f$, we get
    \[
        \nabla f(x) = A^\top (Ax - b)\,,
    \]
    hence, for all $x,y\in\R^d$,
    \begin{equation}
        \label{eq:proof:lem:j-and-pi-are-smooth}
        |\!| \nabla f(x) - \nabla f(y) |\!| = |\!| A^\top A(x - y) |\!| \leq |\!|A^\top A|\!|_F \, |\!|x - y|\!|\,.        
    \end{equation}
    Thus $f$ is $|\!|A^\top A|\!|_F$-smooth. Note that we can write $j$ as
    \[
        j(x) = \frac{1}{2}\,x^\top\, \E\Big(A^\top(\xi)A(\xi) + \lambda \,I_d\Big)\, x - \Big\langle \E(A(\xi))x,\, b \Big\rangle +\frac{1}{2} b^\top b\,,
    \]
    so, by \eqref{eq:proof:lem:j-and-pi-are-smooth}, the fact that $|\!|\E(A^\top(\xi)A(\xi)) + \lambda \,I_d|\!|_F \leq \E|\!|A^\top(\xi)A(\xi) + \lambda \,I_d|\!|_F$, and \cref{ass:random-matrix},
    it follows that $j$ is $\E|\!|A^\top(\xi)A(\xi) + \lambda \,I_d|\!|_F$-smooth.

    Next, we consider $\pi$. Let $g(t) := \max(0, t)$ for $t\in\R$. Clearly, for $t,s \in (0,\infty)$,
    we have $|g(t) - g(s)| = |t - s|$. If $t\geq 0$ and $s \leq 0$, we have $|g(t) - g(s)| = t \leq t - s = |t - s|$.
    By symmetry, the same holds if $t \leq 0$ and $s \geq 0$. We conclude that
    $|g(t) - g(s)| \leq |t - s|$ for all $t,s\in\R$, making $g$ $1$-Lipschitz continuous. Now,
    \[
        \nabla \pi(x) = 2 \E(A^\top(\xi)(A(\xi)x - c)_+)\,,
    \]
    where $(x)_+$ applies $g$ element-wise to all components of $x = (x_1,\dots, x_d)^\top\in\R^d$. We thus have
    \begin{align*}
        |\!| \nabla \pi(x) - \nabla \pi(y) |\!| &= 2 |\!| \E(A^\top(\xi)(A(\xi)x - c)_+ - A^\top(\xi)(A(\xi)y - c)_+) |\!| \\
                                            &\leq \E\Big|\!\Big| \left( A^\top(\xi)A(\xi)x -  A^\top(\xi)c \right)_+ - \left( A^\top(\xi)A(\xi)y -  A^\top(\xi)c \right)_+ \Big|\!\Big| \\
                                            &\leq \E|\!|A^\top(\xi)A(\xi) (x-y)|\!| \\
                                            &\leq \E|\!|A^\top(\xi)A(\xi)|\!|_F |\!|x - y|\!|\,,
    \end{align*}
    as desired.
\end{proof}

\subsection{Bounding the surrogate error}

In this section, we will restrict ourselves to special cases for $\pi$, in order to bound the surrogate
error $|\!|x_k^\star - x^\star|\!|$.

\begin{theorem}
    \label{thm:surrogate-error-squared-hinge-loss}
    In the situations of \eqref{eq:general-problem} and \eqref{eq:penalized-general-problem}, let
    $\pi(x) := \E|\!|(0, A(\xi)x - c)_+|\!|^2$ and assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-parameters,ass:feasible-point,ass:random-matrix,ass:active-set,ass:f-is-smooth} hold.
    Then there exists a unique solution $x^\star$ to \eqref{eq:general-problem} and, for all $k\in\N$, there
    exists a unique solution $x^\star_k$ to \eqref{eq:penalized-general-problem}. Further, we have
    \[
        |\!|x_k^\star - x^\star|\!| = \mathcal{O}(\gamma_k^{-1}).
    \]
\end{theorem}
\noindent
The following statement will be used multiple times, so we state it here, before proving
\cref{thm:surrogate-error-squared-hinge-loss}.
\begin{lemma}
    \label{lem:square-hinge-penalty-satisfies-assumption}
    Let $A(\xi)$ satisfy \cref{ass:random-matrix}. Then, the function $\pi(x) := \E|\!|(0, A(\xi)x - c)_+|\!|^2$, $x\in\R^d$, satisfies \cref{ass:penalty-function}.
\end{lemma}
\begin{proof}
    The function $r(t) := \max(0, t)$ is convex and $t\mapsto t^2$ is convex and nondecreasing on $\R$ (\cref{ex:convex-functions}).
    Hence, their composition, $h(t):= \max(0, t)^2$, is also convex (\cref{prop:convex-circ-increasing-is-convex}).
    Now, for $x = (x_1, \dots, x_d)^\top\in\R^d$, we
    interpret $r(x)$ as the vector $(r(x_1), \dots, r(x_d))^\top$. Then, the function
    \[
        \phi(x) := ||r(x)||^2 = \sum_{i=1}^{d} h(x_i)
    \]
    is also convex, since it is the sum of convex functions (\cref{prop:operations-conserving-convexity}).
    Thus, the function $\psi\colon \R^d\times \R^m\to\R$,
    defined by $\psi(x, y) := \phi(A(y)x - c)$, is convex in its first argument,
    by \cref{prop:operations-conserving-convexity}.
    Finally, by \cref{prop:expectation-preserves-convexity}, $\pi(x) = \E(\psi(x,\xi))$ is convex.
    Clearly, $\pi(x) \geq 0$ for all $x\in\R^d$ and $\pi(x) = 0$ if and only if $x\in\R^d$ is feasible for
    \eqref{eq:model-problem}, so $\pi$ satisfies \cref{ass:penalty-function}.
\end{proof}

\begin{proofof}{thm:surrogate-error-squared-hinge-loss}
    \Cref{ass:random-matrix} ensures that $\pi$ is well-defined. Existence and uniqueness of $x^\star$ and $x_k^\star$ for all $k\in\N$ follows from
    \cref{ass:strongly-convex-radially-unbounded,ass:feasible-point,lem:square-hinge-penalty-satisfies-assumption,prop:strongly-convex-implies-unique-minimizer,prop:operations-conserving-convexity}.
    By \cref{ass:active-set}, there exists $K\in\N$ such that $\mathcal{A}(x_k^\star, \omega) = \mathcal{A}(x^\star,\omega)$
    for all $k\geq K$ and almost every $\omega\in\Omega$. This implies that the gradient of $\pi$,
    \[
        \nabla \pi(x) = 2\,\E\big( A(\xi)^\top \max(0, A(\xi)x - c) \big),
    \]
    is affine on the set $\{x_k^\star, \, k\geq K\}$. Specifically, if $\mathcal{A}(x^\star) = \{i_1, \dots, i_r\} \subset \{1,\dots, d\}$
    for some $r\in\{1,\dots, d\}$, then, for all $k\geq K$,
    \begin{equation}
        \label{eq:proof:thm:surrogate-error-squared-hinge-loss:eq}
        \nabla \pi(x_k^\star) = 2\,\E\big( A(\xi)^\top P \cdot (A(\xi) x_k^\star - \,c) \big)
        = 2\,\E\big( A(\xi)^\top P A(\xi) \big) x_k^\star -  2\,\E\big( A(\xi)^\top \big) P \,c\,,
    \end{equation}
    where $P = (p_{ij})_{i,j\in\{1,\dots, d\}}\in\R^{d\times d}$, such that $p_{ii} = 1$ if $i\notin \mathcal{A}(x_K^\star)$
    and $p_{ij} = 0$, otherwise. Let $k\geq K$. Since $\nabla\pi(x^\star) = 0$, we have
    \[
        |\!| \nabla \pi (x_k^\star) |\!|
        = |\!| \nabla \pi (x_k^\star) - \nabla \pi (x^\star) |\!|
        = 2 \, |\!|\E(A(\xi)^\top P A(\xi))  |\!| \, |\!| x_k^\star - x^\star |\!|.
    \]
    By optimality of $x_k^\star$ for $f^k$, it holds that
    \[
        0 = \nabla f^k(x_k^\star) = \nabla f(x_k^\star) + \frac{\gamma_k}{2} \pi(x_k^\star),
    \]
    which implies
    \begin{equation}
        \label{eq:proof:thm:surrogate-error-squared-hinge-loss:ineq}
        |\!| \pi(x_k^\star) |\!|
        \leq \frac{2}{\gamma_k} |\!| \nabla f(x_k^\star) |\!|
        \leq \frac{2\, |\!| \nabla f(x_k^\star) - \nabla f(x^\star) |\!|}{\gamma_k} + \frac{2 \, |\!| \nabla f(x^\star) |\!|}{\gamma_k}.
    \end{equation}
    \Cref{ass:f-is-smooth,prop:lipschitz-gradients} imply that
    \[
        |\!| \nabla f(x_k^\star) - \nabla f(x^\star) |\!| \,\leq\, L \, |\!| x_k^\star - x^\star |\!|,
    \]
    and, combining that with \eqref{eq:proof:thm:surrogate-error-squared-hinge-loss:ineq}, we obtain
    \[
        |\!| \pi(x_k^\star) |\!| \,\leq\, \frac{2 L}{\gamma_k}\, |\!| x_k^\star - x^\star |\!| + \frac{2 \, |\!| \nabla f(x^\star) |\!|}{\gamma_k}\,.
    \]
    Together with the equality \eqref{eq:proof:thm:surrogate-error-squared-hinge-loss:eq}, we have
    \begin{equation}
        \label{eq:proof:thm:surrogate-error-squared-hinge-loss:ineq2}
        2 \, |\!| \E(A(\xi)^\top P A(\xi)) |\!| \, |\!| x_k^\star - x^\star |\!|
        \,\leq\,
        \frac{2 L}{\gamma_k}\, |\!| x_k^\star - x^\star |\!| + \frac{2 \, |\!| \nabla f(x^\star) |\!|}{\gamma_k}\,.
    \end{equation}
    Set $q := 2 \, |\!| \E(A(\xi)^\top P A(\xi)) |\!|$. By \cref{ass:penalty-parameters},
    there exists $K^\prime\geq K$ such that $2L_f/\gamma_k < q$ for all $k\geq K^\prime$. Hence,
    rearrainging \eqref{eq:proof:thm:surrogate-error-squared-hinge-loss:ineq}, we obtain
    \[
        |\!| x_k^\star - x^\star |\!| \,\leq\, \frac{2 \, |\!| \nabla f(x^\star) |\!|}{\gamma_k (q - 2 \,L_f/\gamma_k)}
        \,=\,
        \mathcal{O}(\gamma_k^{-1}),
    \]
    as desired.
\end{proofof}

\begin{corollary}
    In the situations of \eqref{eq:model-problem} and \eqref{eq:penalized-model-problem}, let
    $\pi(x) := \E|\!|(0, A(\xi)x - c)_+|\!|^2$ and assume that
    \cref{ass:penalty-parameters,ass:feasible-point,ass:random-matrix,ass:active-set} hold.
    Then there exists a unique solution $x^\star$ to \eqref{eq:model-problem} and, for all $k\in\N$, there
    exists a unique solution $x^\star_k$ to \eqref{eq:penalized-model-problem}. Further, we have
    \[
        |\!|x_k^\star - x^\star|\!| = \mathcal{O}(\gamma_k^{-1}).
    \]
\end{corollary}

\begin{proof}
    By \cref{lem:j-satisfies-assumption}, $j$ satisfies \cref{ass:strongly-convex-radially-unbounded}. Since
    $j$ is quadratic, $j$ is also Lipschitz smooth, so \cref{ass:f-is-smooth} also holds. The
    claim now follows from \cref{thm:surrogate-error-squared-hinge-loss}.
\end{proof}

\subsection{Bounding the tracking error}

The following analysis is an adaptation of techniques used in \cite{cutler2023drift}.
One notable difference is that we do not assume uniformly bounded variance or second moment of the
stochastic gradients. We introduce the following notation
\begin{notation}
    For any $k\in\N$, we define $A_k := |\!|x_k-x_k^\star|\!|^2$,
    $a_k := \E(A_k)$, and $\Delta_k := |\!|x_k^\star - x_{k+1}^\star|\!|$.
    Further, we define $\xi_k^{[b_{k}]} := (\xi_k^1, \dots, \xi_k^{b_k})\in\R^{m\times b_k}$
    and $\E_k(X) := \E\big(X \,|\, \xi_{k-1}^{[b_{k-1}]}, \dots, \xi_1^{[b_{1}]}\big)$.
    If \cref{ass:smoothness-of-f-and-pi} holds, then $f^k$ is $(L+\gamma_kL_\pi)$-smooth. In that case,
    we define $L_k := L+\gamma_kL_\pi$.
\end{notation}

\begin{theorem}
    \label{thm:tracking-error-sgd}
    In the situations of \eqref{eq:general-problem} and \eqref{eq:penalized-general-problem}, assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-function,ass:penalty-parameters,ass:feasible-point,ass:sampling,ass:smoothness-of-f-and-pi,ass:quad-bounded-variance}
    hold. Then, for all $k\in\N$, the iterates $(x_k)_{k\in\N}$ of \cref{algo:ssgd-algo} satisfy
    \[
        a_{k+1}
        \leq
        (1-\tilde{\rho}_k) a_k + 2M^2_k(1 + \gamma_k^2)\tau_k^2 + (1+\eta_k^{-1})\Delta_k^2,
    \]
    where $\tilde{\rho}_k := \mu\tau_k/2 - 2(M_k^2(1+\gamma_k^2) + L_k^2) \tau_k^2$ and $M_k^2 = \mathcal{O}(b_k^{-1})$.
\end{theorem}

\begin{remark}
    Note that the strong convexity assumption is crucial for the above result to be useful, or otherwise there would not exist a step size $\tau_k$ that would lead to a contraction factor in front of $a_k$.
\end{remark}
\noindent
We will use the following two lemmata in the proof of \cref{thm:tracking-error-sgd}.

\begin{lemma}
    \label{lem:bound-on-G^k(u_k)-squared}
    In the situations of \eqref{eq:general-problem} and \eqref{eq:penalized-general-problem}, assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-function,ass:penalty-parameters,ass:feasible-point,ass:sampling,ass:smoothness-of-f-and-pi,ass:quad-bounded-variance}
    hold. Then, for all $k\in\N$, the iterates $(x_k)_{k\in\N}$ of \cref{algo:ssgd-algo} satisfy
    \[
        \E_k |\!|\tilde{G}^k(x_k)|\!|^2 \leq\left( M_k^2(1+\gamma_k^2) + L_k^2 \right) A_k + M^2_k(1 + \gamma_k^2)\,,
    \]
    where $M_k^2 = \mathcal{O}(b_k^{-1})$.
\end{lemma}

\begin{proof}
    By \cref{prop:variance-linear-for-independent-rvs} and
    \cref{ass:quad-bounded-variance}, we have
    \begin{align*}
        \Var_k(\tilde{G}^k(x_k))
            &= \frac{1}{b_k} \Var_k(G^k(x_k, \xi)) \\
            &\leq \frac{C}{b_k} \left(|\!|x_k|\!|^2 + |\!|x_k|\!|^2\gamma_k^2 + \gamma_k^2 + 1\right) \\
            &\leq \frac{C}{b_k} \left( 2( |\!|x_k - x_k^\star|\!|^2
                    + |\!|x_k^\star|\!|^2 )
                    + 2\gamma_k^2 ( |\!|x_k - x_k^\star|\!|^2 + |\!|x_k^\star|\!|^2 )
                    + \gamma_k^2 + 1\right) \\
            &= \frac{C}{b_k} \left( 2(1+\gamma_k^2) |\!|x_k - x_k^\star|\!|^2
                    + (2|\!|x_k^\star|\!|^2 + 1)\gamma_k^2 + 2|\!|x_k^\star|\!|^2 + 1\right) \\
            &= \frac{C}{b_k} \left( 2(1+\gamma_k^2) A_k
                    + (2|\!|x_k^\star|\!|^2 + 1)\gamma_k^2 + 2|\!|x_k^\star|\!|^2 + 1\right) \\
            &\leq \frac{1}{b_k}\left( 2C(1+\gamma_k^2) A_k
                    + M^2(1 + \gamma_k^2) \right),
    \end{align*}
    where $M^2 := \sup_{k\in\N} C(2|\!|x_k^\star|\!|^2 + 1)$. Note that, by \cref{thm:consistency}, the sequence $(x_k^\star)_{k\in\N}$ converges,
    and so $M^2 < \infty$. Next, note that, by optimality of $x_k^\star$ for $f^k$ and \cref{ass:smoothness-of-f-and-pi}, we have
    \[
        |\!|\nabla f^k(x_k)|\!|^2 = |\!|\nabla f^k(x_k) - \nabla f^k(x_k^\star)|\!|^2 \leq L_k^2 \, A_k.
    \]
    Putting the two bounds together and using \cref{prop:variance-identity}, we get
    \begin{align*}
        \E_k |\!|\tilde{G}^k(x_k)|\!|^2
            &= \Var(\tilde{G}^k(x_k)) + |\!|\nabla f^k(x_k)|\!|^2 \\
            &\leq \frac{1}{b_k}\left( 2C(1+\gamma_k^2) A_k + M^2(1 + \gamma_k^2) \right) + L_k^2 \, A_k \\
            &= \left( \frac{2C}{b_k}(1+\gamma_k^2) + L_k^2 \right) A_k + \frac{M^2}{b_k}(1 + \gamma_k^2) \\
            &= \left( M_k^2(1+\gamma_k^2) + L_k^2 \right) A_k + M^2_k(1 + \gamma_k^2),
    \end{align*}
    where $M_k^2 := \max(2C, \,M^2)/b_k$.
\end{proof}

\begin{lemma}
    \label{lem:one-step-improvement}
    In the situations of \eqref{eq:general-problem} and \eqref{eq:penalized-general-problem}, assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-function,ass:penalty-parameters,ass:feasible-point,ass:sampling,ass:smoothness-of-f-and-pi,ass:quad-bounded-variance}
    hold. Then, for all $k\in\N$, the iterates of \cref{algo:ssgd-algo} satisfy
    \[
        \E|\!|x_{k+1} - x_k^\star|\!|^2 \leq (1 - q_k)a_k + M^2_k(1 + \gamma_k^2) \tau_k^2\,,
    \]
    for all $k\in\N$, where $q_k := \mu\tau_k - (M_k^2(1+\gamma_k^2) + L_k^2) \tau_k^2$ and $M_k^2 = \mathcal{O}(b_k^{-1})$ .
\end{lemma}

\begin{proof}
    Plugging in the definition of $x_{k+1}$ and expanding, we get
    \begin{align*}
        |\!|x_{k+1} - x_k^\star|\!|^2 &= |\!|x_k - x^\star_k - \tau_k \tilde{G}^k(x_k)|\!|^2 \\
                            &= A_k + \tau_k^2\,|\!|\tilde{G}^k(x_k)|\!|^2 - 2 \tau_k \langle x_k - x_k^\star, \tilde{G}^k(x_k) \rangle.
    \end{align*}
    Applying $\E_k$ on both sides, we get
    \begin{align*}
        \E_k|\!|x_{k+1} - x_k^\star|\!|^2 &= A_k + \tau_k^2\,\E_k|\!|\tilde{G}^k(x_k)|\!|^2 - 2 \tau_k \langle x_k - x_k^\star, g^k(x_k) \rangle.
    \end{align*}
    Strong convexity of $f^k$ yields
    \begin{equation}
        \label{eq:proof:lem:one-step-improvement}
        \E_k|\!|x_{k+1} - x_k^\star|\!|^2 \leq (1-\mu\tau_k)A_k + \tau_k^2\,\E_k|\!|\tilde{G}^k(x_k)|\!|^2.        
    \end{equation}
    \Cref{lem:bound-on-G^k(u_k)-squared} yields
    \[
        \E_k |\!|\tilde{G}^k(x_k)|\!|^2 \leq\left( M_k^2(1+\gamma_k^2) + L_k^2 \right) A_k + M^2_k(1 + \gamma_k^2)\,.
    \]
    Plugging this into \eqref{eq:proof:lem:one-step-improvement}, we get
    \[
        \E_k|\!|x_{k+1} - x_k^\star|\!|^2 \leq \left( 1 - \mu\tau_k + \Big( M_k^2(1+\gamma_k^2) + L_k^2 \Big)\tau_k^2 \right) A_k
        + M_k^2(1 + \gamma_k^2)\tau_k^2.
    \]
    Now, taking expectations of both sides, \cref{prop:conditional-expectation-properties} yields the claim.
\end{proof}
\noindent
We will now prove the first main theorem of this subsection.
\begin{proofof}{thm:tracking-error-sgd}
    Let $k\in\N$. First, we have
    \begin{align*}
        A_{k+1} &= \E_k|\!|x_{k+1} - x_k^\star + x_k^\star - x_{k+1}^\star|\!|^2 \\
                &= \E_k|\!|x_{k+1} - x_k^\star|\!|^2 + \Delta^2_k + 2\,\E_k\langle x_{k+1} - x_k^\star, x_k^\star - x_{k+1}^\star \rangle\,.
    \end{align*}
    We can apply the Cauchy-Schwarz and Young inequalities to obtain
    \[
        A_{k+1} \leq (1+\eta_k)\E_k|\!|x_{k+1} - x_k^\star|\!|^2 + (1+\eta_k^{-1})\,\Delta^2_k\,,
    \]
    for all $\eta_k > 0$. Hence, by applying $\E(\cdot)$ on both sides,
    \begin{equation}
        \label{eq:proof:thm:tracking-error-sgd}
        a_{k+1} \leq (1+\eta_k)\E|\!|x_{k+1} - x_k^\star|\!|^2 + (1+\eta_k^{-1})\,\Delta^2_k\,.        
    \end{equation}
    Using \cref{lem:one-step-improvement}, we can bound the first term:
    \begin{equation}
        \label{eq:proof:thm:tracking-error-sgd-2}
        (1+\eta_k)\E|\!|x_{k+1} - x_k^\star|\!|^2
            \leq (1+\eta_k)(1 - q_k)a_k + (1+\eta_k) M^2_k(1 + \gamma_k^2)\tau_k^2\,.
    \end{equation}
    We choose $\eta_k = \min(1, \mu\tau_k/2)$ and have
    \begin{align*}
        (1+\eta_k)(1 - q_k)
                    &= (1+\eta_k)\left( 1 - \mu\tau_k + \Big(M_k^2(1+\gamma_k^2) + L_k^2\Big) \tau_k^2 \right) \\
                    &= 1 + \eta_k - (1+\eta_k)\mu\tau_k + (1+\eta_k)\Big(M_k^2(1+\gamma_k^2) + L_k^2\Big)\tau_k^2 \\
                    &\leq 1 + \frac{\mu}{2}\tau_k - \mu\tau_k + 2\Big(M_k^2(1+\gamma_k^2) + L_k^2\Big)\tau_k^2 \\
                    &= 1 - \frac{\mu}{2}\tau_k + 2\Big(M_k^2(1+\gamma_k^2) + L_k^2\Big)\tau_k^2\,,
    \end{align*}
    Plugging this into \eqref{eq:proof:thm:tracking-error-sgd-2}
    and using $1+\eta_k \leq 2$ again, we arrive at the bound
    \[
        (1+\eta_k)\E|\!|x_{k+1} - x_k^\star|\!|^2
            \leq \left( 1 - \frac{\mu}{2}\tau_k + 2\Big(M_k^2(1+\gamma_k^2) + L_k^2\Big)\tau_k^2 \right)a_k + 2M_k^2(1 + \gamma_k^2)\tau_k^2.
    \]
    Together with \eqref{eq:proof:thm:tracking-error-sgd}, we obtain the first claim.
    The proof of the second claim follows analogously after applying the second part of \cref{lem:one-step-improvement}
    to bound \eqref{eq:proof:thm:tracking-error-sgd}.
\end{proofof}


\subsection{Convergence rates}

In the previous sections, we proved bounds on the iterates of \cref{algo:ssgd-algo}.
We will now use these bounds to choose asymptotically optimal policies for the parameters $(\tau_k)_{k\in\N}$,
$(\gamma_k)_{k\in\N}$, and $(b_k)_{k\in\N}$ in \cref{algo:ssgd-algo}, for solving \eqref{eq:model-problem}.
\begin{assumption}
    \label{ass:random-matrix-2}
    The random matrix $A(\xi)$ satisfies $\E|\!|A(\xi)|\!|_\textnormal{F}^4 < \infty$.
\end{assumption}

\begin{theorem}
    \label{thm:convergence-rate-squared-hinge-penalty}
    In the situations of \eqref{eq:model-problem} and \eqref{eq:penalized-model-problem}, let
    $\pi(x) := \E|\!|(0, A(\xi)x - c)_+|\!|^2$ and assume that
    \cref{ass:random-matrix-2,ass:feasible-point,ass:penalty-parameters,ass:sampling,ass:active-set} hold.
    Then, for any $\epsilon \in (0, 1/3)$,
    \cref{algo:ssgd-algo} with parameters $\tau_k = k^{-2/3}$,
    $\gamma_k = k^{1/3 - \epsilon}$ and $b_k = 1 + k^{2(1/3 - \epsilon)}$, converges, and yields
    iterates $(x_k)_{k\in\N}$ that satisfy
    \[
        \E|\!|x_k - x^\star|\!| = \mathcal{O}(\gamma_k^{-1}) = \mathcal{O}(k^{-1/3 + \epsilon})\,,
    \]
    where $x^\star$ denotes the solution to \eqref{eq:model-problem}.
    Furthermore, it holds that
    \[
        \E(\pi(x_k)) = \mathcal{O}(\gamma_k^{-2}) = \mathcal{O}(k^{-2/3 + 2\epsilon})\,.
    \]
\end{theorem}

\begin{lemma}
    \label{lem:bound-on-Delta_k}
    In the situations of \eqref{eq:general-problem} and \eqref{eq:penalized-general-problem}, assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-function,ass:penalty-parameters,ass:feasible-point} hold.
    Additionally, assume that $f$ and $\pi$ are differentiable.
    Then we have
    \[
        \Delta_k \,\leq\, \frac{\gamma_{k+1} - \gamma_k}{\gamma_k}\, \frac{G}{\mu}\,,
    \]
    for all $k\in\N$, where $G := 2\,\sup_{k\in\N} |\!|\nabla f(x_{k}^\star)|\!| < \infty$.
\end{lemma}

\begin{proof}
    Let $k\in\N$. The claim clearly holds if $x_k^\star = x_{k+1}^\star$. Assume for the rest of the proof that
    $x_k^\star \neq x_{k+1}^\star$.
    We have
    \[
        f(x) = f^k(x) - \frac{\gamma_k}{2} \pi(x)\,,
    \]
    which implies
    \begin{equation}
        \label{proof:bound-successive-optima-by-successive-regularization:eq}
        \nabla f(x_k^\star) = -\frac{\gamma_k}{2} \nabla \pi(x_k^\star)\,,
    \end{equation}
    by optimality of $x_k^\star$ for $f^k$. We can apply strong convexity of $f$ (\cref{ass:strongly-convex-radially-unbounded}) and
    \cref{prop:strongly-convex-subdifferentiable-bound} to obtain
    \begin{align*}
        \frac{\mu}{2}\Delta_k^2
                \,&\leq\, \langle x_k^\star - x_{k+1}^\star, \nabla f(x_k^\star) - \nabla f(x_{k+1}^\star) \rangle \\
                &=\, \big\langle x_k^\star - x_{k+1}^\star, -\frac{\gamma_k}{2}\nabla \pi(x_k^\star) + \frac{\gamma_{k+1}}{2}\nabla \pi(x_{k+1}^\star) \big\rangle \\
                &=\, \big\langle x_k^\star - x_{k+1}^\star, \frac{\gamma_{k+1} - \gamma_k}{2}\nabla \pi(x_{k+1}^\star) + \frac{\gamma_k}{2}(\nabla \pi(x_{k+1}^\star) - \nabla \pi(x_{k}^\star)) \big\rangle \\
                &=\, \frac{\gamma_{k+1} - \gamma_k}{2}\langle x_{k}^\star - x_{k+1}^\star, \nabla \pi(x_{k+1}^\star) \rangle - \frac{\gamma_k}{2} \langle x_{k+1}^\star - x_{k}^\star, \nabla \pi(x_{k+1}^\star) - \nabla \pi(x_k^\star) \rangle\,.
    \end{align*}
    Convexity of $\pi$ (\cref{ass:penalty-function}) implies that
    $\langle x-y, \pi(x) - \pi(y) \rangle \geq 0$ for all $x,y\in\R^d$, by \cref{prop:convex-implies-monotone-gradient}.
    Hence, by positivity of $\gamma_k$ (\cref{ass:penalty-parameters}), we have
    \[
        \frac{\mu}{2}\Delta_k^2 \,\leq\, \frac{\gamma_{k+1} - \gamma_k}{2}\langle x_{k}^\star - x_{k+1}^\star, \nabla \pi(x_{k+1}^\star) \rangle
    \]
    and an application of the Cauchy-Schwarz inequality along with the fact $\gamma_{k+1} > \gamma_k$ (\cref{ass:penalty-parameters}), yield
    \[
        \frac{\mu}{2}\Delta_k^2 \,\leq\, \frac{\gamma_{k+1} - \gamma_k}{2} \cdot \Delta_k \cdot |\!|\nabla \pi(x_{k+1}^\star)|\!|\,.
    \]
    Dividing both sides by $\mu/2\,\Delta_k$, we get
    \[
        \Delta_k \,\leq\, \frac{\gamma_{k+1} - \gamma_k}{\mu}\, |\!|\nabla \pi(x_{k+1}^\star)|\!|\,.
    \]
    Substituting $\nabla f$ for $\nabla \pi$ via \eqref{proof:bound-successive-optima-by-successive-regularization:eq}, we arrive at
    \[
        \Delta_k \,\leq\, \frac{\gamma_{k+1} - \gamma_k}{\mu} \, \frac{2\,|\!|\nabla f(x_{k+1}^\star)|\!|}{\gamma_k} = \frac{\gamma_{k+1} - \gamma_k}{\gamma_k} \, \frac{2\,|\!|\nabla f(x_{k+1}^\star)|\!|}{\mu}\,.
    \]
    Finally, by \cref{thm:consistency}, we know that $x_k^\star \to x^\star$ for $k\to~\infty$.
    Hence, continuity of $\nabla f$ implies that $(\nabla f(x_k^\star))_{k\in\N}$ also converges, and in particular
    $\sup_{k\in\N} |\!|\nabla f(x_{k}^\star)|\!| < \infty$. Hence,
    \[
        \Delta_k \,\leq\, \frac{G}{\mu} \, \frac{ \gamma_{k+1} - \gamma_k }{\gamma_k}\,,
    \]
    where $G := 2\,\sup_{k\in\N} |\!|\nabla f(x_{k}^\star)|\!|$, as desired.
\end{proof}

\begin{lemma}[Chung's lemma]
    \label{lem:chungs-lemma}
    Let $(\alpha_k)_{k\in\N}$ be a nonnegative scalar sequence and $k_0\in\N$ be such that
    \[
        \alpha_{k+1} \leq \left( 1 - \frac{a}{k^s} \right)\alpha_k + \mathcal{O}\left(\frac{b}{k^{s+t}}\right)
    \]
    for all $k\geq k_0$ and some $0 < s \leq 1$, $a,b,t > 0$. Then, it holds that
    \[
        \alpha_k = \mathcal{O}\left(\frac{1}{k^t}\right).
    \]
\end{lemma}
\begin{proof}
    See \cite{chung1954stochastic}.
\end{proof}
\noindent
We can now prove the main theorem of this section.
\begin{proofof}{thm:convergence-rate-squared-hinge-penalty}
    Let $k\in\N$. By the triangle inequality,
    \begin{equation}
        \label{proof:thm:convergence-rate-squared-hinge-penalty:eq}
        \E|\!|x_k - x^\star|\!| \leq \E|\!|x_k - x_k^\star|\!| + |\!|x_k^\star - x^\star|\!|.
    \end{equation}
    First, we analyze $\E|\!|x_k - x_k^\star|\!|$.
    We want to use \cref{thm:tracking-error-sgd}, but for this we first need to show that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-function,ass:quad-bounded-variance,ass:smoothness-of-f-and-pi} hold.
    By Jensen's inequality (\cref{prop:jensens-inequality}), \cref{ass:random-matrix-2} implies \cref{ass:random-matrix}.
    Hence, by \cref{lem:j-satisfies-assumption}, \cref{ass:strongly-convex-radially-unbounded} is satisfied by $j$.
    Further, by \cref{lem:square-hinge-penalty-satisfies-assumption},
    $\pi$ satisfies \cref{ass:penalty-function}.
    \Cref{ass:smoothness-of-f-and-pi} was verified in \cref{lem:j-and-pi-are-smooth}.
    What's left to show is that \cref{ass:quad-bounded-variance} holds.
    Let $Q(\xi) := A(\xi)^\top A(\xi)$, $\tilde{b}(\xi) := A(\xi)^\top b$, and $\tilde{c} := A(\xi)^\top c$.
    A stochastic gradient of $j^k$ at $x\in\R^d$, denoted by $G^k(x, \xi)$, is given by
    \begin{align*}
        G^k(x, \xi)
            &= A(\xi)^{\top} \big( A(\xi)x - b + \gamma \, (0,A(\xi)x-c)_+ \big) + \lambda x \\
            &= Q(\xi) x - \tilde{b}(\xi) + \gamma \, (0,\, Q(\xi)x - \tilde{c}(\xi))_+ + \lambda x.
    \end{align*}
    We have
    \begin{align*}
        |\!| G^k(x, \xi) |\!|
            &\leq |\!| Q(\xi) x |\!| + |\!| \tilde{b}(\xi) |\!|
                    + \gamma \, \big( |\!| Q(\xi)x |\!| + |\!| \tilde{c}(\xi) |\!| \big) + \lambda |\!|x|\!| \\
            &\leq |\!|Q(\xi)|\!|_F |\!| x |\!| + |\!| \tilde{b}(\xi) |\!|
                    + \gamma \, \big(|\!|Q(\xi)|\!|_F |\!| x |\!| + |\!| \tilde{c}(\xi) |\!|\big) + \lambda |\!|x|\!| \\
            &= (|\!|Q(\xi)|\!|_F + \lambda) |\!| x |\!|
                    + \gamma \, |\!|Q(\xi)|\!|_F |\!| x |\!| + \gamma\, |\!| \tilde{c}(\xi) |\!| + |\!| \tilde{b}(\xi) |\!|.
    \end{align*}
    % \[
    %     \nabla \pi(x) = 2\,\E\big( A(\xi)^\top \max(0, A(\xi)x - c) \big).
    % \]
    Using the inequality $(a + b + c + d)^2 \,\leq\, 4 \, (a^2 + b^2 + c^2 + d^2)$, $\forall a,b,c,d\in\R$, we can conclude
    \[
        \E|\!| G^k(x, \xi) |\!|^2
            \leq 4
                \big( 
                    \E(|\!|Q(\xi)|\!|_F + \lambda)^2 \, |\!| x |\!|^2
                    + \gamma^2 \, \E(|\!|Q(\xi)|\!|_F^2) \, |\!| x |\!|^2
                    + \gamma^2 \, \E|\!| \tilde{c}(\xi) |\!|^2
                    + \E|\!| \tilde{b}(\xi) |\!|^2
                \big).
    \]
    Note that all expectations are finite, by \cref{ass:random-matrix-2}.
    Indeed, it holds that $|\!|Q(\xi)|\!|_F^2 = |\!|A(\xi)^\top A(\xi)|\!|_F^2 \leq |\!|A(\xi)|\!|_F^4 < \infty$,
    and thus
    \[
        \E(|\!|Q(\xi)|\!|_F + \lambda)^2 \,\leq\, 2\, \E|\!|Q(\xi)|\!|_F^2 + 2 \lambda \,\leq\, 2\, \E|\!|A(\xi)|\!|_F^4 + 2 \lambda \,<\, \infty.
    \]
    The terms $\E|\!| \tilde{b}(\xi) |\!|^2$ and $\E|\!| \tilde{c}(\xi) |\!|^2$ are similarly
    bounded by a constant times $\E|\!|A(\xi)|\!|_F^2$\,, which is also finite by \cref{ass:random-matrix-2}
    and Jensen's inequality (\cref{prop:jensens-inequality}).
    Hence, \cref{ass:quad-bounded-variance} is satisfied.
    
    With our choices for $\gamma_k$ and $b_k$,
    \cref{thm:tracking-error-sgd} now yields
    \[
        a_{k+1}
        \leq
        (1-\tilde{\rho}_k) a_k + 2D\tau_k^2 + (1+\eta_k^{-1})\Delta_k^2,
    \]
    where $D\in (0, \infty)$ is a constant, $\eta_k = \min\left(1, \,\mu\tau_k/2\right)$ and
    $\tilde{\rho}_k = \mu\tau_k/2 - 2(D + L_k^2)\tau_k^2$.
    For large enough $k\in\N$, we have
    $(D + L_k^2)\tau_k^2 \approx \gamma_k^2\tau_k^2 = k^{-2/3 - 2\epsilon}$, and, since this decays
    faster than $\tau_k$, we then have
    \begin{equation}
        \label{proof:thm:convergence-rate-squared-hinge-penalty:eq:1.5}
        \tilde{\rho}_k \geq \frac{\mu\tau_k}{4}\,.
    \end{equation}
    By \cref{lem:bound-on-Delta_k}, we know that $\Delta_k = \mathcal{O}((\gamma_{k+1} - \gamma_k)/\gamma_k)$.
    To further analyze this, consider the function $h(x) := x^\alpha$ on $\R$ for some $\alpha>0$.
    By the mean value theorem, there exists some $\theta\in [x, y]$, s.\,t.
    \[
        \frac{h(y) - h(x)}{y - x} = h^\prime (\theta) = \alpha \theta^{\alpha - 1}.
    \]
    In particular, if $\alpha \leq 1$, it holds that
    \[
        \frac{h(y) - h(x)}{y - x} \leq \alpha x^{\alpha - 1}.
    \]
    Setting $\alpha = 1/3 - \epsilon$ gives $h(k) = \gamma_k$, so
    \[
        \gamma_{k+1} - \gamma_k \leq \alpha k^{\alpha - 1}.
    \]
    Hence,
    \[
        \Delta_k^2 = \mathcal{O}(k^{-2}).
    \]
    Combining this with \eqref{proof:thm:convergence-rate-squared-hinge-penalty:eq:1.5}, there exists $k_0\in\N$ such that
    \[
        a_{k+1} \leq \left( 1-\frac{\mu}{4k^{2/3}} \right)a_k + \mathcal{O}\left( \frac{1}{k^{4/3}} \right),
    \]
    for all $k\geq k_0$.
    Hence, by \cref{lem:chungs-lemma}, we have $a_k = \mathcal{O}(k^{-2/3})$ and an application of
    Jensen's inequality
    (\cref{prop:jensens-inequality}) yields $\E|\!|x_k - x_k^\star|\!| = \mathcal{O}(k^{-1/3})$.
    
    Finally, by \cref{thm:surrogate-error-squared-hinge-loss}, we know
    $|\!|x_k^\star - x^\star|\!| = \mathcal{O}(k^{-1/3 + \epsilon})$. Combining this
    with \eqref{proof:thm:convergence-rate-squared-hinge-penalty:eq}, we get
    \begin{equation}
        \label{proof:thm:convergence-rate-squared-hinge-penalty:eq:2}
        \E|\!|x_k - x^\star|\!| = \mathcal{O}(k^{-1/3 + \epsilon}).     
    \end{equation}
    The remaining claim follows from the facts that $\pi$ has Lipschitz gradients and
    $\pi(x^\star) = \nabla \pi(x^\star) = 0$, which together imply (\cref{prop:lipschitz-gradients})
    \begin{align*}
        \E(\pi(x_k)) &= \E(\pi(x_k) - \pi(x^\star)) \\
                 &\leq \E\left(\big\langle x_k - x^\star, \nabla \pi(x^\star) \big\rangle + \frac{L_\pi}{2} |\!|x_k - x^\star|\!|^2\right) \\
                 &= \frac{L_\pi}{2} \E|\!|x_k - x^\star|\!|^2
    \end{align*}
    for some constant $L_\pi\in (0, \infty)$. The claim now follows from \eqref{proof:thm:convergence-rate-squared-hinge-penalty:eq:2}.
\end{proofof}

\begin{remark}
    The $\epsilon$ in the definition of $\gamma_k$ is needed in order
    to ensure that the factor $1 - \tilde{\rho}_k$ is eventually smaller than $1$ for
    all $k$ large enough, without needing to know the constants involved in $\tilde{\rho}_k$.
\end{remark}



\subsection{Iterate averaging}
\label{sec:iterate-average}

We will now analyze the convergence properties of the iterate average
$\bar{x}_k := 1/k \sum_{i=1}^{k} x_i$, where $x_i$ denotes the $i$th iterate
of \cref{algo:ssgd-algo}.
\begin{lemma}
    \label{lemma:average-inherits-asymptotics}
    Let $(\alpha_k)_{k\in\N}$ be a sequence of real numbers such that $\alpha_k = \mathcal{O}(k^{-a})$
    for some $a\in (0, 1)$. Then, we have
    \[
        \frac{1}{k} \sum_{i=1}^{k} \alpha_i = \mathcal{O}(k^{-a})\,.
    \]
\end{lemma}
\begin{proof}
    If $\alpha_k = \mathcal{O}(k^{-a})$, then there exists a constant $c\in (0, \infty)$ and $k_0\in\N$,
    such that $\alpha_k \leq c\,k^{-a}$ for all $k \geq k_0$, hence
    \begin{align*}
        \frac{1}{k} \sum_{i=1}^{k} \alpha_i - \frac{1}{k} \sum_{i=1}^{k_0-1} \alpha_i
            &=  \frac{1}{k} \sum_{i=k_0}^{k} \alpha_i \\
            &\leq \frac{c}{k} \sum_{i=k_0}^{k} i^{-a} \\
            &\leq \frac{c}{k} \int_{k_0}^{k} x^{-a} \, dx \\
            &=  \frac{c\, (k^{1-a} - k_0^{1-a})}{k \, (1-a)} = \mathcal{O}(k^{-a})\,.    
    \end{align*}
    Since $1/k \sum_{i=1}^{k_0-1} a_i = \mathcal{O}(k^{-1})$ and $a\in (0, 1)$,
    we obtain $1/k\sum_{i=1}^{k} \alpha_i = \mathcal{O}(k^{-a})$.
\end{proof}

\begin{theorem}
    \label{thm:iterate-averaging-square-hinge-penalty}
    In the situations of \eqref{eq:model-problem} and \eqref{eq:penalized-model-problem},
    let $\pi(x) = \E|\!|(A(\xi)x - b)_+|\!|^2$,
     and assume that \cref{ass:feasible-point,ass:penalty-parameters,ass:random-matrix-2,ass:sampling,ass:active-set} hold.
    Then, for all $\epsilon \in (0, 1/3)$, \cref{algo:ssgd-algo} with parameters
    $\tau_k = k^{-2/3}$, $\gamma_k = k^{1/3 - \epsilon}$ and $b_k = 1 + k^{2(1/3 - \epsilon)}$, converges, and yields
    iterates $(x_k)_{k\in\N}$ that satisfy
    \[
        \E|j^k(\bar{x}_k) - j(x^\star)| = \mathcal{O}(\gamma_k^{-1}) = \mathcal{O}(k^{-1/3 + \epsilon}),
    \]
    where $\bar{x}_k := 1/k \sum_{i=1}^{k} x_i$, for $k\in\N$, and $x^\star$ denotes the solution
    to \eqref{eq:model-problem}.
\end{theorem}
\begin{proof}
    Smoothness of $j$ and $\pi$ is verified in \cref{lem:j-and-pi-are-smooth}. Let $k\in\N$. We have
    \begin{align}
        \label{eq:proof:thm:iterate-averaging-square-hinge-penalty:triangle-split}
        \E|j^k(\bar{x}_k) - j(x^\star)| &\leq \E|j^k(\bar{x}_k) - j^k(x_k^\star)| + |j^k(x_k^\star) - j(x^\star)| \notag \\
                                  &= \E(j^k(\bar{x}_k) - j^k(x_k^\star)) + (j(x^\star) - j^k(x_k^\star)).
    \end{align}
    We will first analyze $\E(j^k(\bar{x}_k) - j^k(x_k^\star))$. By convexity of $j$ and $\pi$, we have
    \begin{equation}
        \label{eq:proof:thm:iterate-averaging-square-hinge-penalty:eq}
        j^k(\bar{x}_k) \overset{\textnormal{def}}{=} j(\bar{x}_k) + \frac{\gamma_k}{2}\pi(\bar{x}_k)
                \leq \frac{1}{k} \sum_{i=1}^{k} j(x_i) + \frac{\gamma_k}{2k}\sum_{i=1}^{k}\pi(x_i).
    \end{equation}
    Next, note that $j^i(x_i^\star) \leq j^k(x_k^\star)$ for all $i\in\{ 1,\dots,k \}$, and thus
    \begin{align*}
        0 \leq j^k(\bar{x}_k) - j^k(x_k^\star)
        &\leq
        \frac{1}{k} \sum_{i=1}^{k} (j(x_i) - j^k(x_k^\star)) + \frac{\gamma_k}{2k}\sum_{i=1}^{k}\pi(x_i) \\
        &\leq
        \frac{1}{k} \sum_{i=1}^{k} (j(x_i) - j^i(x_i^\star)) + \frac{\gamma_k}{2k}\sum_{i=1}^{k}\pi(x_i) \\
        &\overset{\textnormal{def}}{=}
        \frac{1}{k} \sum_{i=1}^{k} (j^i(x_i) - j^i(x_i^\star)) + \frac{1}{2k}\sum_{i=1}^{k}(\gamma_k-\gamma_i)\pi(x_i),
    \end{align*}
    where, in the last step, we used that $j(x) = j^i(x) - \frac{\gamma_i}{2} \pi(x)$
    for all $x\in\R^d$, $i\in\N$. By \cref{thm:convergence-rate-squared-hinge-penalty}, we have
    $\pi(x_i) = \mathcal{O}(\gamma_i^{-2})$, thus
    \[
        \frac{1}{k}\sum_{i=1}^{k}(\gamma_k-\gamma_i)\pi(x_i)
        =
        \mathcal{O}\left(\frac{1}{k} \sum_{i=1}^{k}\frac{\gamma_k-\gamma_i}{\gamma_i^2}\right).
    \]
    Using \cref{lemma:average-inherits-asymptotics}, we have
    \[
        \frac{1}{k} \sum_{i=1}^{k}\frac{\gamma_k-\gamma_i}{\gamma_i^2}
        =
        \gamma_k\left(\frac{1}{k} \sum_{i=1}^{k} \frac{1}{\gamma_i^2}\right) - \frac{1}{k}\sum_{i=1}^{k} \frac{1}{\gamma_i}
        =
        \gamma_k \cdot \mathcal{O}(\gamma_k^{-2}) - \mathcal{O}(\gamma_k^{-1}) = \mathcal{O}(\gamma_k^{-1})
    \]
    and thus
    \[
        \frac{1}{k}\sum_{i=1}^{k}(\gamma_k-\gamma_i)\pi(x_i) = \mathcal{O}(\gamma_k^{-1}).
    \]
    Next, note that $\nabla j^i(x^\star_i) = 0$ for all $i\in\N$.
    Furthermore, since $j^i$ is a linear combination of two Lipschitz-smooth functions with
    constants which we will call $L$ and $L_\pi$, respectively, \cref{prop:linear-combination-of-lipschitz-is-lipschitz} implies that
    $j^i$ must also be Lipschitz smooth with constant $L_i := L + \gamma_i  L_\pi = \mathcal{O}(\gamma_i)$, for all $i\in\N$.
    Hence, by use of
    \cref{prop:lipschitz-gradients} and \cref{thm:convergence-rate-squared-hinge-penalty}, we obtain
    \[
        j^i(x_i) - j^i(x_i^\star) \leq L_i\, |\!|x_i - x_i^\star|\!|^2 = \mathcal{O}(\gamma_i^{-1}),
    \]
    for all $i\in\N$.
    An application of \cref{lemma:average-inherits-asymptotics} now yields
    \[
        \frac{1}{k}\sum_{i=1}^{k} j^i(x_i) - j^i(x_i^\star) = \mathcal{O}(\gamma_k^{-1}),
    \]
    and combining with \eqref{eq:proof:thm:iterate-averaging-square-hinge-penalty:eq}, we get
    \begin{equation}
        \label{eq:proof:thm:iterate-averaging-square-hinge-penalty:bound1}
        j^k(\bar{x}_k) - j^k(x_k^\star) = \mathcal{O}(\gamma_k^{-1}).
    \end{equation}
    Similarly, since $\pi(x^\star) = 0$, we have
    \[
        j(x^\star) - j^k(x_k^\star) = j^k(x^\star) - j^k(x_k^\star) \leq L_k\, |\!|x^\star - x_k^\star|\!|^2
    \]
    and an application of \cref{thm:surrogate-error-squared-hinge-loss} yields
    \begin{equation}
    \label{eq:proof:thm:iterate-averaging-square-hinge-penalty:bound2}
        j(x^\star) - j^k(x_k^\star) = \mathcal{O}(\gamma_k^{-1}).        
    \end{equation}
    Combining \eqref{eq:proof:thm:iterate-averaging-square-hinge-penalty:triangle-split},
    \eqref{eq:proof:thm:iterate-averaging-square-hinge-penalty:bound1} and
    \eqref{eq:proof:thm:iterate-averaging-square-hinge-penalty:bound2}, we arrive
    at the desired result.
\end{proof}


% \subsection{Ergodic \texorpdfstring{$\mathcal{O}(k^{-1/2})$}{O(1/sqrt(k))} convergence}

% The results in this section are not very practical. However, they may be of help for eventually establishing
% a $\mathcal{O}(k^{-1/2})$ convergence rate of the values $f^k(\bar{x}_k)$ to the optimal value
% of \eqref{eq:model-problem}, $f(x^\star)$, where $\bar{x}_k = 1/k \sum_{i=1}^{k} x_i$ is the average
% of the first $k$ iterates of \cref{algo:ssgd-algo}.

% \begin{theorem}
%     \label{thm:ergodic-bound-on-values}
%     Assume that $f$ is differentiable and $f^k$ is $L_k$-smooth for all $k\in\N$.
%     Additionally assume that the step sizes $(\tau_k)_{k\in\N}$ (i) are non-increasing and (ii) satisfy
%     $\tau_k \in \left( 0, \,\mu/8(C + L_k^2) \right)$
%     for all $k\in\N$. Then, for all $K\in\N$, it holds that
%     \[
%         \frac{1}{K}\sum_{k=1}^K \E( f^k(x_k) - f^k(x_k^\star) )
%         \leq
%         \frac{D_K^2}{K\tau_K} + \frac{1}{K} \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \frac{1}{K}\sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k},
%     \]
%     where $D_K^2 := \max_{k=1,\dots, K} a_k$ and $\eta_k := \min(1, \mu\tau_k/2)$.
% \end{theorem}
% \begin{proof}
%     Starting from the equality
%     \[
%         \E_k|\!|x_{k+1} - x_k^\star|\!|^2 = A_k + \tau_k^2 \,\E_k|\!|\tilde{G}^k(x_k)|\!|^2 - 2 \tau_k \langle x_k - x_k^\star, \nabla f^k(x_k) \rangle,
%     \]
%     we apply strong convexity of $f^k$ to get
%     \begin{equation*}
%         \E_k|\!|x_{k+1} - x_k^\star|\!|^2 \leq (1-\mu\tau_k)A_k + \tau_k^2\,\E_k|\!|\tilde{G}^k(x_k)|\!|^2 - 2\tau_k(f^k(x_k) - f^k(x_k^\star)).        
%     \end{equation*}
%     Similar calculations to those in the proof of \cref{thm:tracking-error-sgd} first yield
%     \begin{equation*}
%         (1+\eta_k)\E|\!|x_{k+1} - x_k^\star|\!|^2
%             \leq (1 - \tilde{\rho}_k)a_k + 2 \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right) \tau_k^2 - 2\tau_k(1+\eta_k)(f^k(x_k) - f^k(x_k^\star)),
%     \end{equation*}
%     where $\tilde{\rho}_k = \mu\tau_k/2 - 4(C + L_k^2)\tau_k^2$, and then
%     \[
%         a_{k+1} \leq (1 - \tilde{\rho}_k)a_k + 2 \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right) \tau_k^2 - 2\tau_k(1+\eta_k)\E(f^k(x_k) - f^k(x_k^\star)) + (1+\eta_k^{-1})\Delta_k^2,
%     \]
%     where $\eta_k := \min(1, \mu\tau_k/2)$. Rearranging and using $1+\eta_k \geq 1$, we have
%     \[
%         2\tau_k\E(f^k(x_k) - f^k(x_k^\star)) \leq (1 - \tilde{\rho}_k)a_k - a_{k+1} + 2 \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right) \tau_k^2 + (1+\eta_k^{-1})\Delta_k^2.
%     \]
%     Assumption (ii) guarantees that $1 - \tilde{\rho}_k \leq 1$, and thus we have
%     \[
%         2\tau_k\E(f^k(x_k) - f^k(x_k^\star)) \leq a_k - a_{k+1} + 2 \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right) \tau_k^2 + (1+\eta_k^{-1})\Delta_k^2.
%     \]
%     The following is an adaptation of the proof of Theorem 5 in \cite{orvieto2022dynamics}. Divide both sides by $\tau_k$ and sum from $k=1,\dots, K\in\N$ to get
%     \begin{align*}
%         2\sum_{k=1}^K \E(f^k(x_k) - f^k(x_k^\star))
%         &\leq
%         \sum_{k=1}^K \frac{a_k}{\tau_k} - \sum_{k=1}^K \frac{a_{k+1}}{\tau_k} + 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k} \\
%         &=
%         \frac{a_1}{\tau_1} + \sum_{k=2}^K \frac{a_k}{\tau_k} - \sum_{k=1}^{K-1} \frac{a_{k+1}}{\tau_k} - \frac{a_{K+1}}{\tau_K} \\
%             &\quad\quad+ 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k} \\
%         &\leq
%         \frac{a_1}{\tau_1} + \sum_{k=2}^K \frac{a_k}{\tau_k} - \sum_{k=1}^{K-1} \frac{a_{k+1}}{\tau_k} \\
%             &\quad\quad+ 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k} \\
%         &=
%         \frac{a_1}{\tau_1} + \sum_{k=1}^{K-1} \frac{a_{k+1}}{\tau_{k+1}} - \sum_{k=1}^{K-1} \frac{a_{k+1}}{\tau_k} \\
%             &\quad\quad+ 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k} \\
%         &=
%         \frac{a_1}{\tau_1} + \sum_{k=1}^{K-1}a_{k+1} \left( \frac{1}{\tau_{k+1}} - \frac{1}{\tau_k} \right) \\
%             &\quad\quad+ 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k} \\
%         &\leq
%         D_K^2 \left(\frac{1}{\tau_1} + \sum_{k=1}^{K-1} \left( \frac{1}{\tau_{k+1}} - \frac{1}{\tau_k} \right)\right) \\
%             &\quad\quad+ 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k} \\
%         &= \frac{D_K^2}{\tau_K} + 2 \sum_{k=1}^K \left(C\frac{\gamma_k^2}{b_k} +  M_k^2\right)\tau_k + \sum_{k=1}^K \frac{(1+\eta_k^{-1})\Delta_k^2}{\tau_k},
%     \end{align*}
%     where we used assumption (i) in the last inequality to be able to collect the $a_k$ terms into $D_K^2 = \max_{k=1, \dots, K} a_k$. Now dividing both sides by $2K$ yields the desired result.
% \end{proof}


\section{Fast Convergence with Iterate Moving Averages}
\label{sec:ima}

We will now analyze an accelerated version of the SSGD algorithm, which makes use of \textit{iterate moving averages}.
% For background on momentum, see the discussion in \cref{sec:background-stochopt}.

\begin{algorithm}
    \label{algo:ima}
    For $k\in\N$, let $x_1\in\R^d, \tau_k\in (0, 4/\mu)$, $\gamma_k\in (0, \infty)$ and $b_k\in\N$.
    In the setting of \eqref{eq:penalized-general-problem}, 
    the \textbf{Iterate Moving Average SSGD (IMA-SSGD)} iterates
    have the form
    \begin{align*}
        x_{k+1} &:= x_k - \tau_k \tilde{G}^k(x_k) \\
        \hat{x}_{k+1} &:= \left(1-\frac{\mu \tau_k}{4 - \mu \tau_k}\right)\hat{x}_{k} + \frac{\mu \tau_k}{4 - \mu \tau_k}\, x_{k+1}\,,
    \end{align*}
    where 
    \[
        \tilde{G}^k(x) := \frac{1}{b_k} \sum_{j=1}^{b_k} G^{k}(x, \xi_k^j)\,,
    \]
    $(\xi_i^j)_{i=1,\dots,k, j=1,\dots b_k}$ are i.\,i.\,d.
    samples from the distribution of $\xi$ and $G^{k}(x, \xi)$
    is a stochastic subgradient of $f^k$ at $x$.
    We refer to $\tau_k$ as a \textbf{step size}, $\gamma_k$ as a \textbf{penalty parameter} and
    $b_k$ as a \textbf{batch size}.
\end{algorithm}

Our analysis of \cref{algo:ima} is an adaptation of methods used by Cutler and Drusvyatskiy in
\cite{cutler2023drift}, who in turn adapt averaging techniques used by Ghadimi and Lan in
\cite{ghadimi2012optimal}. The analysis hinges on the following fundamental lemma.
\begin{lemma}[Averaging lemma]
    \label{lem:averaging-lemma}
    Let $h:\R^d\to\R \cup \{\infty\}$ be a convex function and let $(x_t)_{t\in\N_0}$ be a sequence
    of vectors in $\R^d$.
    Suppose that there are constants $c_1, c_2\in\R$, a sequence of nonnegative scalars
    $(\rho_t)_{t\in\N}$, and scalar sequences $(V_t)_{t\in\N_0}$, $(\omega_t)_{t\in\N}$,
    satisfying
    \[
        \rho_t h(x_t) \leq (1 - c_1 \rho_t)V_{t-1} - (1 + c_2\rho_t) V_t + \omega_t\,
    \]
    for all $t\in\N$. Define $\hat{\Gamma}_0 := 0$,
    \[
        \hat{\rho}_t := \frac{(c_1 + c_2)\rho_t}{1 + c_2\rho_t}
        \quad\textnormal{and}\quad
        \hat{\Gamma}_t := \prod_{i=1}^t (1-\hat{\rho}_i)\,,
    \]
    for all $t\in\N$. Further, let $\hat{x}_0 := x_0$ and recursively define the averages
    \[
        \hat{x}_{t} := (1 - \hat{\rho}_t)\hat{x}_{t-1} + \hat{\rho}_t x_t
    \]
    for all $t\in\N$.
    Suppose that the relations $c_1 + c_2 > 0$, $1-c_1\rho_t > 0$, and $1 + c_2\rho_t > 0$
    hold for all $t\in\N$. Then, the following estimate holds for all $t\in\N_0$:
    \[
        \frac{h(\hat{x}_t)}{c_1 + c_2} + V_t \leq \hat{\Gamma}_t\left( \frac{h(x_0)}{c_1 + c_2} + V_0 + \sum_{i=1}^{t} \frac{\omega_i}{\hat{\Gamma}_i(1+c_2\rho_i)} \right)\,.
    \]
\end{lemma}
\begin{proof}
    See lemma 42 in \cite{cutler2023drift}.
\end{proof}

Before we make use of the averaging lemma, we will derive some preparatory
results.

\begin{lemma}
    \label{lem:accelerated-ssgd-one-step-improvement}
    Let \cref{ass:strongly-convex-radially-unbounded,ass:sampling,ass:smoothness-of-f-and-pi} hold. Then, \cref{algo:ima} with step sizes $\tau_k\in (0, 1/L_k)$ yields iterates $(x_k)_{k\in\N}$,
    such that
    \[
        2\tau_k\E(f^k(x_{k+1}) - f^k(x)) \leq \left( 1 - \mu\tau_k + 2m_k\tau_k^2 \right)\E|\!|x-x_k|\!|^2 - \E|\!|x-x_{k+1}|\!|^2 + m_k M_x^2 \tau_k^2,    
    \]
    % \[
    %     2\tau_k\E(f^k(x_{k+1}) - f^k(x_k^\star))
    %     \leq
    %     \left(1-\frac{\mu}{2}\tau_k\right) \E|\!|x-x_k|\!|^2 - \E|\!|x-x_{k+1}|\!|^2 + \frac{\mu}{4} \tau_k M_x^2\,,
    % \]
    % for all $x\in\R^d$ and $k\in\N$, where $M_x^2 := 2\,|\!|x|\!|^2 + 1$.
    for all $x\in\R^d$, where $m_k := C(1+\gamma_k^2)/b_k(1-L_k\tau_k)$ and $M^2_x := 2|\!|x|\!|^2 + 1$.
\end{lemma}
\begin{proof}
    For $k\in\N$, let $\tau_k \in (0, 1/L_k)$ and define $z_k := \nabla f^k(x_k) - \tilde{G}^k(x_k)$. Then,
    \begin{align*}
        f^k(x_{k+1}) &\leq f^k(x_k) + \langle \nabla f^k(x_k), x_{k+1} - x_k \rangle + \frac{L_k}{2} |\!| x_{k+1} - x_k |\!|^2 \\
                     &= f^k(x_k) + \langle \tilde{G}^k(x_k), x_{k+1} - x_k \rangle + \frac{L_k}{2} |\!| x_{k+1} - x_k |\!|^2 + \langle z_k , x_{k+1} - x_k \rangle.
    \end{align*}
    By Cauchy-Schwarz and Young's inequality, for all $\epsilon_k > 0$, we have
    \begin{align}
        \label{eq:proof:lem:accelerated-ssgd-one-step-improvement:1}
        f^k(x_{k+1}) &\leq f^k(x_k) + \langle \tilde{G}^k(x_k), x_{k+1} - x_k \rangle + \frac{L_k + \epsilon_k^{-1}}{2} |\!| x_{k+1} - x_k |\!|^2 + \frac{\epsilon_k}{2}|\!|z_k|\!|^2 \notag \\
                     &= f^k(x_k) + \langle \tilde{G}^k(x_k), x_{k+1} - x_k \rangle + \frac{1}{2\tau_k}|\!|x_{k+1} - x_k|\!|^2 \notag \\
                     &\phantom{= f^k(x_k) } + \frac{L_k + \epsilon_k^{-1} + \tau_k^{-1}}{2} |\!| x_{k+1} - x_k |\!|^2 + \frac{\epsilon_k}{2}|\!|z_k|\!|^2,
    \end{align}    
    where in the last step we added and subtracted $1/2\tau_k\, |\!|x_{k+1} - x_k|\!|$. Using \cref{prop:sgd-iterates-as-minimizers}, we see that $x_{k+1}$ is the minimizer of the $1/2\tau_k$-strongly convex function
    $x\mapsto \langle \tilde{G}^k(x_k), x - x_k \rangle + 1/2\tau_k\,|\!|x - x_k|\!|^2$. Hence, by the last statement in \cref{prop:strongly-convex-subdifferentiable-bound},
    we have
    \[
        \langle \tilde{G}^k(x_k), x_{k+1} - x_k \rangle + \frac{1}{2\tau_k}|\!|x_{k+1} - x_k|\!|^2 \leq \langle \tilde{G}^k(x_k), x - x_k \rangle + \frac{1}{2\tau_k}|\!|x - x_k|\!|^2 - \frac{1}{2\tau_k} |\!|x - x_{k+1}|\!|^2
    \]
    for all $x\in\R^d$. Plugging this into \eqref{eq:proof:lem:accelerated-ssgd-one-step-improvement:1}, we obtain
    \begin{align*}
        f^k(x_{k+1}) \leq f^k(x_k) &+ \langle \tilde{G}^k(x_k), x - x_k \rangle + \frac{1}{2\tau_k}|\!|x - x_k|\!|^2 - \frac{1}{2\tau_k} |\!|x - x_{k+1}|\!|^2 \\
                                   &+ \frac{L_k + \epsilon_k^{-1} + \tau_k^{-1}}{2} |\!| x_{k+1} - x_k |\!|^2 + \frac{\epsilon_k}{2}|\!|z_k|\!|^2.
    \end{align*}
    We would like to use strong convexity of $f^k$ to proceed. To do this, we first need to add and subtract $\langle \nabla f^k(x_k), x - x_k \rangle$.
    Applying \cref{prop:strongly-convex-subdifferentiable-bound} then yields
    \begin{align*}
        f^k(x_{k+1}) \leq f^k(x) - \frac{\mu}{2}|\!|x-x_k|\!|^2 &- \langle z_k, x_k - x \rangle + \frac{1}{2\tau_k}|\!|x-x_k|\!|^2 - \frac{1}{2\tau_k}|\!|x-x_{k+1}|\!|^2 \\
                &+ \frac{L_k + \epsilon_k^{-1} + \tau_k^{-1}}{2} |\!| x_{k+1} - x_k |\!|^2 + \frac{\epsilon_k}{2}|\!|z_k|\!|^2
    \end{align*}
    for all $x\in\R^d$. Simplifying, and noting that $\langle z_k, x_k - x \rangle = -\langle z_k, x - x_k \rangle $, we thus have
    \begin{align*}
        f^k(x_{k+1}) \leq f^k(x) + \left( \frac{1}{2\tau_k} - \frac{\mu}{2} \right)|\!|x-x_k|\!|^2 &+ \langle z_k, x - x_k \rangle - \frac{1}{2\tau_k}|\!|x-x_{k+1}|\!|^2 \\
                &+ \frac{L_k + \epsilon_k^{-1} + \tau_k^{-1}}{2} |\!| x_{k+1} - x_k |\!|^2 + \frac{\epsilon_k}{2}|\!|z_k|\!|^2 
    \end{align*}
    for all $x\in\R^d$ and $\epsilon_k > 0$. Choosing $\epsilon_k := \tau_k/(1-L_k\tau_k)$, we obtain
    \[
        f^k(x_{k+1}) \leq f^k(x) + \left( \frac{1}{2\tau_k} - \frac{\mu}{2} \right)|\!|x-x_k|\!|^2 + \langle z_k, x - x_k \rangle - \frac{1}{2\tau_k}|\!|x-x_{k+1}|\!|^2
                                 + \frac{\tau_k}{2(1-L_k\tau_k)}|\!|z_k|\!|^2\,,
    \]
    for all $x\in\R^d$.
    Taking expectations, we can drop the inner product term, and subsequently multipliying by $2\tau_k$ yields
    \begin{equation}
        \label{eq:proof:lem:accelerated-ssgd-one-step-improvement:2}
        2\tau_k\E(f^k(x_{k+1}) - f^k(x)) \leq \left( 1 - \mu\tau_k \right)\E|\!|x-x_k|\!|^2 - \E|\!|x-x_{k+1}|\!|^2 + \frac{\tau_k^2}{1-L_k\tau_k}\E|\!|z_k|\!|^2\,,
    \end{equation}
    for all $x\in\R^d$.
    Note that, by definition,
    \[
        \E_k|\!|z_k|\!|^2 = \E_k|\!| \tilde{G}^k(x_k) - \nabla f^k(x_k) |\!|^2 = \E_k|\!| \tilde{G}^k(x_k) - \E_k(G^k(x_k)) |\!|^2 = \Var_k(\tilde{G}^k(x_k)),
    \]
    thus
    \begin{equation}
        \label{eq:proof:lem:accelerated-ssgd-one-step-improvement:3}
        \E|\!|z_k|\!|^2 = \E(\Var_k(\tilde{G}^k(x_k))),     
    \end{equation}
    by \cref{prop:conditional-expectation-properties}.
    \Cref{ass:quad-bounded-variance} and \cref{prop:conditional-variance-properties} imply, for all $x\in\R^d$,
    \begin{align*}
        \Var_k(\tilde{G}^k(x_k)) &= \frac{1}{b_k}\Var_k(G^k(x_k)) \\
                                &\leq \frac{C}{b_k} \Big( |\!|x_k|\!|^2 + |\!|x_k|\!|^2\gamma_k^2 + \gamma_k^2 + 1 \Big) \\
                                 &\leq \frac{C}{b_k} \Big( 2|\!|x_k - x|\!|^2 + 2|\!|x|\!|^2
                                                            + 2|\!|x_k - x|\!|^2\gamma_k^2 + 2|\!|x|\!|^2\gamma_k^2 + \gamma_k^2 + 1 \Big) \\
                                &= \frac{C}{b_k} \Big( 2(1 + \gamma_k^2)|\!|x_k - x|\!|^2 + 2(1 + \gamma_k^2)|\!|x|\!|^2 + \gamma_k^2 + 1 \Big) \\
                                &= \frac{C(1+\gamma_k^2)}{b_k}\Big( 2|\!| x_k - x |\!|^2 + 2|\!|x|\!|^2 + 1 \Big) \\
                                &= \frac{C(1+\gamma_k^2)}{b_k}\Big( 2|\!| x_k - x |\!|^2 + M^2_x \Big),
    \end{align*}
    where $M^2_x := 2|\!|x|\!|^2 + 1$, and we used that $(a+b)^2 \leq 2a^2 + 2b^2$ for all $a,b\in\R$. Taking expectations on both sides,
    and using \eqref{eq:proof:lem:accelerated-ssgd-one-step-improvement:3}, we have
    \[
        \E|\!|z_k|\!|^2 \leq \frac{C(1+\gamma_k^2)}{b_k}\Big( 2\,\E|\!| x_k - x |\!|^2 + M^2_x \Big).
    \]
    Define $m_k := C(1+\gamma_k^2)/b_k(1-L_k\tau_k)$.
    Combining the above with \eqref{eq:proof:lem:accelerated-ssgd-one-step-improvement:2}, we obtain
    \[
        2\tau_k\E(f^k(x_{k+1}) - f^k(x)) \leq \left( 1 - \mu\tau_k + 2m_k\tau_k^2 \right)\E|\!|x-x_k|\!|^2 - \E|\!|x-x_{k+1}|\!|^2 + m_k M_x^2 \tau_k^2\,,
    \]
    for all $x\in\R^d$.
\end{proof}

In \cite{cutler2023drift}, the authors make an assumption, which in our setting
would essentially boil down to imposing global boundedness of $|\!|\nabla\pi(x)|\!|$.
For our purposes, however, this would be too strong, which motivates the following relaxed
assumption.
\begin{assumption}
    \label{ass:pi-gradient-bound}
    The penalty function $\pi$ is differentiable and there exist constants $D_1,D_2\in (0, \infty)$ such that
    \[
        |\!|\nabla\pi(x) - \nabla\pi(y)|\!| \leq D_1 + D_2 |\!|x-y|\!|
    \]
    for all $x,y\in\R^d$.
\end{assumption}

Note that \Cref{ass:pi-gradient-bound} holds if $\pi$ is diffierentiable and Lipschitz continuous or has Lipschitz continuous gradients.

\begin{lemma}
    \label{lem:transfer-lemma}
    In the situation of \eqref{eq:penalized-general-problem}, assume that \cref{ass:pi-gradient-bound} holds.
    Then, for all $t,i\in\N$, $u,v\in\R^d$, we have
    \[
        (f^t(u) - f^t(v)) - (f^i(u) - f^i(v))
        \leq
        \frac{\Big( D_1^2 + |\!|\nabla\pi(v)|\!|^2 \Big)(\gamma_t - \gamma_i)^2}{2\epsilon} + \left( \frac{D_2(\gamma_t-\gamma_i) + \epsilon/2}{2} \right) |\!|u-v|\!|^2\,,
    \]
    for all $\epsilon > 0$.
\end{lemma}
\begin{proof}
    Let $u, v\in\R^d$. By definition, we have
    \[
        f^k(u) - f^k(v) = f(u) - f(v) + \frac{\gamma_k}{2}(\pi(u) - \pi(v))
    \]
    for all $k\in\N$. Hence, for all $t,i\in\N$,
    \begin{equation}
        \label{eq:proof:lem:transfer-lemma:eq}
        (f^t(u) - f^t(v)) - (f^i(u) - f^i(v)) =  \frac{\gamma_t - \gamma_i}{2}(\pi(u) - \pi(v))\,.        
    \end{equation}
    For $\tau\in [0, 1]$, let $u_\tau := v + \tau(u - v)$. By the fundamental theorem of calculus and Cauchy-Schwarz,
    we have
    \begin{align*}
        \pi(u) - \pi(v) &= \int_0^1 \langle \nabla\pi(u_\tau), u - v \rangle \,\textnormal{d}\tau \\
                        &\leq \sup_{\tau\in [0, 1]} |\!|\nabla\pi(u_\tau)|\!| \, |\!|u - v|\!|\,.
    \end{align*}
    We can use \cref{ass:pi-gradient-bound} and the triangle inequality to obtain
    \begin{align*}
        |\!|\nabla \pi(u_\tau)|\!| &\leq |\!|\nabla \pi(u_\tau) - \nabla \pi(v)|\!| + |\!|\nabla\pi(v)|\!| \\
                                &\leq D_1 + D_2\tau|\!|u-v|\!| + |\!|\nabla\pi(v)|\!| \\
                                &\leq D_1 + D_2|\!|u-v|\!| + |\!|\nabla\pi(v)|\!|
    \end{align*}
    for all $\tau\in [0, 1]$. Thus,
    \[
        (\gamma_t - \gamma_i) (\pi(u) - \pi(v)) \leq (D_1 + |\!|\nabla\pi(v)|\!|)(\gamma_t - \gamma_i)|\!|u-v|\!| + D_2(\gamma_t-\gamma_i)|\!|u-v|\!|^2\,.
    \]
    By Young's inequality, for all $\epsilon > 0$,
    \[
        (\gamma_t - \gamma_i) (\pi(u) - \pi(v)) \leq \frac{(D_1 + |\!|\nabla\pi(v)|\!|)^2(\gamma_t - \gamma_i)^2}{2\epsilon} + \frac{\epsilon}{2} |\!|u-v|\!|^2 + D_2(\gamma_t-\gamma_i)|\!|u-v|\!|^2\,,
    \]   
    hence, using the fact that $(a+b)^2\leq 2(a^2+b^2)$ for all $a,b\in\R$, we have
    \[
       (\gamma_t - \gamma_i) (\pi(u) - \pi(v)) \leq \frac{\Big( D_1^2 + |\!|\nabla\pi(v)|\!|^2 \Big)(\gamma_t - \gamma_i)^2}{\epsilon} + \left( D_2(\gamma_t-\gamma_i) + \frac{\epsilon}{2} \right) |\!|u-v|\!|^2\,.
    \]
    Using this bound in \eqref{eq:proof:lem:transfer-lemma:eq}, we arrive at
    \[
        (f^t(u) - f^t(v)) - (f^i(u) - f^i(v)) \leq \frac{\Big( D_1^2 + |\!|\nabla\pi(v)|\!|^2 \Big)(\gamma_t - \gamma_i)^2}{2\epsilon} + \left( \frac{D_2(\gamma_t-\gamma_i) + \epsilon/2}{2} \right) |\!|u-v|\!|^2\,,
    \]
    as desired.
\end{proof}

% \begin{lemma}[Robbins-Siegmund]
%     Let $(\mathcal{F}_k)_{k\in\N}$ be an increasing sequence of $\sigma$-algebras and $a_k, b_k, c_k, d_k$ be
%     nonnegative $\mathcal{F}_k$-measurable random variables. If
%     \[
%         \E(a_{k+1} \,|\, \mathcal{F}_k) \leq a_k(1+b_k) + c_k - d_k
%     \]
%     and $\sum_{k=1}^\infty b_k < \infty, \sum_{k=1}^\infty c_k < \infty$ almost surely, then with probability one,
%     $(a_k)_{k\in\N}$ converges and it holds that $\sum_{k=1}^{\infty} < \infty$.
% \end{lemma}
% \begin{proof}
%     See \cite{robbins1971convergence}.
% \end{proof}

\begin{lemma}
    \label{lem:ima-ssgd-one-step-bound-ii}
    In the situation of \eqref{eq:penalized-general-problem}, assume that
    \cref{ass:strongly-convex-radially-unbounded,ass:penalty-parameters,ass:penalty-function,ass:feasible-point,ass:sampling,ass:smoothness-of-f-and-pi}
    hold.
    Further, let $(x_k)_{k\in\N}$ be iterates generated by \cref{algo:ima}, with
    $\gamma_k := \gamma \cdot k^\alpha$ for $\alpha \in (0,1)$, $\gamma\in (0,\infty)$,
    $b_k \geq 8C\tau_k(1+\gamma_k^2)/\mu$,
    and $\tau_k \in (0, 1/2L_k)$ for
    all $k\in\N$.
    Then, there exists a sequence $(b_k)_{k\in\N}$ with $b_k = \mathcal{O}(\gamma_k)$,
    and a natural number $K\in\N$, such that for all $k\in\N$ and $t\in\N_0$ with $k,t \geq K$, it holds that
    \[
        2\tau_{t}\E(f^k(x_{t+1}) - f^k(x^\star_k)) 
        \leq
        \left( 1 - \frac{\mu}{2}\tau_t \right)\E|\!|x_k^\star-x_t|\!|^2
        -
        \left( 1 - \frac{\mu}{4}\tau_t \right)\E|\!|x_k^\star-x_{t+1}|\!|^2
        +
        \frac{\mu}{2}M^2 \tau_t \,,
    \]
    where $M^2\in (0,\infty)$.
\end{lemma}
\begin{proof}
    Let $k\in\N$, $t\in\N_0$, and let $\tau_t\in (0, 1/2L_t)$.
    Note that \cref{ass:smoothness-of-f-and-pi} implies \cref{ass:pi-gradient-bound}.
    Hence, we can apply \cref{lem:accelerated-ssgd-one-step-improvement,lem:transfer-lemma} and have, for all $\epsilon > 0$,
    \begin{align*}
        2\tau_{t}\E(f^k(x_{t+1}) - f^k(x^\star_k))
        &\leq
            2\tau_{t}(f^{t}(x_{t+1}) - f^{t}(x_k^\star)) \\
            &\qquad+ \tau_{t}\left(\frac{\Big( D_1^2 + |\!|\nabla\pi(x_k^\star)|\!|^2 \Big)(\gamma_k - \gamma_{t})^2}{\epsilon} + \left( D_2(\gamma_k-\gamma_{t}) + \epsilon/2 \right) |\!|x_k^\star - x_{t+1}|\!|^2\right) \\
        &\leq \left( 1 - \mu\tau_{t} + 2m_{t}\tau_{t}^2 \right)\E|\!|x_k^\star-x_{t}|\!|^2 - \E|\!|x_k^\star-x_{t+1}|\!|^2 + m_{t} M_{x_k^\star}^2 \tau_{t}^2 \\
            &\qquad+ \tau_{t}\left(\frac{\Big( D_1^2 + |\!|\nabla\pi(x_k^\star)|\!|^2 \Big)(\gamma_k - \gamma_{t})^2}{\epsilon} + \left( D_2(\gamma_k-\gamma_{t}) + \epsilon/2 \right) |\!|x_k^\star - x_{t+1}|\!|^2\right) \\
        &= \left( 1 - \mu\tau_t + 2m_t\tau_t^2 \right)\E|\!|x_k^\star-x_t|\!|^2 \\
            &\qquad- (1 - \tau_t(D_2(\gamma_k - \gamma_t) + \epsilon/2))\E|\!|x_k^\star-x_{t+1}|\!|^2 + m_{t} M_{x_k^\star}^2 \tau_{t}^2 \\
            &\qquad+ \tau_t\frac{\Big( D_1^2 + |\!|\nabla\pi(x_k^\star)|\!|^2 \Big)(\gamma_k - \gamma_{t})^2}{\epsilon}\,,
    \end{align*}
    where $m_t = C(1+\gamma_t^2)/b_t(1-L_t\tau_t)$ and $M^2_{x_k^\star} = 2|\!|x_k^\star|\!|^2 + 1$.
    Note that, by \cref{thm:consistency}, we have $\sup_{k\in\N} M^2_{x_k^\star} \leq M^2 \in (0, \infty)$.
    Since
    $\tau_t\in (0, 1/2L_t)$, we have $1 - L_t\tau_t \geq 1/2$, and thus
    \[
        m_t \leq \frac{2C(1+\gamma_t^2)}{b_t}\,.
    \]
    With the choice of batch size $b_t \geq 8C\tau_t(1+\gamma_t^2)/\mu$, we then have
    \[
        2m_t\tau_t^2 \leq 2 \Big(\frac{\mu}{4\tau_t}\Big) \tau_t^2 = \frac{\mu}{2}\tau_t\,,
    \]
    and
    \[
        m_t M^2_{x_k^\star}\tau_t^2 \leq \frac{\mu}{4}M^2\tau_t \,.
    \]
    Since $\gamma_i = \gamma \cdot i^\alpha$ with $\alpha \in (0, 1)$, for all $i\in\N$, there exists $k_0\in\N$,
    such that for all $k,t\geq k_0$, it holds that $\gamma_{k} - \gamma_{t} \leq \mu/(8D_2)$ (this follows from
    the mean-value theorem, see for example the proof of \cref{thm:convergence-rate-squared-hinge-penalty} for
    the argument). Hence,
    with the choice $\epsilon := (D_1^2 + 1)\mu/4 \leq \mu/4$, we have
    \[
        D_2(\gamma_k - \gamma_t) + \frac{\epsilon}{2} \leq \frac{\mu}{4}\,,
    \]
    for all $k,t\geq k_0$. The extra factor $D_1^2 + 1$ in the definition of $\epsilon$ exists to simplify
    terms later. Since $x_k^\star$ is optimal for $f^k$, we have
    \[
        0 = \nabla f^k(x_k^\star) = \nabla f(x_k^\star) + \frac{\gamma_k}{2} \nabla \pi(x_k^\star),
    \]
    which implies $|\!|\nabla \pi(x_k^\star)|\!| = |\!|\nabla f(x_k^\star)|\!|/\gamma_k \leq G/\gamma_k$,
    where $G := \sup_{k\in\N} |\!| \nabla f(x_k^\star) |\!|$ and $G < \infty$ due to convergence of $(x_k^\star)_{k\in\N}$
    (\cref{thm:consistency}) and continuity of $\nabla f$. Since, $\lim_{k\to\infty} \gamma_k = \infty$ and $(\gamma_k)_{k\in\N}$ is increasing,
    there exists some $k_1\in\N$, such that $G/\gamma_k \leq 1$ for all $k\geq k_1$. Keeping in mind the definition of
    $\epsilon$, we obtain
    \[
        \frac{\Big( D_1^2 + |\!|\nabla\pi(x_k^\star)|\!|^2 \Big)(\gamma_k - \gamma_{t})^2}{\epsilon}
        \leq
        \frac{\Big( D_1^2 + 1 \Big)(\gamma_k - \gamma_{t})^2}{\epsilon}
        =
        \frac{4(\gamma_k - \gamma_t)^2}{\mu}
    \]
    for all $k\in\N$ with $k\geq k_1$. Note that there exists a natural number, which we will also refer to as
    $k_0$ for simplicity, such that $(\gamma_k - \gamma_t)^2 \leq (\mu^2/16) M^2$ for all $k,t\geq k_0$.
    Hence,
    \[
        \frac{\Big( D_1^2 + |\!|\nabla\pi(x_k^\star)|\!|^2 \Big)(\gamma_k - \gamma_{t})^2}{\epsilon}
        \leq
        \frac{\mu}{4} M^2\,,
    \]
    for all $k,t\geq k_0$.
    Putting everything together, we arrive at the bound
    \[
        2\tau_{t}\E(f^k(x_{t+1}) - f^k(x^\star_k)) 
        \leq
        \left( 1 - \frac{\mu}{2}\tau_t \right)\E|\!|x_k^\star-x_t|\!|^2
        -
        \left( 1 - \frac{\mu}{4}\tau_t \right)\E|\!|x_k^\star-x_{t+1}|\!|^2
        +
        \frac{\mu}{2}M^2 \tau_t \,,
    \]
    for all $k,t \geq \max(k_0, k_1)$.
    % Define $h(x) := f^k(x) - f^k(x_k^\star)$ for all $x\in\R^d$, $\rho_{t+1} := 2\tau_{t}$, $c_1 = \mu/4$, $c_2 = -\mu/8$,
    % $V_{i} := \E|\!|x_k^\star - x_{i}|\!|^2$, $i\in\{t, t+1\}$, and $\omega_{t+1} := (\mu/2) M^2 \tau_t$.
    % Then
    % \[
    %     \rho_{t+1} h(x_{t+1}) \leq (1-c_1\rho_{k}) V_{t} - (1 + c_2 \rho_{k}) V_{t+1} + \omega_{t+1}\,.
    % \]
\end{proof}

\begin{corollary}
    In the situation of \cref{lem:ima-ssgd-one-step-bound-ii}, assume additionally
    that $\tau_k < \mu/4$ for all $k\in\N$. Then, it holds that the iterates
    $(\hat{x}_k)_{k\in\N}$
    generated by \cref{algo:ima} satisfy
    \[
        f^k(\hat{x}_k) - f^k(x_k^\star) \leq \hat{\Gamma}_k\left( f^k(x_0) - f^k(x_k^\star) + \frac{\mu}{8}V_0 + \frac{\mu^2 M^2}{16}\sum_{i=1}^{k} \frac{\tau_{i-1}}{\hat{\Gamma}_i(1-\frac{\mu}{4}\tau_{i-1})} \right)\,,
    \]
    where
    \[
        \hat{\Gamma}_k := \prod_{i=1}^k \left( 1-\frac{\mu\tau_{i-1}}{4 - \mu\tau_{i-1}} \right)\,,
    \]
    for all $k\in\N$.
\end{corollary}
\begin{proof}
    Let $h(x) := f^k(x) - f^k(x_k^\star)$ for $x\in\R^d$, $\rho_{t+1} := 2\tau_{t}$, $c_1 = \mu/4$, $c_2 = -\mu/8$,
    $V_{i} := \E|\!|x_k^\star - x_{i}|\!|^2$, $i\in\{t, t+1\}$, and $\omega_{t+1} := (\mu/2) M^2 \tau_t$.
    Then, by \cref{lem:ima-ssgd-one-step-bound-ii}, we have
    \[
        \rho_{t+1} h(x_{t+1}) \leq (1-c_1\rho_{t+1}) V_{t} - (1 + c_2 \rho_{t+1}) V_{t+1} + \omega_{t+1}\,,
    \]
    for all $t\in\N_0$.
    Setting $k := t+1$, we thus have
    \[
        \rho_{k} h(x_{k}) \leq (1-c_1\rho_{k}) V_{k-1} - (1 + c_2 \rho_{k}) V_{k} + \omega_{k}\,.
    \]
    for all $k\in\N$. For $\tau_{k-1}\in (0, 4/\mu)$, we have
    $0 < c_1 \rho_k < 1$, $-1 < c_2 \rho_k < 0$, hence $1 - c_1\rho_k > 0$ and $1 + c_2\rho_k > 0$.
    Of course, $c_1 + c_2 = \mu/8 > 0$. Thus, all conditions of \cref{lem:averaging-lemma} are
    satisfied. Setting
    \[
        \hat{\rho}_k := \frac{(c_1 + c_2)\rho_{k}}{1 + c_2\rho_{k}} = \frac{\mu\tau_{k-1}}{4-\mu\tau_{k-1}}\,, \quad \hat{x}_k := \left(1-\frac{\mu \tau_{k-1}}{4 - \mu \tau_k}\right)\hat{x}_{k-1} + \frac{\mu \tau_{k-1}}{4 - \mu \tau_{k-1}}\, x_{k}\,,
    \]
    and
    \[
        \Gamma^k := \prod_{i=1}^k (1-\hat{\rho}_i)\,,
    \]
    we can now conclude
    \[
        h(\hat{x}_k) \leq \hat{\Gamma}_k\left( h(x_0) + (c_1+c_2)V_0 + (c_1+c_2)\sum_{i=1}^{k} \frac{\omega_i}{\hat{\Gamma}_i(1+c_2\rho_i)} \right)\,,
    \]
    as desired.
\end{proof}

\chapter{Applications \& Numerical Examples}
\section{Optimal Control}
\section{Reinforcement Learning}

\chapter{Summary and Outlook}

\textcolor{red}{Restate problem} \\
\textcolor{red}{Summarize main contributions} \\
\textcolor{red}{Outlook}:
Extension to settings beyond strong-convexity assumption. High-probability bounds. Adaptive gradient methods. More general penalties.
Non-asymptotic bounds. Extension to online setting.

\printbibliography

\end{document}
